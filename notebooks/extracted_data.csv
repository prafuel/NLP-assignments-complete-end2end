URL_ID,URL,crawled_data
Netclan20241017,https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/,"AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaS, Marketing StrategyOrganization Size: 10+The ProblemBuilding AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriber’s watching behaviour, help them in content research, creation and publication. Our SolutionCreated a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model. Solution Architecture
https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0
DeliverablesWeb ApplicationsSupportsMaintenance Feature EnhancementTech StackTools used
VS code
Language/techniques used
React.js
Express.js
Node.js
Python
Models used
Python libraries
Skills used
Data scientise
Full Stack developer
Databases used
MongoDB
Web Cloud Servers used
Google Cloud PlatformProject Snapshots Home PageTool PageDashboardBlog PageSingle Blog PostAbout UsContact UsLogin PageTitle and Description tool PageThumbnail Quality check toolProject website urlhttps://tubetool.aiSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241018,https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-experience-and-dashboard-accuracy-in-partner-hospital-application/,"Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application$Client BackgroundClient: A leading hospital chain in the USAIndustry Type: HealthcareProducts & Services: Healthcare solutionsOrganization Size: 200+The ProblemBuild a web application. Develop , deploy and maintain the system in the background, and there will be more hospitals partnering with our organization utilizing this tool per our service offering, which could lead to long-term working contracts with us, if interested.The current project is a HIPPA-compliant SPA Web Application that will interface with hospital data dashboards involving discharged patients. Call-Navigator utilizes an SFTP server that is hosted in AWS, this allows our partner hospital to store patient discharge files directly to an S3 bucket.The application and dashboard are currently up and running and pulling information and data from our partner hospital. However, the project was rushed through initial development and there are several front-end features that need to be addressed (or fine-tuned/upgraded) and functions added (search/flag options and redirection) to make the application more user-friendly (dashboard accuracy). There are deliverables such as reporting that is part of the project scope that was not developed but are considered a priority after the features and functionality are improved.According to the OG developer, a GitHub account for our organization will need to be created and code uploaded to two separate projects – one for backend and one for frontend.Let me know if this project would be something you would like to further discuss. Thank you so much for your time and consideration.More information from the OG developer follows:Developer should specifically know (or be able to learn) these specific AWS features since they will be responsible for deployment and maintaining the system.For deployment, the developer must: modify AWS security groups and target groups, etc… push code to product **DO NOT REMOVE OG DEVELOPER ACCESS/NETWORK CONNECTION**Currently, there are GitHub actions created that will deploy the code if you set up the proper secrets, GitHub actions on FE/Angular app with deploy code to our static S3 website as well as creations invalidations for CloudFront that bypasses the caching. An ssh config link and key file are available.Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect.– Flags and marked issues also need to be represented on the Facility Dashboard page.– Automate the import process of file from hospital.– Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on).– Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email.– Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up.– On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again.Our Solution– Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page.– Automate the import process of file from hospital.– Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on).– Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email.– Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up.– On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again.Also, let me know if there is a “scope of project” document that we need to complete to help you and your team develop a project plan and timeline. Hopefully, with the above notes and attached files, you’ll be able to put together a project plan and proposal for this project.DeliverablesFully functional application productMaintenance and supportTech StackTools used
VS Code
Language/techniques used
Angular
Node.js
Express.js
Databases used
MySQL
Web Cloud Servers used
AWSProject SnapshotsProject website urlhttps://callsnavigator.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241019,https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-google-ads-ap/,"ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemCreate a ROASing dashboard to fetch google ads budget spent data using google ads api(campaign-wise). The challenge is to develop a ROAS (Return on Ad Spend) dashboard that efficiently retrieves and displays Google Ads budget spent data on a campaign-wise basis using the Google Ads API. The current system lacks a streamlined method for tracking and analyzing ad spend across various campaigns, leading to difficulties in assessing performance and optimizing budget allocation. The goal is to create a comprehensive dashboard that accurately fetches and visualizes budget data, enabling more effective analysis and decision-making. This involves integrating with the Google Ads API, ensuring data accuracy, and providing clear, actionable insights for better campaign management.4o miniOur SolutionGet Data source accessa. Google ads accounts
Get Access to google cloud platforma. Bigqueryb. VM
Setup Google Big Query Data warehouse
Design db schema / models
Develop Python ETL tool to pull data from source and save it to the data warehouse
Deploy ETL tool to VM and run it as cron to update data at db frequentlya. Check frequency to runb. Recommended 3 to 6 times a dayc. The ETL tool should run automatically at the set intervals and update the dbd. Add slack integration module for notifications on Cronjob failure
Get Design Access of the Dashboard
Implement the Dashboarda. Use Vuexy React Templateb. List KPI to display in UIc. List Charts to display in the UI
Implement Backend API needed to build the dashboardSolution Architecture
https://drive.google.com/file/d/1ygYpSWiOQnREeOMAn4Zk5taNbWv8eaN8/view?usp=sharing
DeliverablesETL ToolDashboardDocumentationsSource codesSupport and MaintenanceTech StackTools used
VS Code
Digital Ocean
Language/techniques used
Python
Google Big Query
Google Cloud Platform
Google Ads
API
React.js
Node.js
Github
Models used
Python libraries (Pandas) 
Skills used
Data Scientist
Full Stack developer 
Databases used
MySQL
Web Cloud Servers used
Digital OceanWhat are the technical Challenges Faced during Project ExecutionGetting the Google Ads campaigns dataHow the Technical Challenges were SolvedUsed the google Ads APIsProject SnapshotsProject website urlhttps://roasing.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241020,https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-addressing-formatting-inconsistencies-and-ensuring-data-integrity-for-a-toyota-dealership-management-firm/,"Efficient Processing and Analysis of Financial Data from PDF Files: Addressing Formatting Inconsistencies and Ensuring Data Integrity for a Toyota Dealership Management Firm$Client BackgroundClient: A leading automobile & tech firm in the USAIndustry Type: AutomobilesProducts & Services: Manufacturing & Dealership, Financial ServicesOrganization Size: 200+The ProblemThe client, a Toyota dealership management firm, faced significant challenges in efficiently processing and analyzing financial data extracted from PDF files. These documents contained crucial information regarding sales, expenses, and other financial metrics across various departments, but parsing and extracting this data accurately proved to be a daunting task. The primary issues included inconsistency in PDF formatting, difficulty in table extraction, and ensuring data integrity throughout the processing pipeline.Our SolutionTo address these challenges, we developed a comprehensive solution tailored specifically for parsing financial data from Toyota dealership PDF documents. Our solution comprised a series of modular components, each designed to handle specific aspects of the data processing pipeline. We utilized advanced PDF parsing libraries like pdfplumber to extract tables and metadata accurately. Additionally, we implemented custom algorithms for data cleaning and validation to ensure the integrity and accuracy of the extracted data.Solution ArchitectureThe architecture of our solution was designed with modularity and scalability in mind. It consisted of the following key components:PDF Parsing Module: Responsible for extracting tables and metadata from PDF documents using pdfplumber.Data Cleaning and Validation Module: Implemented custom algorithms to clean and validate the extracted data, ensuring consistency and accuracy.Data Aggregation and Analysis Module: Utilized pandas for aggregating and analyzing financial metrics across different departments and time periods.MongoDB Integration: Stored structured financial data in MongoDB collections for efficient storage and retrieval.DeliverablesCustom Python scripts for PDF parsing and data processing tailored for Toyota dealership documents.Structured financial data stored in MongoDB collections, ensuring easy access and retrieval.Comprehensive documentation detailing system architecture, usage guidelines, and maintenance procedures.Tech StackTools used
pdfplumber, pandas, MongoDB
Language/techniques used
Python, data cleaning, aggregation
Models used
Custom parsing algorithms
Skills used
Data processing, Python programming
Databases used
MongoDB
Web Cloud Servers used
GCPWhat are the technical Challenges Faced during Project ExecutionVariability in PDF document formats: Different Toyota dealership documents exhibited varying formatting styles, making consistent parsing challenging.Handling large volumes of PDF files: Processing a large number of PDF files efficiently without compromising performance was a significant challenge.Ensuring data consistency and accuracy: Maintaining data integrity throughout the processing pipeline, especially in the presence of inconsistent or erroneous data, required careful handling.How the Technical Challenges were SolvedDeveloped custom parsing algorithms capable of handling variability in PDF document formats, ensuring consistent and accurate extraction of financial data.Implemented optimized file handling techniques to efficiently process large volumes of PDF files, minimizing processing time and resource utilization.Employed rigorous data cleaning and validation routines to identify and rectify inconsistencies or errors in the extracted data, ensuring its integrity and accuracy.Business ImpactStreamlined financial data processing for Toyota dealerships, resulting in improved operational efficiency and decision-making.Enhanced data accuracy and reliability facilitated better insights into dealership performance and financial health.Reduced manual effort and processing time, enabling stakeholders to focus on strategic tasks rather than mundane data processing activities.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241021,https://insights.blackcoffer.com/development-of-ea-robot-for-automated-trading/,"Development of EA Robot for Automated Trading$Objective:The goal of this project is to build an Expert Advisor (EA) Robot that automates trading by using predictions generated from a machine learning (ML) model. The EA will operate in real-time, leveraging both historical and live data to make buy/sell decisions. Data can be obtained via two primary methods:API: A free version that provides limited data for a single currency pair, and a paid version offering access to multiple pairs.
Trading Platforms: FX Pro or XM, which will serve as direct sources for real-time trading data.Additionally, the EA Robot will use an MQL5 script that defines the trading strategy, enabling automated trading on any MetaTrader 5 (MT5) platform. The core focus is to train an ML model for predictive trading, integrate data streams, and implement a scalable MQL5 strategy for execution.Key Steps:1. Project SetupAPI and Platform Access:Obtain API credentials for both free and paid versions.
Set up FX Pro or XM trading platforms and retrieve login details.
Ensure compatibility with MetaTrader 5 (MT5) for seamless integration.
Test API Connection:Test API endpoints to verify data retrieval for currency pairs. This ensures real-time data flow for both historical and live trading.2. Data Retrieval and PreparationAPI Data Integration:Set up API connections to retrieve historical and live trading data.
The free version will provide access to data for a single currency pair (e.g., EURUSD), while the paid version allows for multiple pairs (e.g., GBPUSD, GOLD, USDCAD, etc.).
FX Pro / XM Data Handling:Integrate FX Pro and XM platforms for data reading and trade execution.
Use Python and MT5 libraries to retrieve real-time data for selected currency pairs.
Historical Data Storage:Collect historical data for model training, storing it in a structured format (e.g., CSV) for further processing.3. Model Development and TrainingFeature Engineering:Compute technical indicators (e.g., RSI, MACD, EMA) for selected currency pairs using historical data.
Generate buy/sell signals for training, ensuring the data reflects actual market patterns.
Model Training:Develop and train the model using historical data to predict buy/sell signals by choosing appropriate ML model like ( XGBoost, LSTM, or Reinforcement Learning)
Focus on ensuring that the model can generalize well to unseen data and respond effectively to market changes.
Model Evaluation:Evaluate model performance based on metrics like accuracy, precision, recall, and profit optimization.
Adjust and fine-tune hyperparameters for improved predictive performance.4. Backtesting and Performance EvaluationBacktesting Framework Setup: 
Develop a system to test the model on historical data.
Performance Metrics Definition: 
Define relevant metrics (e.g., Sharpe ratio, drawdown).
Strategy Backtesting: 
Run the model through historical data to evaluate performance.
Results Analysis: 
Analyze backtesting results and identify areas for improvement.5. Real-Time Data IntegrationReal-Time Data Handling:Set up continuous data feeds via APIs or FX Pro/XM platforms, fetching real-time trading data at regular intervals (e.g., 15 minutes).
Ensure that the data is cleaned, preprocessed, and normalized on the fly for live prediction purposes.
Technical Indicators Calculation:Compute technical indicators on real-time data, ensuring the model uses up-to-date market conditions.
Prediction Script:Develop scripts to apply the trained model to real-time data and generate buy/sell predictions.6. MQL5 Script DevelopmentStrategy Implementation:Develop an MQL5 script that includes the trading strategy based on model predictions are creates an interface between the AI system and trading platform.
The script will incorporate logic for stop-loss, take-profit, and position sizing, ensuring that risk management is integrated.
Platform Compatibility:Ensure the MQL5 script is compatible with any MT5 platform for easy deployment.
Execution of Trades:Implement a system where the EA uses the strategy to execute trades automatically based on the model’s predictions._________________________________________________________________________This approach ensures the development of a robust, data-driven EA Robot that integrates ML predictions with real-time trading platforms and APIs. It also guarantees flexibility for live trading on various currency pairs and adaptability to changing market conditions.



Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241022,https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/,"AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaS, Marketing StrategyOrganization Size: 10+The ProblemBuilding AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriber’s watching behaviour, help them in content research, creation and publication. Our SolutionCreated a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model. Solution Architecture
https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0
DeliverablesWeb ApplicationsSupportsMaintenance Feature EnhancementTech StackTools used
VS code
Language/techniques used
React.js
Express.js
Node.js
Python
Models used
Python libraries
Skills used
Data scientise
Full Stack developer
Databases used
MongoDB
Web Cloud Servers used
Google Cloud PlatformProject Snapshots Home PageTool PageDashboardBlog PageSingle Blog PostAbout UsContact UsLogin PageTitle and Description tool PageThumbnail Quality check toolProject website urlhttps://tubetool.aiSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241023,https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-experience-and-dashboard-accuracy-in-partner-hospital-application/,"Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application$Client BackgroundClient: A leading hospital chain in the USAIndustry Type: HealthcareProducts & Services: Healthcare solutionsOrganization Size: 200+The ProblemBuild a web application. Develop , deploy and maintain the system in the background, and there will be more hospitals partnering with our organization utilizing this tool per our service offering, which could lead to long-term working contracts with us, if interested.The current project is a HIPPA-compliant SPA Web Application that will interface with hospital data dashboards involving discharged patients. Call-Navigator utilizes an SFTP server that is hosted in AWS, this allows our partner hospital to store patient discharge files directly to an S3 bucket.The application and dashboard are currently up and running and pulling information and data from our partner hospital. However, the project was rushed through initial development and there are several front-end features that need to be addressed (or fine-tuned/upgraded) and functions added (search/flag options and redirection) to make the application more user-friendly (dashboard accuracy). There are deliverables such as reporting that is part of the project scope that was not developed but are considered a priority after the features and functionality are improved.According to the OG developer, a GitHub account for our organization will need to be created and code uploaded to two separate projects – one for backend and one for frontend.Let me know if this project would be something you would like to further discuss. Thank you so much for your time and consideration.More information from the OG developer follows:Developer should specifically know (or be able to learn) these specific AWS features since they will be responsible for deployment and maintaining the system.For deployment, the developer must: modify AWS security groups and target groups, etc… push code to product **DO NOT REMOVE OG DEVELOPER ACCESS/NETWORK CONNECTION**Currently, there are GitHub actions created that will deploy the code if you set up the proper secrets, GitHub actions on FE/Angular app with deploy code to our static S3 website as well as creations invalidations for CloudFront that bypasses the caching. An ssh config link and key file are available.Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect.– Flags and marked issues also need to be represented on the Facility Dashboard page.– Automate the import process of file from hospital.– Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on).– Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email.– Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up.– On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again.Our Solution– Need to be able to pull reports on call info by unit, by caller, by diagnosis, physician or across the hospital, specifically on flags for medications, conditions, ect. – Flags and marked issues also need to be represented on the Facility Dashboard page.– Automate the import process of file from hospital.– Set up or ability to set up Hospital employees to log in with their Blessing Credentials (single sign on).– Flags thrown need to show up somewhere in Call Navigator and also be able to go into them and view patients and issues. Right now, the only notification and way to view this is strictly through email.– Ability to hover over icons on Facility Dashboard and see info and be able to click on info and have a page with that info pop up.– On the Discharge tab, when setting a filter, we want that filter to stay set unless we change it. Right now, if you set a filter and then go into a patient, when you come back out, the filter is not set anymore and you have to set all over again.Also, let me know if there is a “scope of project” document that we need to complete to help you and your team develop a project plan and timeline. Hopefully, with the above notes and attached files, you’ll be able to put together a project plan and proposal for this project.DeliverablesFully functional application productMaintenance and supportTech StackTools used
VS Code
Language/techniques used
Angular
Node.js
Express.js
Databases used
MySQL
Web Cloud Servers used
AWSProject SnapshotsProject website urlhttps://callsnavigator.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241024,https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-google-ads-ap/,"ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemCreate a ROASing dashboard to fetch google ads budget spent data using google ads api(campaign-wise). The challenge is to develop a ROAS (Return on Ad Spend) dashboard that efficiently retrieves and displays Google Ads budget spent data on a campaign-wise basis using the Google Ads API. The current system lacks a streamlined method for tracking and analyzing ad spend across various campaigns, leading to difficulties in assessing performance and optimizing budget allocation. The goal is to create a comprehensive dashboard that accurately fetches and visualizes budget data, enabling more effective analysis and decision-making. This involves integrating with the Google Ads API, ensuring data accuracy, and providing clear, actionable insights for better campaign management.4o miniOur SolutionGet Data source accessa. Google ads accounts
Get Access to google cloud platforma. Bigqueryb. VM
Setup Google Big Query Data warehouse
Design db schema / models
Develop Python ETL tool to pull data from source and save it to the data warehouse
Deploy ETL tool to VM and run it as cron to update data at db frequentlya. Check frequency to runb. Recommended 3 to 6 times a dayc. The ETL tool should run automatically at the set intervals and update the dbd. Add slack integration module for notifications on Cronjob failure
Get Design Access of the Dashboard
Implement the Dashboarda. Use Vuexy React Templateb. List KPI to display in UIc. List Charts to display in the UI
Implement Backend API needed to build the dashboardSolution Architecture
https://drive.google.com/file/d/1ygYpSWiOQnREeOMAn4Zk5taNbWv8eaN8/view?usp=sharing
DeliverablesETL ToolDashboardDocumentationsSource codesSupport and MaintenanceTech StackTools used
VS Code
Digital Ocean
Language/techniques used
Python
Google Big Query
Google Cloud Platform
Google Ads
API
React.js
Node.js
Github
Models used
Python libraries (Pandas) 
Skills used
Data Scientist
Full Stack developer 
Databases used
MySQL
Web Cloud Servers used
Digital OceanWhat are the technical Challenges Faced during Project ExecutionGetting the Google Ads campaigns dataHow the Technical Challenges were SolvedUsed the google Ads APIsProject SnapshotsProject website urlhttps://roasing.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241025,https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-addressing-formatting-inconsistencies-and-ensuring-data-integrity-for-a-toyota-dealership-management-firm/,"Efficient Processing and Analysis of Financial Data from PDF Files: Addressing Formatting Inconsistencies and Ensuring Data Integrity for a Toyota Dealership Management Firm$Client BackgroundClient: A leading automobile & tech firm in the USAIndustry Type: AutomobilesProducts & Services: Manufacturing & Dealership, Financial ServicesOrganization Size: 200+The ProblemThe client, a Toyota dealership management firm, faced significant challenges in efficiently processing and analyzing financial data extracted from PDF files. These documents contained crucial information regarding sales, expenses, and other financial metrics across various departments, but parsing and extracting this data accurately proved to be a daunting task. The primary issues included inconsistency in PDF formatting, difficulty in table extraction, and ensuring data integrity throughout the processing pipeline.Our SolutionTo address these challenges, we developed a comprehensive solution tailored specifically for parsing financial data from Toyota dealership PDF documents. Our solution comprised a series of modular components, each designed to handle specific aspects of the data processing pipeline. We utilized advanced PDF parsing libraries like pdfplumber to extract tables and metadata accurately. Additionally, we implemented custom algorithms for data cleaning and validation to ensure the integrity and accuracy of the extracted data.Solution ArchitectureThe architecture of our solution was designed with modularity and scalability in mind. It consisted of the following key components:PDF Parsing Module: Responsible for extracting tables and metadata from PDF documents using pdfplumber.Data Cleaning and Validation Module: Implemented custom algorithms to clean and validate the extracted data, ensuring consistency and accuracy.Data Aggregation and Analysis Module: Utilized pandas for aggregating and analyzing financial metrics across different departments and time periods.MongoDB Integration: Stored structured financial data in MongoDB collections for efficient storage and retrieval.DeliverablesCustom Python scripts for PDF parsing and data processing tailored for Toyota dealership documents.Structured financial data stored in MongoDB collections, ensuring easy access and retrieval.Comprehensive documentation detailing system architecture, usage guidelines, and maintenance procedures.Tech StackTools used
pdfplumber, pandas, MongoDB
Language/techniques used
Python, data cleaning, aggregation
Models used
Custom parsing algorithms
Skills used
Data processing, Python programming
Databases used
MongoDB
Web Cloud Servers used
GCPWhat are the technical Challenges Faced during Project ExecutionVariability in PDF document formats: Different Toyota dealership documents exhibited varying formatting styles, making consistent parsing challenging.Handling large volumes of PDF files: Processing a large number of PDF files efficiently without compromising performance was a significant challenge.Ensuring data consistency and accuracy: Maintaining data integrity throughout the processing pipeline, especially in the presence of inconsistent or erroneous data, required careful handling.How the Technical Challenges were SolvedDeveloped custom parsing algorithms capable of handling variability in PDF document formats, ensuring consistent and accurate extraction of financial data.Implemented optimized file handling techniques to efficiently process large volumes of PDF files, minimizing processing time and resource utilization.Employed rigorous data cleaning and validation routines to identify and rectify inconsistencies or errors in the extracted data, ensuring its integrity and accuracy.Business ImpactStreamlined financial data processing for Toyota dealerships, resulting in improved operational efficiency and decision-making.Enhanced data accuracy and reliability facilitated better insights into dealership performance and financial health.Reduced manual effort and processing time, enabling stakeholders to focus on strategic tasks rather than mundane data processing activities.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241026,https://insights.blackcoffer.com/transforming-and-managing-a-large-scale-sql-pedigree-database-to-neo4j-graph-db/,"Transforming and Managing a Large-Scale SQL Pedigree Database to Neo4j Graph DB$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemTransfer an SQL relational/pedigree database that has child-parent across ~500,000 records. Client was interested in1) transforming it into a Neo4j graph DB2)Setup CRUD operation of adding,editing,deletion of horses 3)Calculate Coefficient of inbreeding 4)Setting up complete end to end graph database management system5)Setting up cron jobs which updates coidiff, coi every month and cron job which cleanse the data6)Query for tail female, mtdna, broodmareSire propertyOur SolutionData Migration to Neo4j:Analyze the existing SQL database schema to understand the relationships between entities.Design a corresponding graph schema in Neo4j that effectively represents the pedigree data.Develop scripts or use ETL tools to transfer data from the SQL database to Neo4j, ensuring data integrity and consistency during the migration process.CRUD Operations Implementation:Implement Create, Read, Update, and Delete operations for managing horse records in the Neo4j graph database.Develop APIs or user interfaces to interact with the database, allowing users to add, edit, and delete horse records as needed.Coefficient of Inbreeding Calculation:Design algorithms to calculate the COI for each horse based on its pedigree information stored in the graph database.Implement these algorithms using Cypher queries or integrate them into the application code to automate COI calculation for all horses.End-to-End Graph Database Management System:Set up the Neo4j database environment, ensuring proper configuration, security, and scalability.Establish monitoring and logging mechanisms to track database performance and detect any issues proactively.Develop backup and disaster recovery strategies to ensure data availability and integrity.Scheduled Jobs for Updates and Data Cleansing:Configure cron jobs or scheduling mechanisms to run monthly updates for COI calculation and COI differences (COIdiff).Implement data cleansing routines to identify and remove duplicate, outdated, or inconsistent records from the graph database.Query Implementation for Specific Properties:Develop Cypher queries to retrieve tail female, mtDNA, and broodmare sire properties from the graph database.Optimize queries for performance and efficiency, considering the size of the dataset and the complexity of relationships.Solution ArchitectureDeliverablesNeo4j graph database containing migrated pedigree data with CRUD operations implemented.Automated COI calculation and scheduled updates for COIdiff and data cleansing tasks.Comprehensive documentation covering database schema, CRUD operations, COI calculation algorithms, system setup, and maintenance procedures.User guides and tutorials for interacting with the graph database and executing specific queries.Training sessions for client personnel on using and maintaining the system effectively.Tech StackTools used
Auradb,gcp
Language/techniques used
Python,cypher query,Neo4j
Skills used
Cypher Querying
Databases used
graph database 
Web Cloud Servers used
GCPWhat are the technical Challenges Faced during Project ExecutionDuring the execution of the project, several technical challenges may arise, particularly when dealing with the migration to a graph database, implementation of CRUD operations, COI calculation, setting up scheduled jobs, and query optimization. Some of these challenges include:1. Data Mapping and Schema Design:   – Translating the relational schema of the SQL database into an efficient graph schema for Neo4j may be challenging, especially when dealing with complex relationships and hierarchies present in pedigree data.2. Data Migration and Integrity:   – Ensuring the accuracy and completeness of data migration from the SQL database to Neo4j while maintaining data integrity can be technically challenging. Handling large volumes of data during the migration process may also require optimizations to prevent performance issues.3. CRUD Operations Implementation:   – Implementing CRUD operations in a graph database like Neo4j requires a different approach compared to traditional SQL databases. Ensuring efficient data retrieval, update, and deletion while preserving graph relationships can be challenging.4. Coefficient of Inbreeding Calculation:   – Developing algorithms to calculate the COI based on pedigree data stored in the graph database requires a deep understanding of genetics and graph traversal techniques. Optimizing the COI calculation process for performance and accuracy can pose technical challenges.5. Database Management and Optimization:   – Setting up and managing a Neo4j database environment involves configuring parameters, optimizing queries, and monitoring performance. Ensuring scalability, security, and high availability while minimizing downtime can be technically challenging.6. Scheduled Jobs and Automation:   – Configuring cron jobs or scheduling mechanisms to automate tasks such as COI calculation, COIdiff updates, and data cleansing requires careful planning and implementation. Ensuring the reliability and correctness of scheduled jobs in a production environment can be challenging.7. Query Optimization:   – Writing efficient Cypher queries to retrieve specific properties like tail female, mtDNA, and broodmare sire from the graph database requires optimization techniques such as index usage, query planning, and query rewriting. Balancing query performance with data consistency and complexity can be challenging.8. Integration with Machine Learning (ML) Models:   – If the project involves the development of ML models for data analysis or prediction, integrating these models with the graph database and ensuring seamless data flow between them can be technically challenging. Handling real-time data updates and model inference can also pose challenges.Addressing these technical challenges requires a combination of domain knowledge, expertise in graph database technologies like Neo4j, proficiency in query optimization techniques, and robust software engineering practices. Close collaboration between database administrators, developers, data scientists, and domain experts is essential to overcome these challenges effectively during project execution.How the Technical Challenges were SolvedTo overcome the technical challenges faced during the project execution, the following strategies and approaches can be adopted:1. Data Mapping and Schema Design:   – Collaborate with domain experts to understand the intricacies of pedigree data and design a graph schema in Neo4j that accurately represents the relationships between entities.   – Use Neo4j’s data modeling best practices and guidelines to optimize the schema for efficient data retrieval and traversal.2. Data Migration and Integrity:   – Develop robust ETL (Extract, Transform, Load) processes or use specialized migration tools to transfer data from the SQL database to Neo4j.   – Implement data validation checks and reconciliation procedures to ensure the accuracy and integrity of migrated data.3. CRUD Operations Implementation:   – Utilize Neo4j’s Cypher query language and official drivers to implement CRUD operations efficiently.   – Leverage Neo4j’s transaction support to ensure data consistency and atomicity during CRUD operations.4. Coefficient of Inbreeding Calculation:   – Collaborate with geneticists or domain experts to design and validate algorithms for COI calculation based on pedigree data.   – Implement COI calculation algorithms using Cypher queries or integrate them into the application code, optimizing for performance and accuracy.5. Database Management and Optimization:   – Follow Neo4j’s best practices for database setup, configuration, and optimization.   – Monitor database performance using built-in tools or third-party monitoring solutions and fine-tune configuration parameters as needed.6. Scheduled Jobs and Automation:   – Used cron jobsavailable in the programming language/framework used for the project to schedule tasks like COI calculation, COIdiff updates, and data cleansing.   – Implement error handling and logging mechanisms to ensure the reliability and correctness of scheduled jobs.7. Query Optimization:   – Profile and analyze Cypher queries to identify performance bottlenecks and optimize them using techniques like query planning, index usage, and query rewriting.   – Leverage Neo4j’s query execution plans and profiling tools to identify optimization opportunities.8. Integration with Machine Learning (ML) Models:   – Develop APIs or services to integrate ML models with the graph database, enabling seamless data exchange between them.   – Implement real-time data pipelines or batch processing workflows to feed data to ML models and ingest predictions or insights back into the database.Throughout the project execution, maintain close collaboration between database administrators, developers, data scientists, and domain experts to address technical challenges effectively. Conduct regular reviews and iterations to refine solutions and ensure alignment with project goals and requirements.Business ImpactThe successful execution of the project can lead to significant business impact across several dimensions:1. Enhanced Data Management:   – Migrating the SQL relational database to a graph database like Neo4j enables more efficient and intuitive representation of pedigree data. This improves data accessibility, query performance, and scalability, leading to better overall data management.2. Improved Operational Efficiency:   – Implementing CRUD operations and automation of tasks such as COI calculation and data cleansing streamlines the management of pedigree data. This reduces manual effort, minimizes errors, and enhances operational efficiency.3. Better Decision Making:   – Access to accurate and up-to-date pedigree information, along with calculated COI values, empowers stakeholders to make informed breeding decisions. This can lead to improved breeding outcomes, such as healthier offspring and desired traits, ultimately enhancing the competitiveness of the business.4. Cost Savings:   – By automating routine tasks and optimizing database performance, the project can result in cost savings associated with labor, maintenance, and infrastructure. Additionally, better breeding decisions based on COI calculations can help avoid costly genetic issues in the long term.5. Competitive Advantage:   – Leveraging advanced technologies like Neo4j and machine learning for pedigree management positions the business at the forefront of innovation in the equine industry. This can differentiate the business from competitors and attract customers who value data-driven breeding practices.6. Compliance and Risk Management:   – Maintaining accurate pedigree records and ensuring data integrity through automated validation and cleansing processes helps mitigate regulatory compliance risks. It also reduces the risk of breeding-related issues such as genetic disorders or inbreeding depression, safeguarding the reputation of the business.7. Scalability and Growth:   – The scalable architecture of Neo4j and the automation of key processes lay the foundation for handling larger volumes of pedigree data and supporting business growth. As the business expands, the graph database can accommodate increasing data complexity and user demands.8. Customer Satisfaction:   – Providing stakeholders with easy access to comprehensive pedigree information and tools for informed decision-making enhances customer satisfaction. Breeders, owners, and other stakeholders benefit from a more transparent and efficient breeding process, fostering long-term relationships with the business.Overall, the successful execution of the project can drive significant business impact by optimizing data management processes, improving decision-making capabilities, reducing costs, and strengthening the business’s competitive position in the equine industry.Project SnapshotsProject website urlVelox.Horse (velox-dev-ezvwl7dg6a-uc.a.run.app)Project Videohttps://www.loom.com/share/bb2cf781db30489b84d2d539c4b09e7e?sid=9a408b46-b860-42f2-8f62-ac5ff2a772c0SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241027,https://insights.blackcoffer.com/enhancing-model-accuracy-from-58-to-over-90-strategies-for-improving-predictive-performance/,"Enhancing Model Accuracy from 58% to Over 90%: Strategies for Improving Predictive Performance$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemTo improve the accuracy to 90%+ where the current models which were built were giving an accuracy of 58%. The existing predictive models are currently achieving an accuracy of 58%, which is insufficient for meeting the desired performance benchmarks. The objective is to enhance the accuracy of these models to exceed 90%. This requires a comprehensive evaluation and improvement of the model development process, including data quality, feature engineering, algorithm selection, and hyperparameter tuning. The challenge lies in identifying and implementing effective strategies to significantly boost the model’s predictive accuracy while ensuring robustness and generalizability.Our SolutionOur solution architecture is designed to efficiently preprocess financial data, perform feature selection, and train a GRU (Gated Recurrent Unit) model for predictive analysis. The architecture comprises the following components:Data Preprocessing: Involves loading the dataset from a CSV file, checking for null values, and performing initial exploratory data analysis (EDA).Feature Selection: Utilizes correlation analysis to identify relevant features and drop irrelevant ones.Model Building: Constructs a GRU model using TensorFlow’s Keras API, consisting of multiple GRU layers followed by dropout regularization and a dense output layer with a sigmoid activation function.Model Evaluation: Evaluates the trained model on test data to measure its performance.Solution ArchitectureDeliverablesCustom Python scripts for data preprocessing, feature selection, and model building.Trained GRU model saved to a file for future use.Tech StackTools used
pandas, scikit-learn, TensorFlow, Keras, seaborn, matplotlib
Language/techniques used
<Python programming, data preprocessing, feature selection, deep learning
Models used
GRU (Gated Recurrent Unit)
Skills used
 Data preprocessing, feature engineering, model building, evaluation
Databases used
Multiplexer What are the technical Challenges Faced during Project ExecutionHandling data variability: Dealing with varying data formats and distributions across different datasets.Feature selection: Identifying relevant features and discarding irrelevant ones to improve model performance.Model optimization: Tuning hyperparameters and optimizing the architecture of the GRU model for better accuracy and generalization.How the Technical Challenges were SolvedThe solution provides a reliable framework for analyzing financial data and making predictions, aiding in strategic decision-making for the business.By accurately predicting financial trends and performance metrics, the solution enables proactive measures to be taken to optimize operations and maximize profitability.Business ImpactThe solution provides a reliable framework for analyzing financial data and making predictions, aiding in strategic decision-making for the business.By accurately predicting financial trends and performance metrics, the solution enables proactive measures to be taken to optimize operations and maximize profitability.Project Snapshots Project Videohttps://www.loom.com/share/bed67661fa7540d2ab39705096c4591d?sid=91b0bdb7-ca4b-43ab-8d81-aaff7ed4a679SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241028,https://insights.blackcoffer.com/securing-sensitive-financial-data-with-privacy-preserving-machine-learning-for-predictive-analytics/,"Securing Sensitive Financial Data with Privacy-Preserving Machine Learning for Predictive Analytics$Client BackgroundClient: A leading Finance & tech firm in the USAIndustry Type: FinanceProducts & Services: Financial Services, SaaSOrganization Size: 200+The ProblemOur client, a leading financial institution, faced the challenge of leveraging machine learning for predictive analytics while ensuring the security and privacy of sensitive data. Traditional machine learning models posed limitations in preserving data privacy, especially when dealing with sensitive financial information.Our SolutionWe proposed integrating Fully Homomorphic Encryption (FHE) with the powerful XGBoost model to provide secure and privacy-preserving predictive analytics capabilities. By implementing FHE-XGBoost, we aimed to enable the client to leverage machine learning for decision-making without compromising data privacy.Solution ArchitectureData Ingestion and Preprocessing:Ingest data from various sources and preprocess it for analysis, ensuring data quality and consistency.Homomorphic Encryption Integration:Apply Fully Homomorphic Encryption (FHE) to the preprocessed data to maintain end-to-end security and privacy.FHE-XGBoost Model Integration:Integrate FHE-XGBoost within the Predictive Decision Tree Engine (PDTE) for secure model training and inference.Model Evaluation and Decision Support:Evaluate the trained FHE-XGBoost model using validation datasets and provide decision support based on predictive analytics insights.Security, Compliance, and Scalability:Implement security measures, ensure compliance with data privacy regulations, and design for scalability to accommodate growing datasets and computational demands.Deliverables Integration and Implementation:Implement Fully Homomorphic Encryption (FHE) with the XGBoost model within the Predictive Decision Tree Engine (PDTE) framework, ensuring seamless integration for both training and inference. Modify PDTE source code to accommodate FHE-XGBoost, optimizing for compatibility and performance with large-scale datasets.Support and Evaluation:Provide comprehensive documentation and training materials for deploying and utilizing the FHE-XGBoost solution. Develop a robust testing framework to validate functionality, accuracy, and security, utilizing synthetic and real-world datasets. Offer deployment assistance, ongoing support, and conduct a business impact analysis to assess improvements in data security, compliance adherence, and decision-making processes.Tech StackTools used
SEAL library, XGBoost, PDTE (Predictive Decision Tree Engine), FHE libraries
Language/techniques used
Python, homomorphic encryption, machine learning model integration
Models used
FHE-XGBoost
Skills used
Machine learning, cryptography, software development
Web Cloud Servers used
Virtual Machine (Linux)What are the technical Challenges Faced during Project ExecutionIntegration Complexity: Integrating FHE with XGBoost and PDTE posed significant challenges due to the complexity of homomorphic encryption and the need to maintain model performance.Performance Overhead: FHE imposes computational overhead, potentially impacting the performance of predictive analytics models, especially with large datasets.Data Handling: Managing encrypted data while ensuring efficient computation and preserving data privacy presented technical hurdles.How the Technical Challenges were SolvedCollaborative Approach: We collaborated closely with cryptography experts and machine learning engineers to address integration complexities and ensure compatibility between FHE, XGBoost, and PDTE.Optimization Techniques: We employed optimization techniques to mitigate the performance overhead of FHE, including algorithmic optimizations and leveraging hardware acceleration where possible.Data Encryption Strategies: We devised efficient strategies for encrypting and handling data to minimize computational overhead while maintaining data privacy.Business ImpactEnhanced Data Security: By implementing FHE-XGBoost, the client achieved enhanced data security by performing predictive analytics on encrypted data, mitigating the risk of data breaches.Compliance Adherence: The solution enabled the client to comply with regulatory requirements regarding data privacy and security in the financial industry.Improved Decision Making: With access to secure and privacy-preserving predictive analytics, the client could make data-driven decisions with confidence, leading to improved operational efficiency and strategic planning.Project SnapshotsProject VideoLink : – https://www.loom.com/share/81281bccfbd64d85abb3f0738213e26f?sid=68ad062e-94de-48eb-b03c-0ccbd4a92869SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241029,https://insights.blackcoffer.com/enhancing-data-collection-for-research-institutions-addressing-survey-fatigue-and-incorporating-verbal-communication-for-richer-insights/,"Enhancing Data Collection for Research Institutions: Addressing Survey Fatigue and Incorporating Verbal Communication for Richer Insights$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemOur client, a leading research institution, faced challenges in collecting rich, engaging data through traditional survey methods. Participants often found surveys tedious and uninteresting, leading to low response rates and limited insights. Additionally, the client sought a way to accommodate participants who preferred verbal communication over written responses.Our SolutionTo address these challenges, proposed integrating Generative AI technology into the Qualtrics platform. This would allow for dynamic, AI-driven conversations with participants, making the survey experience more interactive and personalized. Additionally, we suggested implementing speech-to-text functionality to cater to participants who preferred vocalizing their responses.Solution ArchitectureIn solution involved integrating ChatGPT and Gemini AI models into the Qualtrics environment. Participants could engage in conversations with the AI either by typing or speaking their prompts. Speech inputs were processed using Gemini’s speech-to-text capabilities, enabling seamless interaction with the AI models.Deliverables Integration of ChatGPT and Gemini AI with Qualtrics
Speech-to-text functionality for verbal responses
Enhanced participant engagement and data collection capabilitiesTech StackTools used
Qualtrics,Chat-GPT
Language/techniques used
JavaScript, APIs
Models used
GPT-3.5
Skills used
Programming, AI Integration, Problem-solving, Communication, Adaptability, Testing, Project ManagementWhat are the technical Challenges Faced during Project ExecutionReal-time Integration: Achieving seamless real-time integration of AI models with Qualtrics proved challenging due to the asynchronous nature of the platforms.Speech-to-Text Accuracy: Ensuring accurate transcription of speech inputs posed a technical challenge, especially in noisy environments or with accented speech.How the Technical Challenges were Solved:How the Technical Challenges were SolvedSimulated AI Model: To overcome real-time integration challenges, we implemented a simulated AI model that generated personalized responses based on user prompts.Enhanced Speech Processing: We fine-tuned Gemini’s speech-to-text capabilities to improve accuracy and robustness, enabling reliable transcription of verbal inputs.Business ImpactImproved Participant Engagement: The implementation of AI-driven conversations led to higher participant engagement and increased survey completion rates.Richer Data Collection: Participants provided more detailed and nuanced responses, leading to richer datasets and deeper insights for the client’s research initiatives.Project SnapshotsProject website urlDemo Preview:- https://yale.yul1.qualtrics.com/jfe/preview/previewId/247abe2a-632f-408c-9471-ed8b4a4021e3/SV_9MRMZ9vLHtj5t1c?Q_CHL=preview&Q_SurveyVersionID=currentProject VideoLink : – https://www.loom.com/share/ab673582fbd4447ea87ea00b3125864c?sid=0e3ad2d5-bd74-46c0-ac61-f7a06ee15008SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241030,https://insights.blackcoffer.com/analyzing-the-impact-of-positive-emotions-and-pandemic-severity-on-mental-health-and-resilience-among-entrepreneurs-insights-and-predictive-modeling/,"Analyzing the Impact of Positive Emotions and Pandemic Severity on Mental Health and Resilience Among Entrepreneurs: Insights and Predictive Modeling$Client BackgroundClient: A leading research institution in the USAIndustry Type: R&DProducts & Services: Research, Higher EducationOrganization Size: 20000+The ProblemFaced challenges in understanding the impact of positive emotions and perceived severity of the pandemic on mental health and resilience among entrepreneurs. The goal was to provide a detailed analysis and predictive models to help comprehend these interactions and their implications.Our SolutionWe developed a comprehensive solution involving descriptive, inquisitive, and predictive analytics. This included formulating hypotheses, performing exploratory factor analysis, designing predictive models, and visualizing interactions through graphs and tables. Our solution aimed to provide actionable insights into the relationships between positive emotions, perceived severity of the pandemic, and mental health.Solution Architecture Data collection and preprocessing Exploratory Factor Analysis (EFA) to identify key indicators Regression analysis to understand relationships between variables Structural Equation Modeling (SEM) to visualize and quantify interactions Creation of detailed tables and figures for publicationDeliverables Descriptive statistics table Fit statistics table for hypothesized and modified models SEM results with coefficients and interaction effects Visualizations of interactions between resiliency, perceived severity of the pandemic, positive emotions, and mental health Comprehensive report with charts, tables, results, and methodologyTech StackTools used
Jupyter Notebook, Python Libraries
Language/techniques used
Python, Exploratory Factor Analysis, Regression Analysis,
Models used
Ordinary Least Squares (OLS), Factor Analysis, Structural Equation Model (SEM)
Skills used
Data preprocessing, feature engineering, statistical analysis, model evaluation, data visualization.
Databases used
CSV files containing survey data.What are the technical Challenges Faced during Project ExecutionWe faced challenges with missing data, which affected the accuracy of the models. Integrating different data sources and ensuring data quality were also significant hurdles.How the Technical Challenges were SolvedWe used data imputation techniques to handle missing values and implemented robust data cleaning processes. For integration, we used efficient ETL (Extract, Transform, Load) pipelines to ensure seamless data flow.Business ImpactThe project provided Jean Kabongo with valuable insights into the factors affecting entrepreneurial resilience during the pandemic. The detailed analysis and predictive models helped in understanding the key drivers of mental health and provided a foundation for further research and intervention strategies.Project Snapshots Project website urlhttps://github.com/AjayBidyarthy/Jean-Kabongo-Research-ProjectProject VideoCreate video of the project.https://www.loom.com/share/a39b1aea53b64db19085d86b16de4f18?sid=d3e39a65-bdf5-4022-b458-b023e845c13bSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241031,https://insights.blackcoffer.com/dynamic-brand-centric-dashboard-for-automotive-dealerships-pdf-to-financial-insights-with-flask-react-architecture-and-aws-cloud-hosting/,"Dynamic, Brand-Centric Dashboard for Automotive Dealerships: PDF to Financial Insights with Flask-React Architecture and AWS Cloud Hosting$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemIn today’s fast-paced digital environment, automotive brands and dealerships demand personalized and dynamic online experiences that reflect their unique identities. Static web pages or dashboards that cannot be customized limit engagement, fail to showcase specific brand elements, and lead to lost opportunities. A dynamic system that can adjust each page’s look and feel based on the dealership or brand’s unique requirements is essential for maintaining consistency and increasing customer engagement.Our SolutionPDF to Dashboard: Streamlining Financial Statements is a comprehensive solution designed to create a fully dynamic and customizable online experience. Key features include:Dynamic Customization:Each page or section of the dashboard can be personalized according to the brand’s identity and the dealership’s preferences. Logos, color schemes, and layout adapt dynamically to reflect specific branding guidelines.Flask and React-based System:Our dashboard employs a robust architecture using Flask for the backend and React with ApexCharts for the frontend. This combination ensures a seamless data flow, efficient processing, and a responsive user interface.Financial Statement Integration:With over 40 financial formulas, the system provides insights into key metrics like gross profit and fixed operations. The metrics are dynamically generated based on the dealership’s financial data extracted from PDFs via PDF Plumber and Optical Character Recognition (OCR) technology.Pickle Object Storage:Calculated variables are stored as pickle objects in a MongoDB database, ensuring efficient retrieval and manipulation of data.Cloud Hosting on AWS:The solution is securely hosted on an AWS instance, providing scalability and availability for multiple dealerships and brands.Backend (Flask):Endpoints: Provide RESTful APIs for data extraction, processing, and storage.Data Extraction Module: Uses PDF Plumber and OCR for extracting data from PDF statements.Data Calculation Module: Calculates financial metrics using over 40 built-in formulas.Data Storage: Stores processed data as pickle objects in MongoDB.Frontend (React and ApexCharts):User Interface: React-based UI provides a dynamic and responsive user experience.Dashboard Personalization: Allows users to personalize dashboards by changing logos, colors, and layouts dynamically.Visualizations: ApexCharts generates dynamic graphs and charts based on financial data.Cloud Infrastructure (AWS):Hosting: The entire application is hosted on an AWS EC2 instance.Database: MongoDB database hosted on an AWS DocumentDB instance for scalable storage.Security: AWS security protocols ensure secure data transfer and storage.DeliverablesDynamic Dashboard:A fully customizable dashboard that allows brands and dealerships to personalize each page according to their unique requirements.Real-time financial metrics visualized through dynamic graphs and charts.Data Extraction Module:An automated system that extracts financial data from PDF statements using PDF Plumber and OCR technology.Financial Metrics Calculation Module:A library of over 40 formulas for calculating gross profit, fixed operations, and other key financial metrics.Backend and API Documentation:Detailed API documentation for seamless integration and data management.Training and Support:Training sessions for dealership staff to use the system effectively.Ongoing technical support and updates.Tech StackTools usedREACTPYTHONFLASKMONGODBVUEXY TEMPLATEREACTSTRAPLanguage/techniques usedPYTHONModels usedOptical Character Recognition (OCR) Model:Utilized to extract text from PDF files, the OCR model is integral to converting scanned documents into machine-readable data. We use pre-trained OCR models optimized for financial statements to ensure accurate and efficient data extraction.Custom Financial Calculation Models:These models contain a collection of formulas specifically tailored for analyzing automotive financial statements. They are used to compute metrics like gross profit, fixed operations, and other key financial indicators. These models adapt dynamically to each dealership’s data structure.Skills usedBackend Development:The backend is built using Flask, which handles data processing, API management, and ensures secure data storage. Knowledge of Python, Flask, and RESTful API design was essential for developing robust data processing and serving endpoints.Frontend Development:The frontend is built using React and ApexCharts, providing a dynamic and user-friendly interface. Skills in JavaScript, React, and data visualization tools were necessary for creating a customizable dashboard with rich data visualizations.Data Extraction and Processing:Expertise in PDF Plumber and OCR technologies was leveraged to extract accurate data from various PDF formats. Advanced data manipulation and cleaning were required to ensure data consistency across different dealership documents.Database Management:Storing financial metrics in MongoDB as pickle objects required skills in data modeling and optimization. Understanding of document-based storage and efficient querying techniques was crucial.Cloud Infrastructure:Setting up and managing the AWS instance required knowledge of cloud computing, security protocols, and scalable hosting solutions.Databases usedMONGODBWeb Cloud Servers usedAWSWhat are the technical Challenges Faced during Project ExecutionData Inconsistency Across PDFs:Financial statements from different brands and dealerships varied significantly in layout and data structure, making it difficult to standardize data extraction.Text Recognition Errors in OCR:The Optical Character Recognition (OCR) process sometimes struggled to accurately interpret certain scanned documents, leading to errors in data extraction.Real-Time Data Processing:The need for real-time data processing to generate up-to-date financial metrics placed considerable pressure on system architecture, impacting performance.Dynamic Frontend Personalization:Adapting the dashboard dynamically to each dealership’s branding required a highly modular frontend design.Scalability and Security:Ensuring the system was secure and scalable on AWS while managing sensitive financial data was a significant challenge.How the Technical Challenges were SolvedData Inconsistency Across PDFs:We implemented flexible parsing rules and custom extraction logic using PDF Plumber. By mapping key data points dynamically based on keywords and positions, we could reliably extract relevant information.Text Recognition Errors in OCR:We enhanced OCR performance by training the model specifically on sample financial documents, improving recognition accuracy. We also incorporated post-processing validation checks to identify and correct common errors.Real-Time Data Processing:We optimized the backend by implementing asynchronous data processing using Flask and Python’s concurrency features. This enabled real-time data calculations without blocking the primary data extraction process.Dynamic Frontend Personalization:We modularized the frontend using React components, allowing each dealership to customize their branding independently. Components like logos, colors, and layouts were made configurable through a simple dashboard.Scalability and Security:The AWS instance was secured using VPC, IAM, and other standard protocols. We utilized DocumentDB for MongoDB compatibility and implemented automated backups to ensure data integrity and scalable storage.Business ImpactIncreased Efficiency:Dealerships can now analyze financial performance more efficiently, reducing manual work and errors by automating the data extraction and reporting process.Enhanced Customization:The dynamic dashboards provide a personalized user experience, allowing dealerships to reflect their unique branding and business priorities.Data-Driven Decision-Making:Real-time financial insights enable dealerships to make strategic decisions quickly, improving profitability and operational efficiency.Compliance and Transparency:With accurate data extraction and visualization, dealerships can ensure compliance with financial regulations while providing transparent reports for stakeholders.Scalable Solution:Hosted on AWS, the system can easily handle an increasing number of dealerships, adapting to the growing demand for dynamic dashboards.Project SnapshotsProject website urlhttps://my.dealerdash.ai/loginSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241032,https://insights.blackcoffer.com/cloud-based-data-modeling-and-analysis-platform-with-drag-and-drop-interface-and-openai-api-integration-for-simulation-insights/,"Cloud-Based Data Modeling and Analysis Platform with Drag-and-Drop Interface and OpenAI API Integration for Simulation Insights$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The Problem Create a cloud-based solution where clients can upload datasets, use drag-and-drop functionality to select columns for data modeling, and receive the analysis results. The data analysis will be conducted using the OpenAI API, except for the mixed model, which will be handled manually. Then later user can do the simulation to get the insight of the dataset.Our SolutionDevelop a web-based application using frameworks like React for the frontend and Node.js for the backend. Establish secure methods for database access and data handling. Initially, run statistical analyses using Python and update the interface with results including Mean AUC, Mean Accuracy, Mean Log-Likelihood, Coefficients with p-values, Intercept, BIC, AIC, Y_pred, Y_test, and X_test. Allow users to visualize the dataset with different charts, such as heatmaps, line charts, and actual vs. predicted values. Over time, automate these analyses by integrating Python scripts with the backend. Deploy the application on Google Cloud, ensuring the solution supports different user roles and permissions, with robust testing and scalable infrastructure. Provide features for users to perform simulations and gain insights based on the analysis results.DeliverablesData Analysis Integration:OpenAI API: Integration for performing statistical analyses.
Python Scripts: Manual handling of mixed model analyses.Metrics and Results:Analysis results including Mean AUC, Mean Accuracy, Mean Log-Likelihood, Coefficients with p-values, Intercept, BIC, AIC, Y_pred, Y_test, and X_test.Data Visualization:Charts such as heatmaps, line charts, and actual vs. predicted values for dataset visualization.API Endpoints and Descriptions:Test APIPurpose: Fetch payload data and perform various data modeling tasks.
Modeling Types: Logistic, ordinal, nominal, Poisson regression, multiple models, and mixed models.
Details: This API retrieves the dataset from MongoDB, applies the specified statistical models, and returns the results.
Data APIPurpose: Store the output from the Test API in MongoDB.
Details: This API takes the modeling results from the Test API and stores them in a specified MongoDB collection for future reference and analysis.
Remove APIPurpose: Delete stored outputs from MongoDB.
Details: This API deletes specific records or datasets previously stored in MongoDB by the Data API based on provided criteria or identifiers.
Mixed_Model_Identify APIPurpose: Identify datasets suitable for mixed model analysis.
Details: This API analyzes the dataset to determine if it is appropriate for mixed model applications, identifying key variables and structure.
Type_of_Column APIPurpose: Identify the types of columns in the dataset.
Details: This API examines the dataset to determine the data types (e.g., categorical, ordinal, integer, real) of each column, which aids in data preprocessing and modeling decisions.Tech StackTools used
Google Cloud, VScode, MongoDB
Language/techniques used
Flask framework, Python language, MongoDB as Database, OpenAI API
Models used
Logistic ModelPurpose: Binary classification (e.g., yes/no outcomes).
Details: Predicts the probability of a binary response based on predictors.
Ordinal logistic ModelPurpose: Ordinal outcome variables (e.g., ratings).
Details: Models outcomes with a defined order but unknown distances.
Nominal logistic ModelPurpose: Categorical outcomes without order (e.g., types).
Details: Models categorical responses with no inherent order.
Poisson regression ModelPurpose: Count data modeling (e.g., event occurrences).
Details: Models the count of events within a fixed interval.
Multiple regression ModelPurpose: Multiple linear regression.
Details: Predicts a continuous outcome using multiple predictors.
Mixed ModelPurpose: Hierarchical or grouped data.
Details: Combines fixed and random effects for multi-level data.
Cox ModelPurpose: Survival analysis with time-to-event data.
Details: Models hazard rates over time.
Survival ModelPurpose: Analyzes time until events occur.
Details: Focuses on time-to-event data such as survival times.
Skills used
Prompt engineering, flask, data modelling.
Databases used
MongoDB
Web Cloud Servers used
Google CloudWhat are the technical Challenges Faced during Project Execution1- Generating R-code through ChatGPT and Executing it in the Back-end:Integrating R-cloud services with the backend is complex. It involves setting up secure connections and ensuring compatibility with the existing infrastructure.2-  Prompt Engineering:ChatGPT often struggles to generate complex code that meets specific client requirements. Refining prompts to improve code quality requires significant trial and error.3- Mixed Model Handling:Due to the complexity and dynamic nature of mixed models, using prompt engineering or manual methods is challenging. This often requires expert intervention to ensure accuracy.How the Technical Challenges were SolvedSwitching from R to Python:We replaced R with Python and executed scripts on Google Cloud Platform (GCP), which provided better compatibility, stability, and ease of managing dependencies.Improved Prompt Engineering:To ensure ChatGPT generated accurate code, we provided specific code snippets as templates for each task. This guided the AI and improved the quality and consistency of the generated code.Handling Mixed Models:We combined manual intervention with automated checks to manage the complexity of mixed models. Although initial results sometimes required corrections, iterative testing and refinement helped improve accuracy.Business ImpactThis is mainly used for healthcare field for data analysis enhancing decision-making efficiency and accuracy for users.Project website urlhttps://test.aidprofit.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241033,https://insights.blackcoffer.com/voter-profile-analysis-and-search-application-for-targeted-campaign-engagement-using-government-voter-data/,"Voter Profile Analysis and Search Application for Targeted Campaign Engagement Using Government Voter Data$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemWe are developing a new application that leverages the voter file provided by the government. This application is designed to facilitate in-depth analysis and searches within the voter database, allowing users to identify and understand individual voter profiles using a range of search criteria. A key feature of this application is the ability to view comprehensive voter profiles and update specific information, primarily focusing on the voter’s support for our candidate. This tool aims to enhance political campaign strategies through more efficient and targeted voter engagement.Our SolutionDeveloped a high-performance, end-to-end application for US elections utilising the Flask backend, React frontend, and Elasticsearch database. Successfully handled 600,000+ user data with optimised low-latency search capabilities, with a response time cut down to 2 seconds. Leveraged advanced techniques to ensure rapid retrieval and processing of critical voter information, enhancing electoral efficiency.Solution ArchitectureOur Voter Search Application for US Election operates on a robust architecture designed to ensure seamless data retrieval, processing, and presentation. Leveraging modern technologies and advanced techniques, our solution guarantees high performance and reliability even under heavy loads.Frontend:The frontend of our application is built using React, a popular JavaScript library known for its flexibility and scalability in building user interfaces. With React, we create an intuitive and responsive user experience, allowing users to interact effortlessly with the application. Our frontend components are optimized for speed and efficiency, ensuring swift navigation and data visualization.Backend:At the core of our application lies the Flask framework, a lightweight and efficient Python web framework. Flask facilitates rapid development and deployment of web applications, offering seamless integration with various libraries and tools. Our backend is responsible for handling user requests, orchestrating data retrieval from the Elasticsearch database, and serving dynamic content to the frontend. We employ Flask to implement business logic, authentication mechanisms, and API endpoints for communication between the frontend and backend components.Database:Elasticsearch serves as the database engine powering our Voter Search Application. Elasticsearch is a distributed, highly scalable search and analytics engine designed for real-time data exploration. We utilize Elasticsearch’s advanced indexing and search capabilities to efficiently store and retrieve voter information. By indexing voter data in Elasticsearch, we enable lightning-fast searches across millions of records, ensuring that users can access the information they need within seconds.Integration:Our solution integrates seamlessly with the voter file provided by the government, leveraging APIs or direct data ingestion methods to synchronize voter data with our Elasticsearch database. We implement data validation and cleansing techniques to ensure data accuracy and consistency, enabling reliable analysis and searches within the voter database.Scalability and Performance:To accommodate large volumes of voter data and handle concurrent user requests, our architecture is designed for scalability and performance. We employ horizontal scaling techniques to distribute workload across multiple server instances, ensuring optimal resource utilization and minimal response times. Additionally, we implement caching mechanisms at various layers of the application stack to reduce database load and improve overall responsiveness.Security:Security is paramount in our Voter Search Application, and we implement robust measures to safeguard voter data and user privacy. We employ industry-standard encryption protocols to protect data transmission between the frontend and backend components. Role-based access control (RBAC) mechanisms are implemented to restrict access to sensitive functionality and ensure that only authorized users can view or modify voter profiles. Regular security audits and penetration testing are conducted to identify and address potential vulnerabilities proactively.Monitoring and Logging:To maintain visibility into the performance and health of our application, we implement comprehensive monitoring and logging solutions. Metrics such as request latency, throughput, and error rates are monitored in real-time using tools like Prometheus and Grafana. Logs generated by the application components are aggregated and analysed using ELK (Elasticsearch, Logstash, Kibana) stack, enabling us to identify and troubleshoot issues efficiently.DeliverablesFully Functional Voter Search Application: Deliver a complete, end-to-end application that enables users to search, analyse, and update voter profiles for US elections.
High-Performance Backend: Provide a Flask-based backend system capable of handling 600,000+ user data with optimized low-latency search capabilities.
Responsive Frontend Interface: Deliver a user-friendly React frontend with intuitive navigation and data visualization features.
Elasticsearch Database Setup: Set up and configure Elasticsearch database to efficiently store and retrieve voter information.
Integration with Government Voter File: Implement integration with the voter file provided by the government, ensuring synchronization of voter data with the application database.
Security Measures: Implement security measures, including encryption, authentication, and access control, to protect voter data and user privacy.
Scalability and Performance Optimization: Ensure that the application architecture is scalable and performance-optimized to handle large volumes of voter data and concurrent user requests.
Monitoring and Logging Setup: Set up monitoring and logging solutions to track application performance, identify issues, and facilitate troubleshooting.
Documentation and Training Materials: Provide comprehensive documentation and training materials to guide users in using the application effectively and securely.Tech StackTools used
ReactJS, Flask, Redux, Tailwind CSS, Elasticsearch
Language/techniques used
JavaScript, Python
Skills used
Full Stack Development
Databases used
Elasticsearch
Web Cloud Servers used
AzureWhat are the technical Challenges Faced during Project ExecutionHandling Large Volume of Data: One of the primary technical challenges we encountered was efficiently handling the vast amount of voter data provided by the government. With over 600,000 user records, traditional database systems struggled to provide low-latency search capabilities, leading to performance bottlenecks and increased response times.
Optimizing Search Performance: Another challenge was optimizing search performance to meet the stringent response time requirements of our application. Elasticsearch, while powerful, required careful tuning and optimization to ensure rapid retrieval of voter information across various search criteria.
Scalability: As the application needed to support a growing user base and handle increasing volumes of voter data, ensuring scalability was crucial. We faced challenges in designing an architecture that could scale horizontally to accommodate additional users and data without sacrificing performance or reliability.
Data Integration and Synchronization: Integrating the government-provided voter file with our application posed challenges due to differences in data formats, quality, and updates frequency. Ensuring timely synchronization of voter data while maintaining data integrity was a complex task.How the Technical Challenges were SolvedOptimized Elasticsearch Configuration: We extensively tuned Elasticsearch settings, including shard allocation, index mappings, and query optimization, to enhance search performance. By leveraging features such as index-time and search-time optimizations, we significantly reduced response times and improved overall system efficiency.
Implementing Caching Mechanisms: To mitigate the impact of frequent database queries, we implemented caching mechanisms at various layers of the application stack. By caching frequently accessed data in memory, we reduced the need for redundant database queries, thereby improving response times and reducing server load.
Horizontal Scaling: To address scalability concerns, we designed an architecture that supported horizontal scaling of both frontend and backend components. By deploying multiple instances of application servers and load balancers, we distributed incoming traffic evenly and ensured optimal resource utilization, thereby accommodating growing user demand.
Data Pipeline for Synchronization: We developed a robust data pipeline that automated the process of ingesting, validating, and synchronizing voter data with our Elasticsearch database.Business ImpactThe successful resolution of technical challenges had a significant business impact, enabling our Voter Search Application to deliver value to political campaigns and electoral strategists:Enhanced User Experience: By optimizing search performance and reducing response times to just 2 seconds, we provided users with a seamless and responsive experience, facilitating faster decision-making and analysis of voter profiles.
Improved Campaign Efficiency: With efficient data retrieval and analysis capabilities, political campaigns could identify and target specific voter segments more effectively, leading to optimized resource allocation and increased voter engagement.
Competitive Advantage: Our ability to handle large volumes of voter data and provide real-time insights gave our application a competitive edge in the market, attracting users and generating interest from political organizations seeking advanced campaign tools.
Cost Savings: By implementing scalable and efficient solutions, we minimized infrastructure costs associated with handling high volumes of data and concurrent user requests, enabling cost-effective operation and scalability as the application grows.Overall, the successful resolution of technical challenges enabled our Voter Search Application to fulfill its objective of empowering political campaigns with advanced analytics and targeted voter engagement capabilities, thereby making a positive impact on the electoral process.Project Snapshots Project website urlhttps://votersearch.pinswork2.comSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241034,https://insights.blackcoffer.com/bert-based-classification-of-individuals-and-organizations-into-two-categories-using-natural-language-processing/,"BERT-Based Classification of Individuals and Organizations into Two Categories Using Natural Language Processing$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemThis project aims to classify people and different organizations into two categories using BERT-based models for natural language processing.Our SolutionUtilized pre-trained BERT-based models for text classification.
Fine-tuned the models on the sample dataset of the provided dataset for optimal performance.
Implemented an evaluation metric to assess the model’s accuracy.
Giving the Classification report and accuracy.Solution ArchitectureDeliverablespeople-org-classification-using-bert.ipynbTech StackTools used
Kaggle notebook
Language/techniques used
python, deep learning
Models used
BERT Based Classifier
Skills used
Data ScienceWhat are the technical Challenges Faced during Project ExecutionThe problem was the higher GPU limit is required to process the big dataset.How the Technical Challenges were SolvedWe shifted our notebook from Google Colab to Kaggle Notebook as it offers more GPU usage limits. Also, we took sample data from a given big dataset and processed it quickly.Business ImpactThe project successfully implemented a text classification solution using BERT-based models. The model achieved promising results in classifying people into organizational categories. The documentation outlines the steps from data loading to model training and evaluation, providing a comprehensive project overview.Project SnapshotsProject Website URLhttps://drive.google.com/file/d/1sYlohokwi6e0ITZALvBJvrJkJdL79Rm2/view?usp=sharingProject Videohttps://www.loom.com/share/d1390fced221473ea78d360f5c61fac5SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241035,https://insights.blackcoffer.com/comprehensive-analysis-of-solana-and-ethereum-contributors-using-github-api-with-comparative-study-of-1000-random-github-profiles/,"Comprehensive Analysis of Solana and Ethereum Contributors Using GitHub API with Comparative Study of 1000 Random GitHub Profiles$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemPerform analysis of all contributor’s profiles of Solana and Ethereum repository by collecting information through GitHub API then perform analysis of all random 1000 Github user’s profile. Our SolutionTo achieve this analysis first, we need to extract each GitHub user’s data. These user data are retrieved from the most popular repository. These are the following features which will impact on good developer.From github_username will fetch the all feature as follows 1) total_repos 2) total_contribution (all repos) 3) total_forks 4) total_stars 5) total_achievements 6) popularity 7) total_code_review 8) total_issues 9) total commentsTotal Repository:Description: This feature indicates the overall number of repositories owned by a GitHub user.
Impact on Good Developer: A good developer often has a varied and substantial collection of repositories, showcasing their diverse skills and experiences. A higher number of repositories may indicate a proactive approach to personal and collaborative projects.
Total Commits:Description: The total count of commits made by the GitHub user across all repositories.
Impact on Good Developer: A higher number of commits suggests an active and consistent contribution to projects. It reflects a strong work ethic and continuous engagement in coding activities.
Total Repository Forks:Description: The cumulative number of times repositories owned by the user have been forked by others.
Impact on Good Developer: Forks indicate that others find the developer’s projects valuable. It can be a measure of the influence and usefulness of their code to the community.
Total Stars Achieved:Description: The total number of stars received across all repositories.
Impact on Good Developer: Stars signify appreciation and popularity. A higher star count suggests that the developer’s work is recognized and admired by the GitHub community.
Total Issue Raise:Description: The overall count of issues raised by the user in various repositories.
Impact on Good Developer: Actively participating in issue tracking demonstrates collaboration and problem-solving skills. A good developer not only writes code but also engages in discussions and issue resolution.
Total Code Review:Description: The number of code reviews conducted by the user.
Impact on Good Developer: Engaging in code reviews indicates a commitment to maintaining code quality and collaborating effectively with team members.
Total Commits per Repository:Description: Average number of commits per repository owned by the user.
Impact on Good Developer: This metric gives insights into the developer’s commitment to individual projects. A well-maintained and actively developed repository may reflect a higher level of expertise and dedication.
Total Addition per Repository:Description: Average lines of code added per repository.
Impact on Good Developer: This provides insights into the depth of contributions to individual projects. A good developer not only contributes but does so in a meaningful and impactful way.
Total Deletion per Repository:Description: Average lines of code deleted per repository.
Impact on Good Developer: While deletions are necessary, an excessive average deletion count might indicate a need for improvement in initial code quality. A balanced approach to code maintenance is crucial.
Total Stars per Repository:Description: Average number of stars received per repository.
Impact on Good Developer: This metric helps gauge the overall popularity and impact of individual projects. A higher average star count reflects well-received and valuable work.
Total Forks per Repository:Description: Average number of times repositories are forked.
Impact on Good Developer: A higher average fork count indicates that the developer’s projects are considered as starting points for other projects, showcasing their influence and contribution to the open-source community.
Popularity Scorepopularity_score = ((((stars * weight_stars) + (forks * weight_forks) + (contributions * weight_contributions)) / max_value_among_all_repositories) * weight_popularity)Impact Scoreimpact_score = num_commits * weight_commits + num_reviewers * weight_reviewers + num_comments * weight_comments + num_issues * weight_issues )Combined ScoreCombined Score = Popularity Score + Impact ScoreSolution ArchitectureDeliverablesAnalysis of Filtered 760 users data , 790 solana and ethereum user data and 1000 random people datasetAnalysis of Filtered and combined dataset:
https://github.com/AjayBidyarthy/Abrar-Akhtar-Github-Work/blob/main/Github_Analysis_on_24_01_2024/notebook/Analysis%20of%20Github%20statistics%20of%20combined%20dataset.ipynb
Filtered and combined dataset
https://github.com/AjayBidyarthy/Abrar-Akhtar-Github-Work/blob/main/Github_Analysis_on_24_01_2024/dataset/Filtered_and_combined_data.xlsx
Analysis of 1000 random github profiles:https://github.com/AjayBidyarthy/Abrar-Akhtar-Github-Work/blob/main/Github_Analysis_on_24_01_2024/notebook/Analysis%20of%20random%201000%20github%20profile.ipynb Tech StackTools used
VSCode, MS Excel
Language/techniques used
Python, Github API
Skills used
Python libraries
Databases used
none
Web Cloud Servers used
noneWhat are the technical Challenges Faced during Project ExecutionFetching all the GitHub features from GitHub API was a time-consuming process.How the Technical Challenges were SolvedI solved this problem by parallel processing and creating checkpoints to save the data collected if the API limit was exceeded.Business ImpactAnalyzing the profiles of contributors to Solana and Ethereum repositories through the GitHub API and conducting a similar assessment of random 1000 GitHub users provides critical insights for strategic decision-making, community engagement, and resource optimization. This analysis enables the identification of key contributors, assessment of project quality and sustainability, and efficient allocation of resources based on diverse skill sets. It mitigates risks associated with dependency on a few contributors, aids talent acquisition and retention, and benchmarks the platforms’ competitiveness. Additionally, it guides innovation, and compliance measures, and ensures business continuity by proactively addressing potential disruptions. Overall, this in-depth analysis serves as a valuable tool for enhancing the vitality, innovation, and longevity of blockchain projects on Solana and Ethereum.Project Snapshots Project website urlhttps://github.com/AjayBidyarthy/Abrar-Akhtar-Github-Work/tree/main/Github_Analysis_on_24_01_2024Project Videohttps://www.loom.com/share/1f294d23f8cd4b81afd3302add4518c8SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241036,https://insights.blackcoffer.com/powerbi-rest-api-fetching-dataflow-and-refresh-schedules-with-semantic-models/,"PowerBI REST API – Fetching Dataflow and Refresh Schedules with semantic models$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The ProblemOur client, a large enterprise, faced significant challenges in managing their Power BI dataflows, datasets, and refresh schedules across multiple workspaces. The manual processes were time-consuming, error-prone, and lacked real-time monitoring capabilities. They needed an automated solution to streamline these tasks, ensure data accuracy, and improve overall efficiency.Our SolutionWe developed an automated system using Python that integrated with the Power BI API to manage and monitor dataflows, datasets, and their refresh schedules. The solution provided a comprehensive overview of all workspaces, datasets, and dataflows, including detailed transaction data and refresh schedules. This system allowed the client to efficiently track and manage their Power BI environment, reducing manual effort and improving data reliability.Solution ArchitectureData Retrieval:Utilized the Power BI API to fetch dataflow transactions, datasets, and refresh schedules.
Employed MSAL for secure authentication and authorization.Data Processing:Processed and transformed the retrieved data using Pandas.
Consolidated data from multiple sources into a single Excel file with multiple sheets.Data Storage:Stored processed data in an Excel file, ensuring easy access and further analysis.DeliverablesPython script for automated dataflow and dataset management.
Comprehensive Excel file with detailed information on dataflows, datasets, and refresh schedules.
Documentation and user guide for the solution.Tech StackTools usedPython: Primary programming language.
Power BI API: For interacting with Power BI services.
MSAL: For authentication and authorization.
Pandas: For data processing.
Requests: For making API calls.
Openpyxl: For Excel file operations.Language/techniques usedPython scripting
API integration
Data processing and transformation
Exception handling and loggingModels usedNone (as this project focused on data management rather than predictive modeling)Skills usedAPI integration
Data processing with Pandas
Excel file manipulation
Authentication and authorization using MSALDatabases usedNone (data was processed and stored in Excel files)Web Cloud Servers usedPower BI serviceWhat are the technical Challenges Faced during Project ExecutionAuthentication Issues: Ensuring secure and reliable authentication with Power BI API.API Rate Limits: Handling Power BI API rate limits and managing large volumes of data.Data Consistency: Ensuring data consistency and accuracy across multiple API calls and processing steps.Excel File Management: Efficiently managing and updating large Excel files with multiple sheets.How the Technical Challenges were SolvedAuthentication Issues:Implemented MSAL for secure authentication and handled token expiration and refresh seamlessly.API Rate Limits:Implemented retry logic and rate limit handling to manage API call limits and ensure data retrieval without interruptions.Data Consistency:Used Pandas for efficient data processing and ensured data consistency through rigorous testing and validation steps.Excel File Management:Utilized Openpyxl for efficient Excel file operations and implemented logic to avoid data duplication and ensure accurate updates.Business ImpactThe automated system significantly reduced the manual effort required for managing Power BI dataflows and datasets. It improved data accuracy, provided real-time monitoring capabilities, and allowed the client to focus on data analysis and decision-making rather than data management. The solution also enhanced overall operational efficiency and reliability.Project SnapshotsProject website urlhttps://colab.research.google.com/drive/1lcsMt9v8_apAwuhxCqsNkrOgrVTdnWjO?usp=sharinghttps://www.loom.com/share/361634e2bb014c7f9f3fd8a3fa668d62?sid=e5fa482e-2061-4195-ac7d-c9c502bc4424SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241037,https://insights.blackcoffer.com/automated-job-data-import-and-management-solution-for-enhanced-efficiency/,"Automated Job Data Import and Management Solution for Enhanced Efficiency$Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 200+The Problem:Yohannes Dossena required a streamlined way to import and manage job data on their platform. The challenge was to automate data import and improve data management efficiency.Our Solution:We developed a solution to import data from the backend to Google Sheets and implemented a data entry form for manual input. This facilitated better data management and accessibility.Solution Architecture:– Google Apps Script was used to automate data import.– A user-friendly data entry form was created for manual input.– Data was managed and visualized using Google Sheets.Deliverables:– Automated data import system.– Data entry form.– Documentation of the implemented solution.Tech Stack:– Tools used: Google Sheets– Language/techniques used: Google Apps Script– Models used: None specified– Skills used: Data automation, scripting– Databases used: Google Sheets– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Integrating data import with Google Sheets.– Ensuring data accuracy and consistency.– Creating a user-friendly interface for manual data entry.How the Technical Challenges were Solved:– Utilized Google Apps Script to automate data import.– Implemented data validation techniques to ensure accuracy.– Designed an intuitive data entry form for ease of use.Business Impact:– Streamlined data management processes.– Reduced manual effort and errors.– Enhanced data accessibility and usability for the client.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241038,https://insights.blackcoffer.com/data-analytics-and-optimization-solution-for-enhancing-renewable-energy-efficiency/,"Data Analytics and Optimization Solution for Enhancing Renewable Energy Efficiency$Client BackgroundClient: A leading energy tech firm in the USAIndustry Type: EnergyProducts & Services: Energy & Power Consulting, SaaSOrganization Size: 300+The Problem:Wind Sun, Intrinsic Tech needed a solution to analyze and optimize their renewable energy production data. The challenge was to handle large volumes of data and provide actionable insights to improve energy efficiency.Our Solution:We developed a data analytics platform using Python and machine learning techniques to analyze energy production data. This involved data preprocessing, predictive modeling, and generating optimization insights.Solution Architecture:– Data collection and preprocessing using Python and Pandas.– Predictive modeling and optimization using machine learning algorithms.– Visualization of insights using Google Looker Studio.Deliverables:– Predictive models for energy production.– Optimization insights and recommendations.– Comprehensive report on methodology and results.Tech Stack:– Tools used: Google Looker Studio– Language/techniques used: Python, Pandas, machine learning algorithms– Models used: Various machine learning models– Skills used: Data analysis, predictive modeling, optimization– Databases used: None specified– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Handling and preprocessing large datasets.– Ensuring the accuracy and reliability of predictive models.– Visualizing complex energy data effectively.How the Technical Challenges were Solved:– Utilized efficient data preprocessing techniques.– Fine-tuned machine learning models for better accuracy.– Created intuitive visualizations to represent data insights clearly.Business Impact:– Improved energy efficiency through data-driven insights.– Enabled proactive measures to optimize energy production.– Enhanced decision-making in energy management.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241039,https://insights.blackcoffer.com/time-series-analysis-and-trend-forecasting-solution-for-predicting-news-trends/,"Time Series Analysis and Trend Forecasting Solution for Predicting News Trends$Client BackgroundClient: A leading News Tech firm in the USAIndustry Type: News & MediaProducts & Services: Tech News, SaaSOrganization Size: 100+The Problem:A client required an effective solution to predict news trends using historical data. The challenge was to analyze time series data and forecast future trends accurately.Our Solution:We employed the fbprophet model in Python to predict news trends. This involved processing time series data, training the model, and generating forecasts.Solution Architecture:– Data collection and preprocessing using Python and Pandas.– Time series analysis and trend prediction using fbprophet.– Visualization of results using Google Looker Studio.Deliverables:– Predictive model for news trends.– Visualizations of forecasted trends.– Comprehensive report on methodology and results.Tech Stack:– Tools used: Google Looker Studio– Language/techniques used: Python, Pandas, fbprophet– Models used: fbprophet– Skills used: Time series analysis, data visualization– Databases used: None specified– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Handling and preprocessing large time series datasets.– Ensuring the accuracy and reliability of trend predictions.– Visualizing complex data in an understandable format.How the Technical Challenges were Solved:– Utilized efficient data preprocessing techniques to handle large datasets.– Fine-tuned the fbprophet model to improve prediction accuracy.– Created intuitive visualizations to represent forecasted trends clearly.Business Impact:– Enabled proactive decision-making based on predicted news trends.– Improved strategic planning and resource allocation.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241040,https://insights.blackcoffer.com/advanced-data-visualization-solutions-for-monitoring-key-business-metrics-with-integrated-interactive-dashboards/,"Advanced Data Visualization Solutions for Monitoring Key Business Metrics with Integrated, Interactive Dashboards$Client BackgroundClient: A leading IT tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, SaaS, IT SupportOrganization Size: 300+The Problem:Requisite Agency needed advanced data visualization solutions to effectively monitor key business metrics. The challenge was to blend multiple data sources and create dynamic, interactive dashboards.Our Solution:We explored and implemented advanced functionalities in Google Looker Studio to create comprehensive dashboards. This included data blending and visualization to highlight essential business metrics.Solution Architecture:– Data integration and blending using Google Looker Studio.– Creation of interactive dashboards for data visualization.– Regular updates and maintenance of dashboards.Deliverables:– Interactive dashboards showcasing key business metrics.– Documentation of the implemented solution.– Regular updates and support.Tech Stack:– Tools used: Google Looker Studio– Language/techniques used: Data visualization, data blending– Models used: None specified– Skills used: Data visualization, dashboard creation– Databases used: Various data sources– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Integrating data from diverse sources.– Ensuring real-time data updates and accuracy.– Creating user-friendly and insightful visualizations.How the Technical Challenges were Solved:– Employed advanced data integration techniques.– Set up automated data update mechanisms.– Designed intuitive and informative dashboards.Business Impact:– Enhanced the ability to monitor and analyze business performance.– Improved data-driven decision-making.– Provided actionable insights through dynamic visualizations.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241041,https://insights.blackcoffer.com/advanced-patient-data-analysis-solution-for-trend-identification-and-improved-healthcare-outcome/,"Advanced Patient Data Analysis Solution for Trend Identification and Improved Healthcare Outcome$Client BackgroundClient: A leading healthcare tech firm in the USAIndustry Type: HealthcareProducts & Services: Healthcare SaaSOrganization Size: 100+The Problem:Manu B required an advanced solution for analyzing patient data to identify trends and improve healthcare outcomes. The challenge was to handle sensitive health data while ensuring accuracy and compliance with regulations.Our Solution:We implemented a comprehensive data analysis system using Python and various machine-learning techniques. This involved preprocessing patient data, training predictive models, and generating insights.Solution Architecture:– Data collection and preprocessing using Python and Pandas.– Predictive modeling using machine learning algorithms.– Visualization of insights using Google Looker Studio.Deliverables:– Predictive models for healthcare outcomes.– Visualizations of data insights.– Detailed report on methodology and findings.Tech Stack:– Tools used: Google Looker Studio– Language/techniques used: Python, Pandas, machine learning algorithms– Models used: Various machine learning models– Skills used: Data analysis, predictive modeling– Databases used: None specified– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Handling sensitive health data securely.– Ensuring the accuracy and reliability of predictive models.– Visualizing complex healthcare data effectively.How the Technical Challenges were Solved:– Implemented robust data security measures.– Fine-tuned machine learning models for better accuracy.– Created intuitive visualizations to represent data insights clearly.Business Impact:– Improved patient care through data-driven insights.– Enabled proactive healthcare measures.– Enhanced decision-making in healthcare management.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241042,https://insights.blackcoffer.com/anomaly-detection-and-analysis-for-enhanced-data-integrity-and-user-experience-on-bright-datas-website/,"Anomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website$Client BackgroundClient: A leading IT tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 300+The Problem:Bright Data needed to identify unusual behavior on their website to improve data integrity and user experience. The challenge was to detect and analyze anomalies in web traffic and user activities.Our Solution:We implemented a robust anomaly detection system using various data science tools and techniques. This involved monitoring web traffic, analyzing user behavior, and identifying outliers.Solution Architecture:– Data was collected using web scraping and proxy servers.– Preprocessing and normalization of data were done using Python.– Anomaly detection was carried out using machine learning algorithms.Deliverables:– Comprehensive report on identified anomalies.– Dashboard for real-time monitoring of web traffic.– Documentation of the solution and its implementation.Tech Stack:– Tools used: HTML, CSS– Language/techniques used: Python– Models used: Anomaly detection algorithms– Skills used: Data analysis, anomaly detection– Databases used: None specified– Web Cloud Servers used: None specifiedTechnical Challenges Faced:– Handling large volumes of web traffic data.– Ensuring the accuracy of anomaly detection algorithms.– Integrating various data sources for comprehensive analysis.How the Technical Challenges were Solved:– Optimized data handling processes to manage large datasets.– Fine-tuned machine learning models to improve detection accuracy.– Used efficient data integration techniques to unify different data sources.Business Impact:– Improved data integrity and user experience on Bright Data’s website.– Enabled proactive measures to address unusual activities, enhancing security.– Provided actionable insights for better decision-making.Project Website URL:https://brightdata.comProject Video:https://drive.google.com/drive/folders/1oEzAWpZtEnzpE9fY-bgMA0hY2-IWzDo7?usp=drive_linkSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241043,https://insights.blackcoffer.com/building-custom-tflite-models-and-benchmarking-on-voxl2-chips/,"Building Custom TFLite Models and Benchmarking on VOXL2 Chips$Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241044,https://insights.blackcoffer.com/sports-prediction-model-for-multiple-sports-leagues/,"Sports Prediction Model for Multiple Sports Leagues$Client BackgroundClient: A leading sports tech firm in the USAIndustry Type: SportsProducts & Services: Sports Management, SaaSOrganization Size: 100+The ProblemThe client aimed to develop a sophisticated sports prediction model capable of forecasting game outcomes across five major sports leagues: NCAAFB, NHL, NFL, NBA, and MLB. The primary challenge was to leverage historical data and statistical inputs to accurately predict game winners. The project required integrating data from the SportRadar API, processing it efficiently, and utilizing machine learning techniques to train a predictive model. The ultimate goal was to provide real-time predictions that could assist in sports betting strategies or enhance fan engagement.Our SolutionThe proposed solution involved creating a comprehensive sports prediction model using Python, leveraging the SportRadar API for data acquisition, and storing the data in Google Cloud Storage. The project was structured around a modular approach, with each sport having its dedicated script (`{sport_name}.py`) for data processing and model training. The development workflow included:– **Data Collection**: Utilizing the SportRadar API to gather historical data for the past 3-4 seasons.– **Data Processing**: Storing the data in Google Cloud Storage and processing it into a structured format suitable for model training.– **Model Training**: Preparing the data for model training, focusing on converting JSON data to tabular format and fetching relevant team stats.– **Prediction**: Developing the predictive model to forecast game outcomes based on historical data and statistical inputs.Solution ArchitectureData Acquisition: Leveraging the SportRadar API for data collection.
Data Storage: Utilizing Google Cloud Storage for efficient data storage and retrieval.
Data Processing: Implementing data processing scripts to prepare the data for model training.
Model Development: Building the predictive model using Python and machine learning libraries.
Deployment: Planning for deployment as a Flask API or Google Cloud Function for real-time predictions.DeliverablesEnd-to-end data pipeline
A comprehensive sports prediction model for NCAAFB, NHL, NFL, NBA, and MLB.
Scripts for data collection, processing, and model training.
Documentation detailing the project’s structure, data processing steps, and model development process.
A plan for deploying the model in a production environment.Tech StackTools used
Python
Google Cloud Storage
Machine Learning
Google Cloud FunctionsLanguage/techniques used
Python
Models used
LSTM, GRU, ANN, PyCaret
Skills used
Data Analysis
Data Visualization
Cloud Functions
API Integration
Databases used
Cloud StorageWhat are the technical Challenges Faced during Project ExecutionData Collection and Storage: Collecting data for the past 3-4 seasons’ schedules, team stats, and game stats from the SportRadar API and storing it in GCP was a significant challenge.
Data Processing: Processing the raw data to create a DataFrame and augmenting it with necessary statistics was another challenge.
Model Training: Training the model with the processed data to predict game outcomes was a complex task due to the vast amount of data and the need for accurate predictions.How the Technical Challenges were SolvedData Collection and Storage: The data was collected through the SportRadar API and stored in a Google Cloud Storage bucket named ‘data_parlayy’. The GCP client was configured using a service account JSON key obtained from an environment variable GCS_Service_Account_JSON_KEY.
Data Processing: The raw data was processed through various steps to create a DataFrame and augment it with necessary statistics.
Model Training: The model training involves converting game summary and statistics data from JSON to tabular format, fetching team stats, and past game stats. The team stats and past few games stats are already processed and ready for training.Business ImpactThe implementation of the sports prediction model has the potential to significantly impact the sports betting and fan engagement industries. By providing accurate predictions of game outcomes, the model can assist in sports betting strategies or fan engagement activities by providing great insights. This can lead to improved decision-making processes, increased operational efficiency, and strategic planning within the sports betting and fan engagement industries.The sports prediction model project by Kason Karangwa is a significant step towards leveraging data science and machine learning to predict sports outcomes. The project not only addresses the technical challenges of data collection, processing, and model training but also has the potential to significantly impact the sports betting and fan engagement industries. With the successful completion of the project, Kason Karangwa has demonstrated the power of data science in predicting sports outcomes, setting a new standard for sports prediction models.Project Snapshots

SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241045,https://insights.blackcoffer.com/efficient-coach-allocation-system-for-sports-coaching-organization/,"Efficient Coach Allocation System for Sports Coaching Organization$Client BackgroundClient: A leading sports tech firm in the USAIndustry Type: SportsProducts & Services: IT Consulting, Sports SaaS, Sports ManagementOrganization Size: 100+The ProblemThe sports coaching organization, Esra Salih, faced the challenge of efficiently allocating coaches to various programs across different schools. The allocation process was previously manual and inefficient, leading to inadequate coach-program matching and inefficient use of coaches’ time. The organization needed a system that could automatically assign coaches to programs based on specific criteria such as coach availability, program preferences, and the number of students enrolled in each program.Our SolutionWe developed a custom system that divides the academic year into different terms and assigns coaches to programs based on a set of criteria. The system filters coaches based on their preferences and allocates them to programs efficiently. It ensures that coaches do not have to travel excessively and that their total work time does not exceed a certain threshold. Once a coach is assigned to a program, they are allocated for subsequent program days, ensuring consistency.The development process will involve several stages:Understanding the data: We’ll analyze the data to identify the relevant variables and constraints. These may include the locations of installations, the duration of installations, the availability of team members, and customer preferences.
Defining the objective and constraints: The objective will be to minimize the total travel time or maximize the number of installations completed within a given time frame. The constraints will include the geographical distances between locations, the working hours of team members, and the specific requirements of each installation.
Implementing the algorithm: We’ll use an optimization algorithm, such as the Traveling Salesman Problem (TSP) solver, to find the optimal routes. The algorithm will consider all possible routes and choose the one that best meets the objectives while adhering to the constraints.
Running simulations: To ensure the feasibility of the model, we’ll run simulations using different scenarios and adjust the parameters as needed.
Saving the output: The final output will be the suggested schedules, which can then be reviewed and approved by the relevant parties.In terms of technology, we’ll use Python, a popular language for data analysis and machine learning. We’ll also use the Anaconda distribution, which provides a powerful environment for scientific computing and data analysis.Solution ArchitectureThe system is built on a custom logic implementation that filters programs for a particular date and checks the past history of the program to see if a coach was assigned before. If not, it allocates a new coach based on their availability and feasibility to reach the program location in time. A filtered list of coaches based on preferences like Age-Group and Level is created to aid this process.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
Python: The primary programming language.
Anaconda: The Python distribution used for data analysis and machine learning.
Visual Studio Code: The code editor used during development.
Google App Script for deployment integrated with Google Sheets
Language/techniques used
Python
Models used
Constrained Vehicle Routing Problem with Time Windows (CVRP-TW)
Skills used
Data Analysis
Machine Learning
Optimization Algorithms
Python Programming
Databases used
CSV, Google Sheets: The data will initially be stored in a CSV file, which can be easily imported into Python using libraries like pandas.What are the technical Challenges Faced during Project ExecutionThe main challenge was developing a custom logic implementation that could efficiently allocate coaches to programs based on specific criteria.
Another challenge was ensuring that the system could handle scenarios where multiple coaches were required for a program.How the Technical Challenges were SolvedThe first challenge was addressed by developing a custom logic implementation that filters programs for a particular date and checks the past history of the program to see if a coach was assigned before. If not, it allocates a new coach based on their availability and feasibility to reach the program location in time.
The second challenge was addressed by initially focusing on assigning a single coach and gradually introducing the complexity of multiple coaches after the basic system was functional.Business ImpactThe implementation of the proposed solution has significantly improved the efficiency of the coach allocation process. The system has automated the allocation process, saving valuable time and resources that were previously spent on manual updates. It has also ensured that coaches are allocated in a way that minimizes travel and ensures that their total work time does not exceed a certain threshold.Project SnapshotsThe code for the project has been pushed to GitHub: https://github.com/AjayBidyarthy/Esra-Salih https://docs.google.com/document/d/1guIe0J6dAbznJ0QuTA1YAHWvmmXdkBVlKLbO3PCxq4k/edit#heading=h.3pis0t8r68nfProject website urlhttps://docs.google.com/spreadsheets/d/1nK-CknGIAVZkv0qUdHsRCdB5R6d86SLruqZWCN4OTT0/edit?usp=sharinghttps://docs.google.com/spreadsheets/d/1V9FaEF88B32dUClKNB88CEeufs6AUvMPQngRZx_vgCg/edit?usp=sharingSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241046,https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-using-custom-webhooks-and-google-cloud-function-2/,"Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function$Client BackgroundClient: A leading tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, SaaSOrganization Size: 300+The ProblemThe client needs a consolidated KPI dashboard that aggregates data from various applications and SaaS products. Currently, the data is scattered across different platforms, making it difficult to track key performance indicators (KPIs) effectively. The client wants a dashboard that automatically updates with new data, eliminating the need for manual updates. The dashboard should contain separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Additionally, the client wants to use Google Cloud Functions to sync data regularly between the Podio data app and Google Sheets.Our SolutionThe proposed solution involves the creation of a KPI dashboard in Google Sheets, which will serve as a central hub for all the client’s data. This dashboard will be populated with data from various sources, including Google Sheets and the Podio data app. The data will be organized into separate tabs, each representing a different aspect of the business. The dashboard will be designed to automatically update with new data, removing the need for manual updates.The process begins with obtaining access to the data in Google Sheets. Once the data is accessed, a list of KPIs to be visualized will be prepared. The data from Google Sheets will then be connected to the Google Data Studio dashboard for visualization. The dashboard will be designed to align with the client’s goals, prioritizing the most important KPIs and positioning them at the top of the dashboard. The dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designated data sheets. Collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. The dashboard will be customized with brand-aligned colors and fonts to enhance its appearance and authority.In addition to the dashboard, webhooks will be created for the Podio data app deployed as a Google Cloud Function. This will enable regular data synchronization between the Podio data app and Google Sheets, ensuring that the dashboard is always up-to-date with the latest data.Solution ArchitectureDeliverablesEnd-to-end data pipeline
KPI Dashboard in Google Sheets with separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.
Automatic update functionality to eliminate the need for manual updates.
Webhook for the Podio data app deployed as a Google Cloud Function to sync data regularly.Tech StackTools used
Python
Google Sheets
Google Data Studio
Google Cloud Functions
Podio data app
Language/techniques used
Python
Javascript
Skills used
Data Analysis
Data Visualization
Cloud Functions
API Integration
Databases used
BigQueryWhat are the technical Challenges Faced during Project ExecutionOne of the main challenges was ensuring that the dashboard could seamlessly integrate data from various sources and update automatically. 
Another challenge was designing the dashboard in a way that aligns with the client’s goals and presents the data in a clear and actionable manner.How the Technical Challenges were SolvedThe first challenge was addressed by connecting the data sources to Google Sheets and setting up the dashboard to automatically update with new data. This was achieved by using Google Data Studio and Google Cloud Functions. 
The second challenge was addressed by focusing on the design and organization of the dashboard, ensuring that it aligns with the client’s goals and presents the data in a clear and actionable manner. This was achieved by prioritizing the most important KPIs and positioning them at the top of the dashboard, and by presenting supporting data as charts and tables to help decision-makers make sense of the KPIBusiness ImpactThe implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicators (KPIs). Prior to the solution, the client was struggling with data fragmentation across different SaaS products and applications, which made it difficult to compile comprehensive insights. The KPI dashboard, now consolidated in Google Sheets, has streamlined this process, providing a unified view of the business metrics.This solution has also automated the data update process, saving valuable time and resources that were previously spent on manual updates. The automatic update feature has allowed the client to focus on analyzing the data rather than spending hours updating it.Additionally, the integration of the Podio data app with Google Sheets via Google Cloud Functions has improved data synchronization efficiency. Regular data synchronization ensures that the KPI dashboard is always up-to-date, providing real-time insights into the business performance.These improvements have led to enhanced decision-making processes within the client’s organization. With accurate and timely data, managers can now set and achieve goals more effectively. The consolidation of data has also facilitated cross-departmental collaboration, as teams can now access and share data easily.Overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planningProject SnapshotsProject website urlhttps://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edithttps://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboardSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241047,https://insights.blackcoffer.com/ai-driven-backend-for-audio-to-text-conversion-and-analytical-assessment-in-pharmaceutical-practice/,"AI-Driven Backend for Audio-to-Text Conversion and Analytical Assessment in Pharmaceutical Practice$Client BackgroundClient: A leading pharma tech firm in the USAIndustry Type: PharmaProducts & Services: Pharma Tech ConsultingOrganization Size: 100+The ProblemThe primary challenge is developing a backend model for an application that captures audio responses from students and employs AI to analyze the content. The backend must perform several critical functions:Convert audio to text.Transform the text into analytics KPIs.Manage login/logout operations.Handle analytics API calls.Calculate the cosine similarity of the student’s response with the expected response.Solution ArchitectureTo address this problem, we propose using Python for backend development, leveraging several key steps:Audio to Text Conversion: Utilize the SpeechRecognition library in Python for converting audio inputs into text. This library is known for its ease of use and flexibility, acting as a wrapper for several popular speech APIs 35.Text Analysis: Apply Natural Language Processing (NLP) techniques to the converted text, including sentiment analysis, readability analysis, and named entity recognition (NER). Libraries such as NLTK and SpaCy will be used for these purposes.User Authentication: Implement a secure authentication system using JWT tokens to manage login and logout operations.API Creation: Use Flask, a lightweight Python framework, to create APIs for managing user sessions and handling analytics data.Data Storage: Employ a relational database like PostgreSQL to store user session data, user profiles, and analytics data.Deployment: Deploy the application on a cloud platform such as AWS or Google Cloud.The architecture of the solution involves a backend model developed using Python, APIs for managing user sessions and analytics data, a secure user authentication system, and capabilities for converting audio to text and performing text analysis.DeliverablesBackend model developed using Python.
APIs for managing user sessions and analytics data.
Secure user authentication system.
System capable of converting audio to text.
Text analysis capabilities including sentiment analysis, readability analysis, and NER.
Deployed application on a cloud platform.Tech StackTools usedDjango React, JWT, PostgreSQL, AWS/Google Cloud.Language/techniques used
Python 3.8 or higher
Nodejs 16
Models used
SpeechRecognition for audio to text conversion, NLTK and SpaCy for text analysis.Skills usedBackend development, API creation, Text Sentiment analysis – Cosine Similarity Scoring, Machine learning (Natural Language Processing).Databases used
Sqlite3
PostgreSQL
Web Cloud Servers usedAWS/Google Cloud.What are the technical Challenges Faced during Project ExecutionChallenge: Ensuring accurate audio to text conversion, especially with poor audio quality or heavy accents.
Solution: Use a robust speech recognition library that supports multiple languages and dialects. Implement a mechanism for users to manually edit the transcribed text for accuracy.Business ImpactEnhanced Student Engagement: Providing immediate feedback on student responses can foster a more engaging learning environment.
Improved Learning Outcomes: Detailed analytics can aid educators in understanding student learning patterns and identifying areas where students struggle.
Cost Savings: Automating the conversion of audio to text and the generation of analytics can significantly reduce manual labor costs.
Scalability: The use of scalable technologies allows the system to handle increasing volumes of student responses without compromising performance.
Data Insights: The system generates valuable data insights, including sentiment scores, readability metrics, and named entity recognition counts.
Customer Satisfaction: A seamless, efficient experience for both students and educators can enhance customer satisfaction.This project is aligned with the organization’s strategic goals, and the business impact analysis will ensure that potential disruptions are identified and managed effectively.Project SnapshotsProject website urlURLs :https://www.pharmacyinterns.com.au/ Web App is running successfully on URL – http://34.30.224.139/SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241048,https://insights.blackcoffer.com/cloud-based-web-application-for-financial-data-processing-and-visualization-of-sp-500-metrics/,"Cloud-Based Web Application for Financial Data Processing and Visualization of S&P 500 Metrics$Client BackgroundClient: A leading fintech firm in the USAIndustry Type: FinanceProducts & Services: Financial Services, Trading & InvestmentOrganization Size: 100+The ProblemRequirements: The processing is done by Python 
Please provide recommendations on which cloud platform is more suitable with a comparison list of pros and cons
Please provide recommendations on which data source is suitable for receiving financial data for multiple stocks  with a comparison list of pros and cons
Recommendations on which Python libraries/packages should be used
The Web Application needs to be mobile friendly with possibility of building a responsive mobile app iOS and Android and has login capability 
Recommendation on the front-end development for a user friendly and interactive page for users  for stock trading practices.  Our SolutionThe trading training application combines the robust backend of Django integrated with yfinance for real-time financial data retrieval and analysis. Users access the platform through a secure login system, where subscription-based features offer advanced trading insights and tools. The frontend, developed in React, provides a dynamic and intuitive dashboard interface. This setup not only enhances user engagement with interactive charts, portfolio management tools, and customizable trading strategies but also ensures seamless integration between data visualization and backend functionalities. Whether users are novice investors or seasoned traders, the application serves as a comprehensive resource for learning and practicing trading strategies in a user-friendly environment.DeliverablesTrading Dashboard
Portfolio Page
Super Admin Page
Paypal Payment system in web app Tech StackTools used
Frontend and Backend Frameworks, 
Language/techniques used
Python  3.9 or higher
NodeJs 16
Yahoo finance
Django 3.2
React
Skills used
Stock Visualisation
Portfolio Management
High level Database queries
Scheduled Database queries for latest stock data
Databases used
PostgreSQL
MongoDB
Web Cloud Servers used
Google VMWhat are the technical Challenges Faced during Project ExecutionDuring the project execution, several technical challenges were encountered and successfully addressed. Integrating Yahoo Finance into the Django backend posed initial difficulties due to data formatting inconsistencies and API limitations, requiring careful parsing and error handling to ensure reliable data retrieval. Implementing real-time data updates and maintaining data consistency across different user sessions also presented challenges, necessitating the use of caching mechanisms and background tasks to optimize performance without compromising accuracy.On the frontend side, integrating React with dynamic chart libraries posed its own set of challenges, such as ensuring responsive design across various devices and browsers while managing complex data visualizations. Debugging and optimizing API calls to minimize latency and improve user experience were ongoing concerns, requiring meticulous monitoring and performance tuning throughout development. Additionally, securing user data and maintaining compliance with subscription-based access controls added complexity, necessitating rigorous testing and validation protocols to ensure data privacy and security.Overall, overcoming these technical challenges involved a combination of thorough planning, iterative development, and collaboration between backend and frontend teams. By leveraging best practices in software architecture, data management, and user interface design, the project successfully delivered a robust trading training platform that meets the needs of its users effectively and efficiently.How the Technical Challenges were SolvedThe technical challenges encountered during the project were successfully navigated through a combination of strategic planning, collaborative problem-solving, and leveraging best practices in software development. Integrating Yahoo Finance into the Django backend required meticulous handling of data parsing and API responses. We implemented robust error handling mechanisms and custom middleware to manage data inconsistencies and API rate limits effectively. Real-time data updates were achieved through the implementation of Redis caching and Celery background tasks, ensuring data remained current and accessible across user sessions while optimizing performance.On the frontend, integrating React with dynamic chart libraries posed challenges in managing responsive design and complex data visualization. We adopted CSS media queries and responsive design techniques to ensure a seamless user experience across various devices. Additionally, optimizing API calls using batching and pagination techniques helped reduce latency and enhance overall system responsiveness.Ensuring data security and maintaining compliance with subscription-based access controls were critical aspects addressed through rigorous testing and validation protocols. By iteratively testing and refining our solutions, we were able to deliver a robust trading training platform that meets the performance, scalability, and security expectations of our users. Through these efforts, we not only overcame technical challenges but also enhanced the application’s functionality and user experience, achieving our project goals effectively.Business ImpactThe successful execution of the project has had a significant positive impact on the business. It has enabled the launch of a sophisticated trading training platform that attracts and retains users seeking comprehensive financial insights and tools. The integration of real-time data from Yahoo Finance coupled with intuitive UI/UX design has enhanced user engagement and satisfaction. This has translated into increased subscription sign-ups and user retention rates, ultimately driving revenue growth. Additionally, the platform’s robust performance and security measures have fostered trust among users, positioning the business as a reliable provider in the competitive financial technology sector.Project SnapshotsProject website url
https://nhptrainer.com
Admin Panel – https://backend.nhptrainer.com/admin/SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241049,https://insights.blackcoffer.com/department-wise-kpi-tracking-dashboard-with-technician-performance-analysis-for-atoz-dependable-service/,"Department-Wise KPI Tracking Dashboard with Technician Performance Analysis for AtoZ Dependable Service$Client BackgroundClient: A leading retail tech firm in the USAIndustry Type: RetailProducts & Services: Retail BusinessesOrganization Size: 100+The ProblemThe organization has some services like sales, installs and services for many departments like drain, plumbing, excavation, hvac and bath. Organization wants to track some KPIs based on the given monthly target and tracking should be department wise or technician wise. The dashboard will show tracking the pace of KPIs so that they can understand how the organization is performing in each department in terms of number of jobs, revenue etc and also they want to understand  how technicians are working to achieve their monthly target. Dashboard has two basic views – one for admin persons and one for users only. When an admin person logged in to the web app he will see the dashboard as an iframe and he can filter date range, department, technician to see the progress of department and technicians. When a user logged in to the web app he will see the user dashboard view after  auto-filtering his name so that he can only view his data in the dashboard.Scope of this project is limited to AtoZ Dependable Service but scaleable to auto user filter in dashboard datasource. Each user will see his data only in the dashboard that brings data security and integrity.Our SolutionDefine attributes and entities that needed to determine all the KPIs for trackingGroup KPIs in categories and subcategories. For example- as per work defined in organization can be divided into three main categories called Service, Sales and Install. And each of these categories can be divided into 5 departments called- Drain, BATH, HVAC, plumbing and excavation. So any department will have the same KPIs for the same  category. You can refer the document –  https://drive.google.com/file/d/131epQtbEtvRKFdWkXq86TE35jcNe3Ntx/view?usp=sharing Determine Calculations for each KPIsDesign Dashboard in looker studio and add components need to track those KPIs and use those KPI calculations in dashboard to get actual and target valuesDetermine the Admin view and Users view of the dashboard report and create a separate dashboard for users. In admin create three pages Service, Sales and Install. For users create 3 separate dashboard each belongs to one of the main categoryAdd datasource for each of the main categories- Service, sales and install, assign these datasource to the correct page. Same data source will be reused in users dashboardDetermine what type of charts needed to full fill the client requirement, add them and configure accordinglyCreate embedded URL and provide those to the developer team to use as iframe in the primary product www.atozscoreboard.com. Create a user filter parameter (default- params) for the user dashboard and make this as json string and pass it to the embed url through back-end code. So when a user will log in to the web app they will see a filtered dashboard which will only show his filter data from data sourceDeliverables>> Front-end component – Login page
Signup Page
Change password
Homepage (Login User) – User Dashboard
Homepage (Admin) – Admin Dashboard
Add Actual forms – Install Actual, Sales Actual, Service Actual
Add Target forms – Install Target, Sales Target, Service Target
List View – Install Actual, Sales Actual, Service Actual
List View –  Install Target, Sales Target, Service Target>> Back-end components (in main/views.py)Login View
Signup View
Change Password View
Reset Password By Email View (Email service needed)
Install CreateView (UpdateView DeleteView implemented but not used)
Sales CreateView (UpdateView DeleteView implemented but not used)
Service CreateView (UpdateView DeleteView implemented but not used)
Install Target CreateView Updateview DeleteView
SalesTarget CreateView Updateview DeleteView
ServiceTarget CreateView Updateview DeleteView
STATIC_FOLDER in VM = ‘/demo/www/public/web_app’Tech StackTools used
Django3.2,  LookerStudio
Language/techniques used
Python, Html5, CSS, Jquery, Bootstrap4
Skills used
Data visualisation, Database Optimized Queries, Form Validations, Tracking Algorithms
Databases used
PostgreSQL
Web Cloud Servers used
Linode VMWhat are the technical Challenges Faced during Project Execution  Admin Dashboard:Overview: Provides an overall view of the monthly performance of all departments.
Tracking: Allows admins to track the progress and performance of individual employees within each department.
Analytics: Offers detailed analytics and reports on departmental and employee performance metrics.  Employee Dashboard:Goals and Targets: Displays specific goals and targets for employees to achieve.
Performance Tracking: Enables employees to monitor their progress towards their goals.How the Technical Challenges were SolvedDashboard has two basic views – one for admin persons and one for users only. When an admin person logged in to the web app he will see the dashboard as an iframe and he can filter date range, department, technician to see the progress of department and technicians. When a user logged in to the web app he will see the user dashboard view after  auto-filtering his name so that he can only view his data in the dashboard.Scope of this project is limited to AtoZ Dependable Service but scaleable to auto user filter in dashboard datasource. Each user will see his data only in the dashboard that brings data security and integrity. To add Actual Data User can add data in actual form of their assigned category. And only Admin can add actual entry of any user. Also Admin can Add and  view targets of install, Sales and Service. After adding month targets, Users can see their targets and user can track their own performance and Admin can track all users performance.Business ImpactImplementing a comprehensive tracking system with both an admin dashboard and an employee dashboard will have significant positive impacts on the business, including:Enhanced Performance Management:Improved Oversight: Admins can easily monitor departmental and individual performance, ensuring that goals are met and identifying areas for improvement.
Data-Driven Decisions: Access to detailed analytics enables informed decision-making, allowing for timely interventions and resource allocation.
Increased Employee Accountability:Clear Objectives: Employees have a clear understanding of their goals and targets, fostering a sense of responsibility and ownership over their work.
Real-Time Feedback: Continuous tracking and feedback help employees stay on track and make necessary adjustments to meet their objectives.
Boosted Productivity:Motivation: Transparent performance metrics and progress tracking motivate employees to perform at their best.
Goal Alignment: Ensures that individual goals are aligned with departmental and organizational objectives, driving overall productivity.
Enhanced Communication and Transparency:Open Communication: Facilitates open communication between management and employees regarding performance expectations and achievements.
Transparency: Promotes a culture of transparency, where employees are aware of how their contributions impact the organization’s success.
Operational Efficiency:Streamlined Processes: Reduces the administrative burden of performance tracking, freeing up time for more strategic activities.
Proactive Management: Enables proactive management of performance issues before they escalate, maintaining high levels of operational efficiency.Overall, the implementation of this tracking system will lead to a more motivated and productive workforce, better alignment of individual and organizational goals, and ultimately, enhanced business performance and growth.Project SnapshotsProject website urlAtoZDashboard Web App –  atozscoreboard.comAtoZDashboard-Admin  –  https://www.atozscoreboard.com/admin/AtoZDashboard-Admin –  https://lookerstudio.google.com/reporting/1cbee37f-4b0f-44dc-b8a3-3a7bc2fce6adAtoZDashboard-Install_User –  https://lookerstudio.google.com/reporting/c7158228-9aec-49d7-9ade-c10e5b96930fAtoZDashboard-Sales_User –  https://lookerstudio.google.com/reporting/edd0ca07-e841-471a-af1d-e9c9d4429ae2AtoZDashboard-Service_User –  https://lookerstudio.google.com/reporting/e281ee62-bb6b-454f-9f30-b6b839f7c081SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241050,https://insights.blackcoffer.com/steps-to-convert-a-node-js-api-to-python-for-aws-lambda-deployment/,"Steps to Convert a Node.js API to Python for AWS Lambda Deployment$Client BackgroundClient: A leading consulting firm in the USAIndustry Type: IT & ConsultingProducts & Services: IT Consulting, Tech SupportOrganization Size: 50+The ProblemConverting a Node.js API to Python code for AWS Lambda involves several key steps. First, create a new Lambda function and select Python as the runtime. Then, translate the Node.js handler function to Python, ensuring it performs the same logic. Next, include any necessary Python libraries in your deployment package. Use AWS API Gateway to map HTTP requests to your Lambda function. Finally, package your code and dependencies, upload them to Lambda, and thoroughly test to ensure everything works as expected.Solution Architecture  Setup AWS Lambda Environment:Create a new Lambda function in AWS Management Console.
Choose Python as the runtime environment.  Translate Handler Function:Convert the Node.js handler function to Python. This function is the entry point for your Lambda.  Update Dependencies:Include necessary Python libraries in your Lambda deployment package, similar to Node.js dependencies.  Configure API Gateway:Use AWS API Gateway to map HTTP requests to your Lambda function.  Deploy:Package your Python code and dependencies into a zip file and upload it to Lambda.
Test and debug to ensure the Lambda function works as expected.DeliverablesNew optimised python lamda function Tech StackTools used
AWS Lambda Management Console
AWS API Gateway
AWS CLI (Command Line Interface)
AWS SAM (Serverless Application Model)
Boto3 (AWS SDK for Python)Language/techniques usedPython
AWS Lambda Functions
AWS API Gateway
Handler Functions
Packaging and DeploymentSkills used
Programming (Python)
Serverless Computing (AWS Lambda)
API Management (AWS API Gateway)
Deployment Automation (AWS CLI or AWS SAM)
Cloud SDK (Boto3 for AWS)
Databases used
Amazon RDS
Web Cloud Servers used
AWSWhat are the technical Challenges Faced during Project ExecutionDuring project execution, common technical challenges include managing dependencies and versions, ensuring compatibility across different environments (development, testing, production), handling scalability and performance issues, integrating with third-party services or APIs securely, and maintaining robust error handling and logging practices for debugging and monitoring purposes. These challenges require careful planning, thorough testing, and proactive troubleshooting to ensure the project’s success.How the Technical Challenges were SolvedTechnical challenges were addressed through rigorous testing and version control to manage dependencies effectively. Scalability and performance issues were tackled by optimizing code and leveraging cloud services like AWS Lambda. Secure integration with third-party APIs was ensured through encryption and authentication protocols. Robust error handling and logging mechanisms were implemented to monitor and debug issues promptly, ensuring smooth project execution and reliability.Business ImpactThe successful resolution of technical challenges led to improved system reliability, enhanced performance, and reduced operational costs. This resulted in increased customer satisfaction, streamlined business operations, and strengthened competitive advantage in the market.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241051,https://insights.blackcoffer.com/building-an-analytics-dashboard-with-a-pdf-parsing-pipeline-for-data-extraction/,"Building an Analytics Dashboard with a PDF Parsing Pipeline for Data Extraction$Client BackgroundClient: A leading tech development firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Services, Product DevelopmentOrganization Size: 100+The ProblemCreate an analytics dashboard using pdf parsing pipelineBusinesses often deal with vast amounts of unstructured data stored in PDF documents, such as invoices, reports, contracts, and financial statements. Extracting meaningful insights from these documents manually is a time-consuming and error-prone process. Without an efficient system, it becomes challenging to transform unstructured PDF data into actionable insights, leading to inefficiencies in decision-making and delayed business processes.The problem is further compounded when trying to integrate this data into real-time analytics platforms or dashboards. Organizations need a streamlined process to automatically extract, clean, and analyze the information from PDFs and display it in an intuitive dashboard for immediate insights.Key Challenges:Data Extraction: Parsing PDFs to accurately extract structured and unstructured data, including text, tables, and images.
Data Transformation: Converting extracted data into a usable format for analytics, while addressing inconsistencies and errors.
Integration: Automating the integration of the parsed data into an analytics dashboard, allowing for real-time updates and easy access.
Visualization: Presenting the extracted data in an interactive and user-friendly analytics dashboard for meaningful insights and better decision-making.The solution must focus on building a robust PDF parsing pipeline that automates the process of extracting and transforming data from PDFs into a dynamic analytics dashboard for faster and more accurate decision-making.Our SolutionData Science Tasks:1. **PDF Parsing Pipeline:**   – Use libraries like PyPDF2, pdfplumber, or Camelot to extract data from PDF documents.   – Implement logic to handle different PDF layouts and extract relevant information efficiently.2. **Data Preprocessing:**   – Clean and preprocess the extracted data to handle noise, missing values, and inconsistencies.   – Convert extracted data into a structured format (e.g., pandas DataFrame) for further processing.3. **Formula Calculation:**   – Implement formulas or algorithms to perform calculations on the preprocessed data.   – Calculate derived metrics, perform aggregations, or apply statistical analyses to derive insights.Backend Tasks (Flask API):1. **Setup Flask Application:**   – Create a Flask application to serve as the backend API.   – Define routes to handle incoming requests and serve processed data.2. **PDF Parsing and Preprocessing:**   – Implement PDF parsing pipeline within Flask application.   – Preprocess the extracted data, including cleaning, normalization, and structuring.3. **Formula Calculation:**   – Implement endpoints in Flask API to perform formula calculation on the preprocessed data.   – Expose API endpoints to serve calculated metrics to the frontend.4. **Data Serving:**   – Serve processed data and calculated metrics to the frontend through API endpoints.   – Ensure proper error handling and response formatting for API requests.5. **Authentication and Authorization:**   – Handle authentication and authorization if required.   – Implement user authentication mechanisms to secure API endpoints.6. **Deployment:**   – Deploy Flask API to a hosting service like AWS.   – Configure server environment and ensure the backend API is accessible over the internet.Frontend Tasks (React.js):1. **Setup React.js Application:**   – Create a React.js application to serve as the frontend interface for the analytics dashboard.   – Use Create React App or similar tools to bootstrap your React project.2. **API Integration:**   – Fetch data from the Flask API endpoints using Fetch API.   – Handle responses and update React components with fetched data.3. **Formula Calculation:**   – Integrate formula calculation results into React components to display calculated metrics.   – Update UI elements dynamically based on formula calculation outcomes.4. **Data Visualization:**   – Create interactive data visualizations using libraries like Chart.js, Plotly.js, or D3.js.   – Render charts, graphs, or other visualizations based on the processed data and calculated metrics.5. **User Interaction and Navigation:**   – Implement user interaction features such as dropdowns, filters, and date pickers.   – Handle navigation between different dashboard pages or views within your React application.6. **State Management:**   – Manage application state using React’s built-in state or state management libraries like Redux.   – Keep track of data fetched from the backend API, user interactions, and application state changes.7. **Deployment:**   – Deploy React.js frontend to a static hosting service.   – Build your React application for production and configure deployment settings as needed.Solution Architecture1. Frontend (React.js):– **Components**: Create React components for different parts of the dashboard, such as data visualization, user interaction elements, and navigation.– **API Integration**: Use Axios or Fetch API to make HTTP requests to Flask API endpoints from the React frontend. Fetch data from backend API endpoints for display in the dashboard components.– **Data Visualization**: Utilize libraries like Chart.js, Plotly.js, or D3.js to create interactive data visualizations within React components. Render charts, graphs, or other visualizations based on the data fetched from the Flask API.– **User Interaction**: Implement user interaction features such as dropdowns, filters, and date pickers to allow users to customize their analytics views. Handle user input and update dashboard components accordingly.– **State Management**: Manage application state using React’s built-in state management or state management libraries like Redux. Keep track of data fetched from the backend API, user interactions, and application state changes.– **Deployment**: Deploy the React.js frontend to a static hosting service like Netlify or Vercel. Build the React application for production and configure deployment settings as needed.2. Backend (Flask API):– **Flask Application**: Create a Flask application to serve as the backend API for the analytics dashboard.– **API Endpoints**: Define routes in the Flask application to handle incoming requests from the frontend and serve processed data. Implement endpoints for PDF parsing, preprocessing, formula calculation, and data serving.– **PDF Parsing Pipeline**: Implement PDF parsing pipeline within the Flask application using libraries like PyPDF2, pdfplumber, or Camelot. Extract data from PDF documents and preprocess it for further analysis.– **Data Preprocessing**: Clean and preprocess the extracted data to handle noise, missing values, and inconsistencies. Convert extracted data into a structured format (e.g., pandas DataFrame) for further processing.– **Formula Calculation**: Implement formulas or algorithms to perform calculations on the preprocessed data. Calculate derived metrics, perform aggregations, or apply statistical analyses to derive insights.– **Data Serving**: Serve processed data and calculated metrics to the frontend through API endpoints. Ensure proper error handling and response formatting for API requests.– **Authentication and Authorization**: Handle authentication and authorization if required. Implement user authentication mechanisms to secure API endpoints.– **Deployment**: Deploy the Flask API to a hosting service like Heroku or AWS. Configure the server environment and ensure the backend API is accessible over the internet.3. Integration:– **API Communication**: Enable communication between the frontend and backend by making HTTP requests from React components to Flask API endpoints. Fetch data from backend API endpoints and update the frontend components with fetched data.– **Data Flow**: Ensure smooth data flow between the frontend and backend, with proper handling of data formats, errors, and exceptions.– **Error Handling**: Implement error handling mechanisms to deal with errors and exceptions that may occur during data fetching, processing, or communication between frontend and backend.– **Testing and Debugging**: Test the integration between frontend and backend components to ensure proper functionality and identify any issues or bugs that need to be addressed.DeliverablesAnalytics Dashboard WebpageTech StackTools used
 PDF Parsing:   – pdfplumberData Preprocessing:– Pandas  – NumPyWeb Framework:    – FlaskFrontend:   – React.js   – Redux   – Chart.js
   – Plotly.jsDeployment:   – AWS (Amazon Web Services)Language/techniques used
Data Science:
– Language: Python
– Techniques: PDF Parsing (PyPDF2, pdfplumber, Camelot), Data Preprocessing (Pandas, NumPy)
Backend:
– Language: Python (Flask API development)
– Techniques: Web Framework (Flask), API Development (Flask-RESTful, Flask-RestPlus)
Frontend:
– Language: JavaScript 
– Techniques: JavaScript Library (React.js), State Management (React Context API, Redux), Data Visualization (Chart.js, Plotly.js, D3.js), HTTP Requests (Fetch API)
Deployment and Hosting:
– Techniques: Backend Deployment ( AWS)
Skills usedData Science:– PDF Parsing: Proficiency in extracting data from PDF documents using libraries like PyPDF2, pdfplumber, and Camelot.– Data Preprocessing: Skills in cleaning and processing data using Pandas and NumPy, including handling missing values, noise, and inconsistencies.Backend:– Web Development: Proficiency in Python and Flask for backend API development.– API Development: Skills in designing and implementing RESTful APIs using Flask-RESTful or Flask-RestPlus.Frontend:– JavaScript: Proficiency in ES6+ JavaScript for frontend development.– React.js: Skills in building interactive user interfaces and managing state with React.js.– Data Visualization: Ability to create visualizations using libraries like Chart.js, Plotly.js, and D3.js.– HTTP Requests: Proficiency in making asynchronous HTTP requests using Axios or Fetch API.Deployment and Hosting:– Deployment: Skills in deploying applications to cloud platforms like Heroku, AWS, Netlify, and Vercel.– Server Management: Knowledge of managing server environments and configuring deployment settings.What are the technical Challenges Faced during Project ExecutionData Security: Ensuring the security of sensitive data, especially when handling personally identifiable information (PII) or confidential business data, requires implementing proper encryption, access controls, and compliance with data protection regulations.Testing and Debugging: Identifying and resolving bugs, errors, and performance issues throughout the development lifecycle requires thorough testing and debugging processes, including unit tests, integration tests, and end-to-end testing.How the Technical Challenges were SolvedData Security:Implement encryption mechanisms to protect sensitive data at rest and in transit. Enforce strict access controls and role-based permissions to limit access to sensitive data only to authorized users.Testing and Debugging:Develop comprehensive test suites covering unit tests, integration tests, and end-to-end tests to identify and prevent bugs and errors.Utilize debugging tools and techniques to troubleshoot issues and optimize application performance effectively.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241052,https://insights.blackcoffer.com/building-a-real-time-log-file-visualization-dashboard-in-kibana/,"Building a Real-Time Log File Visualization Dashboard in Kibana$Client BackgroundClient: A leading IT Tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, Support, IT DevelopmentOrganization Size: 300+The ProblemTo create a dashboard that visualizes log files in KibannaOrganizations often generate massive volumes of log files from various systems and applications, which contain crucial information about system performance, errors, security events, and user activities. However, manually analyzing these log files can be time-consuming and inefficient, especially when attempting to identify patterns, anomalies, or potential issues in real time.The challenge is to create a centralized dashboard in Kibana that can efficiently visualize log files, enabling users to monitor system health, detect anomalies, and analyze logs quickly. This solution must support real-time data updates, offer customizable visualizations, and provide users with the ability to filter and drill down into specific log events to enhance operational visibility and decision-making.Our Solution1. Export Log Data:   – Export the log data from Kibana or your logging system into a file format that Python can read. Common formats include CSV, JSON, or plain text.2. Load Log File in Python Script:   – Use Python’s file handling capabilities to read the log file into your script. 3. Extract Error Codes Using Regular Expressions:   – Use regular expressions to extract error codes from each log entry. Define a pattern that matches the format of your error codes. For example.4. Count Log Codes:   – Count the occurrences of each error code using Python’s collections. Counter or a similar method. 5. Export Processed Data to Kibana:   – Export the processed data (error codes and their counts) to a format that Kibana can ingest. We  exported the data to Elasticsearch directly using the Elasticsearch Python client, or you can save it to a file (e.g., CSV) and import it into Kibana manually.6. Visualize Data in Kibana:   – Once the data is available in Kibana, create visualizations (e.g., bar charts, pie charts) based on the error code counts. You can also create dashboards to combine multiple visualizations and monitor the error trends over time.Solution ArchitectureHere’s a solution architecture for the workflow:1. Log Data Export:   – Log data is exported from Kibana or the logging system into a file format such as CSV, JSON, or plain text.2. Python Script Execution:   – A Python script is executed to process the exported log data.3. Data Processing in Python:   – The Python script reads the log file and extracts error codes using regular expressions.   – Error codes are counted to determine their frequency.4. Export Processed Data:   – The processed data (error codes and their counts) is exported to a format suitable for ingestion into Kibana.6. Ingestion into Kibana:   – The processed data is ingested into Kibana. This can be done either directly into Elasticsearch (the backend datastore of Kibana) using the Elasticsearch Python client or by importing the data into Kibana manually.7. Visualization in Kibana:   – In Kibana, the ingested data is used to create visualizations such as bar charts, pie charts, or any other suitable visualization to represent the count of log error codes.   – Dashboards can be created to combine multiple visualizations and provide a comprehensive view of the log error trends over time.DeliverablesKibana DashboardTech StackTools used
   -Elasticsearch, Logstash, or Beats (ELK stack).

   – Python interpreter, VSCode, Jupyter Notebook.

   – Python with libraries such as `re`, `collections`, and `pandas`.

   – `matplotlib` or `seaborn` for creating visualizations.

   – CSV, JSON, or other suitable formats.

   – Elasticsearch Python client or manual import via Kibana’s interface.

   – Built-in visualization and dashboarding capabilities of Kibana.Language/techniques used
– Language: Python is primarily used for scripting and data processing due to its flexibility, rich ecosystem of libraries, and ease of use.
  – Regular Expressions (Regex): Utilized for pattern matching and extracting error codes from log data efficiently.
  – Data Manipulation: Techniques such as filtering, grouping, and counting are employed to process and analyze log data effectively.
  – Visualization: Matplotlib or Seaborn libraries are employed for creating visual representations of log error code counts, facilitating data interpretation and analysis.Skills used– Python Programming: Proficiency in Python programming language for scripting, data processing, and visualization tasks.
– Regular Expressions: Skill in using regular expressions to efficiently extract relevant information, such as error codes, from log data.
– Data Processing: Ability to manipulate and analyze log data using libraries like `re` for regular expressions and `pandas` for data manipulation.
– Data Visualization: Proficiency in creating visualizations using libraries such as Matplotlib or Seaborn to represent log error code counts in an understandable and insightful manner.What are the technical Challenges Faced during Project Execution1. Data Preprocessing:   – Challenge: Log data often arrives in unstructured or semi-structured formats, requiring preprocessing steps such as data cleaning, parsing, and normalization. Inconsistencies in log formats across different systems can further complicate preprocessing efforts.2. Tool Integration:   – Challenge: Integrating different tools and technologies within the tech stack seamlessly can be challenging. For example, connecting Python scripts responsible for log data processing with Elasticsearch for data ingestion into Kibana requires careful configuration and compatibility considerations.How the Technical Challenges were Solved1. Data Preprocessing:   – Solution: Develop robust preprocessing pipelines using tools like Python’s `pandas` library or scripting languages to clean and parse log data. Implement techniques such as regular expressions to extract relevant information from log entries. Utilize data wrangling techniques to handle inconsistencies and outliers effectively.2. Tool Integration:   – Solution: Utilize APIs, SDKs, or libraries provided by the tools to facilitate integration. Ensure compatibility between different components of the tech stack by adhering to supported versions and protocols. Leverage middleware solutions or data integration platforms to streamline communication and data flow between disparate systems. Regularly test and validate integrations to identify and address any compatibility issues proactively.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241053,https://insights.blackcoffer.com/analyzing-the-impact-of-female-ceo-appointments-on-company-stock-prices/,"Analyzing the Impact of Female CEO Appointments on Company Stock Prices$Client BackgroundClient: A leading research institution worldwideIndustry Type: R&DProducts & Services: R&D, Higher EducationOrganization Size: 10000+The ProblemTo find out if the appointment of a female CEO’s affected the stock prices of that companyThe appointment of a new CEO is often a significant event for a company, potentially influencing its strategic direction, investor confidence, and market performance. This study seeks to determine whether the appointment of a female CEO has a measurable impact on the company’s stock prices. Specifically, the research aims to explore if there is a statistically significant change in stock price trends following the appointment of a female CEO, and whether the market reacts differently compared to appointments of male CEOs. Understanding this relationship can provide insights into investor perceptions and the broader implications of leadership diversity on corporate performance.Our SolutionTo analyze whether the appointment of a female CEO affected the stock prices of a company, we followed these steps:1. Define the Hypotheses:   – Null Hypothesis (H0): The appointment of a female CEO has no effect on the stock prices of the company.   – Alternative Hypothesis (H1): The appointment of a female CEO affects the stock prices of the company.2. Data Collection:   – Gather data on the stock prices of the company before and after the appointment of the female CEO.   – Make sure to gather data for a sufficient number of companies to ensure reliability and to control for other factors that might affect stock prices.3. Data Analysis:   – Calculate the average stock prices before and after the appointment of the female CEO.   – Conduct a statistical test to determine if there is a significant difference between the two averages.   – Statistical tests for this scenario include the t-test or ANOVA.4. Interpretation:   – If the p-value from the statistical test is less than the significance level (typically 0.05), then you reject the null hypothesis and conclude that the appointment of a female CEO has a significant effect on the stock prices of the company.   – If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting that there is no significant effect of the appointment of a female CEO on the stock prices.5. Reporting Results:   – Presented our findings, including the statistical analysis and interpretation, in a clear and concise manner.   – Discussed the implications of the results and any further research that may be warranted.Solution ArchitectureWe implemented time-series analysis solution using various libraries. Here’s a architecture using Python:1. Data Collection:   – Use libraries like Pandas, NumPy, or Requests to collect and store historical data from relevant sources. For financial data, we used APIs like Alpha Vantage or Yahoo Finance.2. Data Preparation and Exploration:   – Utilize Pandas for data manipulation and exploration. This includes tasks such as cleaning data, handling missing values, and visualizing time-series data using Matplotlib or Seaborn.3. Modelling:   – Depending on the characteristics of our data, we chose an appropriate time-series model from libraries like StatsModels or Prophet (for forecasting). For more advanced models like deep learning-based approaches, you might use TensorFlow or PyTorch.4. Parameter Estimation:   – Use built-in functions within the chosen libraries to estimate the parameters of the selected model. For example, StatsModels provides functions for fitting ARIMA models.5. Model Evaluation:   – Evaluate the performance of your model using appropriate metrics. we implemented custom evaluation functions or use built-in functions from libraries like scikit-learn.6. Forecasting:   – Use the fitted model to make predictions for future time points. Most time-series libraries provide functions to easily forecast future values.7. Interpretation and Visualization:   – Interpret the results of our analysis and visualize the forecasts along with historical data using Matplotlib, Seaborn.DeliverablesHypothesis testingStatistical ModelsBusiness InsightsTech StackTools used
  – Python
  – VS Code
  – Pandas for data manipulation
  – Matplotlib or Seaborn for data visualization
  – StatsModels or Prophet for time-series modelling
  – scikit-learn for model evaluation 
Language/techniques used
  – Python for programming
  – Time-series analysis techniques such as ARIMA, Exponential Smoothing, etc.
 – Data preprocessing techniques including handling missing values, data normalization, etc.
Models used
ARIMA 
Skills used
– Data manipulation and preprocessing using Pandas
  – Time-series modeling and analysis
  – Statistical analysis and interpretation
  – Data visualization using Matplotlib or Seaborn
  – Machine learning techniques for model evaluation
  – Python programming skillsWhat are the technical Challenges Faced during Project Execution1. Seasonality and Trend Detection:   – Challenge: Identifying and modeling complex seasonality and trends in the data accurately.2. Forecasting Uncertainty:   – Challenge: Providing accurate uncertainty estimates for time-series forecasts is crucial but challenging.How the Technical Challenges were Solved1. Seasonality and Trend Detection:   – Solution: Utilize techniques such as seasonal decomposition (e.g., STL decomposition), which decomposes the time series into seasonal, trend, and residual components, making it easier to model each component individually. Differencing can also be applied to remove trends or seasonality, making the data stationary and easier to model. Advanced time-series models like Seasonal ARIMA (SARIMA) or seasonal Exponential Smoothing methods can capture seasonal patterns and trends effectively by incorporating seasonal parameters into the model structure.2. Forecasting Uncertainty:   – Solution: Implement probabilistic forecasting methods such as Bayesian inference, which allows for the estimation of probability distributions for future observations. Monte Carlo simulations involve generating multiple forecast paths by sampling from the predictive distribution, providing a range of possible outcomes and their associated probabilities. Bootstrapping is another technique where multiple bootstrap samples are drawn from historical data, and forecasts are generated for each sample, enabling the calculation of prediction intervals and quantifying forecast uncertainty. These methods help in quantifying and communicating the uncertainty inherent in time-series forecasts, aiding decision-making processes.SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy"
Netclan20241054,https://insights.blackcoffer.com/ai-chatbot-using-llm-langchain-llama/,"AI Chatbot using LLM, Langchain, LLama$ObjectiveThe primary objective of the is to develop a highly efficient AI chatbot tailored for eye care patients. The chatbot will assist in booking appointments, tracking the status of lens orders, reviewing patient dues, sending statements, and answering general questions about their exams and the practice. It will integrate custom-trained QLoRA models using open-source LLMs, Twilio for SMS communication, and Retrieval-Augmented Generation (RAG) for handling confidential data using vector databases like ChromaDB. The AI related APIs will be developed using FastAPI/Flask, and additional functionalities such as booking, appointment handling, dues management, and order tracking will be managed by the backend system.Solution ArchitectureSolution Overview ➖The solution architecture is designed to integrate various components to provide a seamless user experience. The architecture includes:Custom QLoRA Model Training: Training open-source models like LLaMA and Mixtral with domain-specific data to ensure accurate and relevant responses.
Twilio SMS Integration: Utilizing Twilio’s API to send and receive SMS for appointment confirmations, reminders, and other notifications.
RAG for Confidential Data: Implementing Retrieval-Augmented Generation (RAG) using ChromaDB for secure and efficient access to confidential data.
API Development with FastAPI: Building RESTful APIs using FastAPI to handle communication between the frontend and backend, and to manage data transactions.
Backend Functionality: Handling all other functionalities such as booking, appointments, dues management, and order tracking through robust backend API services.QLoRA Model TrainingQLora- QLoRA is the extended version of LoRA which works by quantizing the precision of the weight parameters in the pre-trained LLM to 4-bit precision. Typically, parameters of trained models are stored in a 32-bit format, but QLoRA compresses them to a 4-bit format. This reduces the memory footprint of the LLM, making it possible to finetune it on a single GPU. This method significantly reduces the memory footprint, making it possible to run LLM models on less powerful hardware, including consumer GPUs.The QLoRA model training involves the following steps:Data Collection: Gather domain-specific data including FAQs, appointment details, and patient interactions.
Preprocessing: Clean and preprocess the data to ensure it is suitable for model training.
Training: Utilize QLora training platforms to train the LLaMA or Mistral models on GPU resources.
Fine-Tuning: Fine-tune the model using QLora Models.
Evaluation: Evaluate the model performance and make necessary adjustments.Technology Stack➖Programming Language: Python, Javascript
AI API: FastAPI
Backend: Django
LLM Models: LLaMA/ Mixtral
Database: Postgres
Vector Database: ChromaDB (for vector storage)
API Framework: FastAPI
SMS Integration: Twilio
Hosting: AWS/ GCP/AzureLLM Selection➖LLM Selection: Mistral 7B / Llama 2 7B / Llama 3 8BSelection CriteriaThe selection of the LLM (Large Language Model) will be based on the performance evaluation of three open-source models: Mistral 7B, Llama 2 7B, and Llama 3 8B. The primary criteria for selection include:Performance: The accuracy and relevance of responses during testing phases.
Resource Efficiency: Ability to run efficiently on CPUs and low VRAM GPUs.
Scalability: Ease of scaling the model for increased usage without significant degradation in performance.
Compatibility: Integration with existing infrastructure and ease of use in the deployment environment.Testing and EvaluationEach model will be subjected to a series of tests designed to measure their performance in real-world scenarios. These tests will include:Accuracy Tests: Evaluating the correctness and relevance of the responses provided by the models using a diverse set of queries.
Resource Utilization Tests: Monitoring CPU and GPU usage to ensure models run efficiently on limited resources.
Latency Tests: Measuring response times to ensure the chatbot can handle real-time interactions smoothly.
Scalability Tests: Testing the models under increased load to ensure they can handle a growing number of users without performance issues.Models Under Consideration and Reasoning Behind Their SelectionMistral 7BOverview: Mistral 7B is known for its efficiency and ability to provide accurate responses with lower computational requirements.
Strengths: High accuracy and relevance in responses, efficient use of computational resources, and good scalability.
Use Cases: Ideal for scenarios requiring fast and accurate responses with minimal resource usage.
Llama 2 7BOverview: Llama 2 7B builds on the strengths of its predecessor with improvements in performance and efficiency.
Strengths: Enhanced accuracy, better resource utilization, and improved response times.
Use Cases: Suitable for deployments where slightly better performance is required without significantly increasing resource consumption.Llama 3 8BOverview: The latest in the Llama series, Llama 3 8B offers the highest performance among the three models under consideration.
Strengths: Superior accuracy and relevance, efficient performance on low VRAM GPUs, and excellent scalability.
Use Cases: Best for applications needing the highest accuracy and capable of handling more complex queries while still being resource-efficient.Final SelectionThe final selection will be made based on the comprehensive evaluation of the models during the testing phase. The model that demonstrates the best overall performance in terms of accuracy, efficiency, and scalability will be chosen for deployment. This approach ensures that the chosen model will not only meet the current requirements but will also be capable of scaling with future needs, providing a robust and reliable solution for the AI chatbot.By focusing on models that are optimized for both CPUs and low VRAM GPUs, we ensure cost-effective deployment and operation, making the solution accessible and sustainable for a wide range of applications.Milestone Documentation➖Milestone 1: Initial Setup and Model TrainingSetup development environment
Collect and preprocess data
Train and fine-tune the QLoRA model
Test various Open Source Models and evaluate their performance.
Develop basic API endpoints using FastAPIMilestone 2: Frontend/Backend Development and IntegrationImplement frontend and backend functionalities for booking, appointments, dues management, and order tracking
Integrate ChromaDB for confidential data handling and RAG.
Create API endpoints to support full functionality
Conduct initial testing and validation
Integrate Twilio for SMS functionalityMilestone 3: Payment Gateway IntegrationIntegrate payment gateway.
Implement payment processing for premium features.Milestone 4: Deployment and TestingDeploy the solution on cloud infrastructure
Conduct thorough testing to ensure reliability and performance
Implement periodic test transitions and demos
Optimize for production and prepare for launchMilestone 5: Deployment and TestingDeploy the solution on cloud infrastructure
Conduct thorough testing to ensure reliability and performance
Implement periodic test transitions and demos
Optimize for production and prepare for launchMilestone 6: Documentation & MaintenancePerform the final round of testing and bug fixes.
Prepare documentation and training materials.
Provide post-launch support and maintenance."
Netclan20241055,https://insights.blackcoffer.com/healthcare-ai-chatbot-using-llama-llm-langchain/,"Healthcare AI ChatBot using LLAMA, LLM, Langchain$Client BackgroundClient: A leading healthcare tech firm in the USAIndustry Type: HealthcareProducts & Services: Healthcare SolutionOrganization Size: 1000+Problem StatementOverview:The AI chatbot is required to assist eye care patients with various tasks, including booking appointments, tracking lens order statuses, reviewing patient dues, sending statements, and answering general questions about their eye exams and the practice.Requirements:Use of open-source LLM models like Llama, with training on custom data.
Implementation of Q&A from custom static data.
SaaS deployment to serve multiple hospitals/practices.
Demonstration of sample chats using the custom-trained model early in the project.
Deployment on optimized servers (preferably CPU-based for execution, GPU-based acceptable for training).
Periodic testing, transition, and demos before payment release.
Provision of APIs for communication with databases to fetch dynamic data.SolutionSolution Overview: Develop and deploy a custom-trained AI chatbot using an open-source LLM model (such as Llama) to assist eye care patients. This solution will be implemented as a SaaS offering, enabling multiple hospitals and practices to utilize the chatbot. The project will include the training of the LLM model on custom data, development of APIs for dynamic data interaction, and deployment on optimized servers.Steps:Model Training: Train the Llama model with custom data specific to eye care patient queries.
API Development: Develop APIs to communicate with the hospital’s database for dynamic data retrieval (appointment booking, order status, patient dues).
Deployment: Deploy the chatbot as a SaaS solution using optimized servers (CPU for execution, GPU for training).
Testing and Demo: Conduct periodic testing and demos to ensure functionality and transition smoothly.
Sample Chat Demonstration: Provide sample chats to demonstrate the effectiveness of the custom-trained model.Technology UsedLLM Model:Llama or another suitable open-source LLM model, trained with custom data only.Backend:Python (preferred), with a possibility of using C# for backend implementation if feasible.Deployment:Optimized CPU-based servers for execution, with GPU-based servers for initial model training.APIs:Custom-developed APIs for database interaction.DeliverablesCustom-Trained LLM Model:A Llama model trained with custom eye care data.
APIs:APIs to handle appointment bookings, order status checks, patient dues reviews, and statement sending.
SaaS Deployment:Deployment of the chatbot as a SaaS solution, accessible by multiple hospitals/practices.
Sample Chats:Demonstration of the chatbot’s effectiveness with sample chats using the custom-trained model.
Documentation:Detailed documentation of the technology stack, APIs, deployment steps, and user guide.Business ImpactImproved Patient Experience:Patients can easily book appointments, track orders, and get answers to their queries, leading to higher satisfaction.
Operational Efficiency:Automation of routine tasks reduces the workload on staff, allowing them to focus on more critical tasks.
Cost Savings:Utilizing open-source tools and optimized servers minimizes costs, avoiding recurring expenses associated with licensed software.
Scalability:SaaS deployment allows multiple hospitals/practices to use the chatbot, enhancing scalability and reach.
Data-Driven Insights:Custom APIs enable real-time data interaction, providing valuable insights into patient interactions and operational metrics.By leveraging the power of open-source LLM models and custom training, this AI chatbot will significantly enhance patient interaction and operational efficiency for eye care providers.Sample Chat ScenariosMilestone 1 Chat SamplesAppointmentsSample 101: Booking an AppointmentPatient:Hello, this is John Doe. I would like to book an appointment at the eyecare location as soon as possible. I’m experiencing issues with my vision. Here are my details:Name: John Doe
DOB: January 1, 1980
Preferred Appointment Date: June 10, 2024
Preferred Time: Morning, if possible
Reason for Visit: Blurred vision and eye discomfortPlease call me back at your earliest convenience to confirm the appointment or suggest an alternative time. You can reach me at the contact number provided above. Thank you.Best regards,John DoeChatBot:Thank you for sharing the details. Could you please provide a phone number and email address?Patient:555123456, hello@blackcoffer.comChatBot:Can you please confirm the following details?Phone Number: 555123456
Email: hello@blackcoffer.comPatient:Yes, it’s correct.ChatBot:Thanks for the details. We have sent you an OTP on the registered email and phone number. Can you please enter the OTP?Patient:1111<<If location name is not found in our location API, the chatbot will ask for location again. If found, the chatbot saves the location ID for further use.>>ChatBot:Thank you for the validation. Please choose a provider from the list:Ales Bru
Juhn Smith
Adam BlakePatient:I’ll go with Ales Bru.<<Validation of entered data, check if all required details are given. Check for available slots with provided preference.>>ChatBot:Thanks, I see the following open slots on June 10, 2024:10:00 AM – 10:20 AM
10:20 AM – 10:40 AM
12:20 PM – 12:40 PMPatient:I’ll take the 10:00 AM slot.ChatBot:Alright! Can you please confirm the following details?Patient Information:Name: John Doe
Date of Birth: January 1, 1980
Contact Number: (555) 123-456
Email: hello@blackcoffer.com
Appointment Details:Date: June 10, 2024
Time: 10:00 AM
Provider: Ales Bru
Reason for Visit: Blurred vision and eye discomfortPatient:Looks good.<<If the patient says data is not correct, ask which data is incorrect and have the user re-enter.>>ChatBot:Your appointment is scheduled. Thanks for choosing eyecare location! Is there anything else I can help you with?Patient:Nothing as of now.ChatBot:Thanks, have a great day!Sample 102: Pediatric Patient with Vision IssuesPatient:Hello, this is Olivia Brown. I’d like to book an eye examination appointment for my child at the eyecare hospital. My child has difficulty seeing the board at school. Here are the details:Parent’s Name: Olivia Brown
Child’s Name: Lily Brown
DOB: June 1, 2012
Preferred Appointment Date: Next Tuesday
Preferred Time: Afternoon, if possible
Reason for Visit: Difficulty seeing the board at schoolPlease call me back at your earliest convenience to confirm the appointment or suggest an alternative time. You can reach me at the contact number provided above. Thank you.Best regards,Olivia Brown(555) 789-0123ChatBot:Thank you for sharing the details. Could you please provide an email address and date of birth?Patient:hello@blackcoffer.com, DOB is December 5, 2000ChatBot:Can you please confirm the following contact details?Phone Number: (555) 789-0123
Email: hello@blackcoffer.com
DOB: 12-05-2000Patient:Yes, it’s correct.ChatBot:Thanks for the details. We have sent you an OTP on the registered email and phone number. Can you please enter the OTP?Patient:1111<<Validation of entered data>><<Get List of Providers>>ChatBot:Thank you for the validation. Please choose a provider from the list:Dr. Emily Thompson
Dr. Mark Davis
Dr. Susan LeePatient:I’ll go with Dr. Emily Thompson.<<Validation of entered data, check if all required details are given>><<API should convert next Tuesday’s date and send date to API>><<Check for available slots with provided preference>>ChatBot:Let me check the availability of the appointment.ChatBot:I see the following open slots on June 18, 2024:2:00 PM
2:20 PM
3:00 PMPatient:Let’s book the 2:00 PM appointment.ChatBot:Alright! Can you please confirm the following details?Patient Information:Parent’s Name: Olivia Brown
Child’s Name: Lily Brown
Date of Birth: December 5, 2000
Contact Number: (555) 789-0123
Appointment Details:Date: June 18, 2024
Time: 2:00 PM
Provider: Dr. Emily Thompson
Reason for Visit: Difficulty seeing the board at schoolPatient:Yes, they are correct!<<API Call>>ChatBot:Your appointment is scheduled. Thanks for choosing eyecare location! Is there anything else I can help you with?Patient:Nothing as of now.ChatBot:Thanks, have a great day!Sample 103: Struggling to Get Required Information from PatientPatient:Hello, I need to book an appointment.ChatBot:Sure, I can help with that. Could you please provide your name, date of birth, phone number, email, and reason for your visit?Patient:It’s John.ChatBot:Thank you, John. Can I have your full name (first and last) and date of birth?Patient:Uh, John… I think it’s in your system.ChatBot:To ensure we have the correct information, I’ll need your full name (first and last) and date of birth. This helps us locate your records accurately.Patient:Oh, okay. It’s Johnathan Smith. I was born on April 3, 1982.<<Validation of data>>ChatBot:Thank you. Could you please provide a contact number and email?Patient:Um, hold on. I think it’s… (555) 123-7890? And I don’t have an email address.<<Validation of data>>ChatBot:Thank you. Just to confirm, your contact number is (555) 123-7890, correct?Patient:Yes, that’s right.<<Validation of data>>ChatBot:Thanks for the details. We have sent you an OTP on the registered phone number. Can you please enter the OTP?Patient:1111<<Validation of data entered>><<Get List of Providers>>ChatBot:Thank you for the validation. Please choose the reason for your visit from the following:<<Show cards>>Patient:Selects one cardChatBot:Great. Do you have any preferred dates and times for your appointment?Patient:Anytime next week is fine.ChatBot:Okay, let me check our availability for next week. Do you prefer mornings or afternoons?Patient:Uh, mornings, I guess.<<Validation of data>><<Get List of Providers>><<API call to get providers>>ChatBot:Please choose a provider from the list:Dr. Robert Anderson
Dr. Laura Bennett
Dr. Michael ClarkPatient:I’ll choose Dr. Robert Anderson.<<Check for available slots with provided preference>>ChatBot:I see the following open slots for next week:Monday, 10:00 AM
Wednesday, 10:20 AM
Friday, 10:40 AMPatient:Let’s book the Monday, 10:00 AM appointment.ChatBot:Alright! Can you please confirm the following details?Patient Information:Name: Johnathan Smith
Date of Birth: April 3, 1982
Contact Number: (555) 123-7890
Appointment Details:Date: Monday, next week (June 3, 2024)
Time: 10:00 AM
Provider: Dr. Robert Anderson
Reason for Visit: Bl"
Netclan20241056,https://insights.blackcoffer.com/ai-bot-audio-to-audio/,"AI Bot Audio to audio$Table of ContentsApplication Architecture
Installation
Running the Project
File DescriptionsApplication ArchitectureBelow is the overview of the architecture for an AI audio-to-audio chatbot application leveraging OpenAI Whisper and ElevenLabs’ text-to-speech (TTS) API:Physical Input (Voice Recording):The user provides a spoken input through a microphone.
PyAudio library is used to record the audio input, saving it as a .wav file.
Transcribe:The recorded .wav file is fed into OpenAI Whisper for speech-to-text transcription.
GPT-3.5 Turbo:The transcribed text is passed to GPT-3.5 Turbo, a large language model fine-tuned for various tasks.
GPT-3.5 Turbo generates a response text based on the input.
Display Response:The generated response text is displayed for the user to read.
Speech Synthesis:The response text is sent to ElevenLabs TTS API for text-to-speech synthesis.
The API synthesizes the text into an audio file, which can be in .wav or .mp3 format.
Play the Response Audio:The synthesized audio file is played back for the user to hear.InstallationFollow these steps to install and set up the project:Clone the repository:git clone https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio.git
Navigate to the project directory:cd AI-Bot-to-Audio
Create a Python virtual environment:python -m venv venvIf you’re using Python 3.x and the python command doesn’t work, try python3 instead.
Activate the virtual environment:On Windows:venv\Scripts\activate
On macOS and Linux:source venv/bin/activate
Install dependencies:pip install -r requirements.txtRunning the ProjectFollow these steps to run the project:Create a .env file:Copy the content from the .env.example file and create a new .env file in the project directory. Populate the .env file with the necessary API keys and configuration variables.
Run the frontend:python display.py
Run the backend application:python main.py
Start the conversation:Once both the frontend and backend are running, you can start speaking into the microphone. The conversation will be displayed on the frontend interface.
Clear conversation and start afresh:If you want to clear the conversation and start a new one, simply click the “New Conversation” button on the frontend interface.
Enjoy your conversation!You’re all set to interact with the project. Have fun chatting!File DescriptionsHere’s a brief description of the files in the project:record.py:This file contains functions to record audio input from the user through a microphone.
main.py:Contains functions for the main backend operations of the application.
Includes functions for transcribing audio to text using OpenAI Whisper base model (speech to text).
Also contains functions for generating response text from OpenAI GPT model.
Utilizes ElevenLabs API for text-to-speech synthesis and playback of the audio response.
display.py:Code for the taipy frontend of the application is implemented in this file.
It handles the user interface and interaction with the backend components.Source codes: https://github.com/AjayBidyarthy/AI-Bot-Audio-to-audio"
Netclan20241057,https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-the-rural-area/,"Recommendation Engine for Insurance Sector to Expand Business in the Rural Area$Client BackgroundClient: A leading insurance firm in the globeIndustry Type:  InsuranceServices: SaaS, Products, InsuranceOrganization Size: 10000+Project ObjectiveDevelop the recommendation engine 
Item-based collaborative filtering based on the use case of the project
Work on Streaming data platform i.e BangDb 
Data Generation for Testing the platformProject DescriptionBangDB is the platform that manages the static data stored on the cluster and also works with live streaming data as Hadoop does. Wherever the bangdb is able to manage machine learning model deployment with their inbuilt parameter and hyper tuning parameters for each model.Streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as Insurance, loans (Business Loans and Personal Loans), Mobile recharge, UPI transactions done by their platform, etc. They wanted the recommendation of other services provided by them to each of their customers who are using their platform.Our SolutionThis Project Module develops according to the Clients Requirements which involves item-based collaborative filtering based on customer behaviour, Firstly classify the customers into various segments on the basis of age, location, gender, and product usage. On the basis of RFM (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaborative filtering.We generated the synthetic data (90 Million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers.Project Deliverables       –   KPI of the Customers       –   Recommendation model       –   Graph databased model       –   Data Generation code based on python (using copula-based on PyTorch)      Tools usedBangDb Tool (ML, AI, NoSQL database supported)
Graph Databased
Google Colab (Data file generation)
Tableau for data visualization Language/techniques usedLinux cloud machine
Python
Graph Database
Data visualization toolsModels used-K means model for clustering-Recommendation Engine model-Collaborative based filtering modelSkills used– Machine learning– NoSQL Database – Graph database– Data Generation using python– Linux – Data VisualizationDatabases used– BangDB– Graph Database– Microsoft MYSQL serverWeb Cloud Servers usedAWS cloud serviceWhat are the technical Challenges Faced during Project ExecutionDecide the Recommendation Engine based on the use case
Finding the RFM score and classifying the customers into clusters
Graph Model to define the relations of customers with each service which they are using 
Synthetic data generation(90 Million events) and around 1.5 Gb structured data.How the Technical Challenges were SolvedItem-based collaborative filtering solves the issue of recommendation because we are dealing with almost 14- 15 services.
Clustering of customers based on their similarities 
Measure the RFM score, and group and classify them based on their scores.
Graph database provides to reduce complexity and increase the processing speed.
Data generation is one of the difficult tasks and generating relational data across 29 different streams using copula and UUID python library function which is based on PyTorch.Business ImpactIt is Qualitative and Quantitative impact on economically where customers are a direct impact of these projects in their life.
It is suggesting to the customers what services they have to utilize from the provider and this is a direct impact of the product on the customers.
Product is providing the action statement of the usage of services by the customers and impacts them economically as well.
The scope impact of product service is Nationwide or statewide.
To provide these impact-full services, there is a tech team of Blackcoffer behind itProject Snapshots Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241058,https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/,"Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI$Client BackgroundClient: A leading solar panel firm in the USAIndustry Type: EnergyServices: Solar PanelOrganization Size: 500+The ProblemSolar Panel organization from America wants to keep track of sales data. They want to see the leadership dashboard of their organization in terms of sales. They also want to keep track of their campaigns and leads generated from sources of those campaigns. They want to keep track of sales data from different sources.  Our SolutionFirst, we fetch the data from CRM to PowerBI. Clean the data of CRM using DAX and then perform calculations on the data. Using cleaned data, we build KPI on PowerBI.Solution ArchitectureTo complete the project, we follow the following data flow pipeline:Data from CRM 🡪 Zapier 🡪 Google Sheet (Dynamic) 🡪PowerBI Language/techniques usedPowerBI, DAX LanguageSkills usedCRM, Zapier , PowerBI, Google SheetWhat are the technical Challenges Faced during Project ExecutionChallenges Faced during the Project Execution :Fetching the data from CRM 
Unclean Data
Merging the Data How the Technical Challenges were SolvedSolution:To Fetch the data from CRM. We used Zapier. It is connector between two applications so that whenever a particular incident happen it will populate into another application. We use Zapier to connect CRM and Google sheets so that whenever a new lead will change or modified data will be stored into google sheets.
Data in google sheets was uncleaned. First, we connect the Google sheet with PowerbI then perform EDA to clean the data using DAX language.
Using merging of two tables by ONE-ON-ONE schema we solve duplicate entries of a particular lead in PowerBI.Business ImpactUsing this Dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a Sale. They can keep track of their sales leadership of employee of the month in term of sales.Project SnapshotsCRM  ZapierDashboard Project Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241059,https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/,"Data Warehouse to Google Data Studio (Looker) Dashboard$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  ITServices: SaaS, Products, healthcare, government, energyOrganization Size: 100+The ProblemOur client needed a Google Data Studio dashboard for different sectors such as Oil and Gas, Government, Healthcare, and Sales analysis. They want to see an analysis of data from which they can provide insights in different domains. They want us to create visual KPIs of meaningful insights.Our SolutionThey provided us with data for different sectors. Using those data first we analyze the data and perform EDA on data for cleaning the data. After cleaning the data, we performed calculations to extract insights for KPIs. Using those KPIs we build a dashboard on Oil and Gas, Government, Healthcare, and Sales analysis.Solution ArchitectureTo build the dashboard we follow the pipeline as follows:Data 🡪 EDA(Cleaning data )🡪 Connection(GDS) 🡪 Building KPIs(Visuals)Tools usedGoogle Data StudioSkills usedEDA, Google data studioWhat are the technical Challenges Faced during Project ExecutionDuring the project execution, we faced the following challenges:The data client provided was not cleaned.
Data was of four different sector which we have to analyse and visualize.
Extracting insights from data.How the Technical Challenges were SolvedTo solve the technical challenges, we provided following solutions as follow:Performed EDA on data to clean it and find the missing values.
As data was from different domains, we have analysed each sector and understand the culture of each domain. We understand the pipeline and flow of work process.
After completing the case study, we use calculations to extract the meaningful insights from data.Business ImpactUsing these dashboards client can visualize the sales insights and understand the workflow. They can take crucial decisions based on these insights which will help them to make an impact on their sales.Project SnapshotsSales Dashboard:Government Dashboard:Oil and Gas Dashboard:Hospital Analysis:Project website urlDashboards on Google Data Studio:1.Government:- https://datastudio.google.com/reporting/dda94ce8-5b77-46aa-a1e0-1a57ccaef5f92.Oil:- https://datastudio.google.com/reporting/47c6529e-1355-4072-babf-1a96f9f842cf3.Healthcare:- https://datastudio.google.com/reporting/b1e95a11-4380-465c-ad45-2d1995c799fb4.Sales:- https://datastudio.google.com/reporting/36ec0e42-6b77-4fbb-9dea-760cccaa741fProject Video







Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241060,https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/,"CRM, Monday.com via Zapier to Power BI Dashboard$Client BackgroundClient: A leading solar panel firm in the USAIndustry Type:  EnergyServices: Solar PanelOrganization Size: 200+Project DescriptionMohsin has Solar Panel Company. He has setup CRMs for that. He wanted to use CRMs data and want to visualize the leads in PowerBIOur SolutionFirst, we check CRMs thoroughly and understand the work culture of his company. It was not easy to fetch data into PowerBI using API key. To fetch new leads from CRMs we used Zapier. The limitation of Zapier is it cannot fetch historical data into spreadsheet. So we download data from CRMs and fetch it into spreadsheet. For new leads we created zaps for every instance. After that we connect the spreadsheet with PowerBI and clean the data accordingly. Using that data, we build KPIs according to client need.Tools usedAPI , Zapier , Spreadsheet , PowerBILanguage/techniques used M language , DAXSkills usedAPI , M language , DAX , PowerBIWhat are the technical Challenges Faced during Project Execution?First challenge was to fetch data from CRMs using API key. Data we were getting was uncleaned and were not able to fetch all data. If there were multiple pages in the CRMs we will not be able to fetch all data from the pages.How the Technical Challenges were SolvedTechnical challenge in this project was to extract data from CRMs. So for that we used Zapier connector from CRMs to spreadsheet. But there was some limitation with Zapier that it will not fetch the historical data of our CRMs. So to solve that we download all historical data from CRMs and append it to the spreadsheet we were using. We fetch new leads to our spreadsheet using Zapier. By doing this now we have all the data historical and new lead which will be pushed by Zapier.Then we fetch the data to our PowerBI and do some cleaning in data. By using cleaned data, we build the KPIs for our client according to there requirements.Business ImpactClient will be able keep track on his company data on PowerBI and it helps them to make decisions accordingly.Project Snapshots CRMsZapierPowerBI DashboardProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241061,https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/,"Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data$Client BackgroundClient: A leading energy firm in the USAIndustry Type:  EnergyServices: Solar panelOrganization Size: 200+Project ObjectiveSetup a dashboard on Monday.com
Fetch client CRM data onto Monday.com dashboard.Project DescriptionMohsin has CRM for his business where he has all data regarding leads of his clients. He wanted to see all his client appointments at one place. Client took subscription of Monday.com. It is an CRM where you can manage your work more easily in neat and clean user-friendly environment. We can easily track our task on Monday.com. Pipeline for Monday.com is very easy to use and also customized according to our needs.Our SolutionThe challenging part of this project was to get CRM data on Monday.com dashboard. Client also has subscription of Zapier. Zapier is a connector which connect two apps to transfer data from each other. Zapier also has limitation to fetch limited type of data from CRM. Like for Mohsin CRM we can only fetch hot lead comes on CRM. But in his CRM there are also other functions like if a customer lead comes on CRM. They manually book appointment for that client. So there is no way to get that data from CRM. Issue for client was he has attached integrated four google calendar account with CRM so whenever he confirms appointment on CRM that data fetched on google calendar. But he has check manually one by one on each calendar which was bit hard task for him. So, we advised Monday.com where he can track all his task at one place.Tools usedMonday.com 
Zapier
Google CalendarDatabases usedGoogle CalendarWhat are the technical Challenges Faced during Project ExecutionThe challenging part of this project was to get CRM data on Monday.com dashboard.There is no direct integration of CRM and Monday.com to fetch data.How the Technical Challenges were SolvedTo solve challenges, we used Zapier to get CRM data to dashboard of Monday.com. We used google calendar of client which were integrated with CRM. All the appointment confirmed leads were present on google calendar.Pipeline of Data:  CRM 🡪 Google Calendar 🡪 Zapier 🡪 Monday.comBusiness ImpactUsing the Monday.com dashboard client can easily track all appointments of customers. He can track data of his team members and connect with them at one place. He will not miss any of his meeting with customer. Monday.com also has timeline and calendar view using that client can see all activity of his work.Project SnapshotsCRM Calendar viewMonday.comGoogle CalendarZapierProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241062,https://insights.blackcoffer.com/data-management-for-a-political-saas-application/,"Data Management for a Political SaaS Application$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  ITServices: SaaS, ProductsOrganization Size: 100+The ProblemAs per the guidelines and discussion. Political Research Automated Data Acquisition (PRADA) in the following phases which included.1. Get pics for existing EOs (Elected Officials)2. Get new EOs and Pictures.3. Run QA checks regularly on EOs4. Get data from government Facebook pages.5. Geospatial project: Create a new version of provided KML without using google earth.  Creating a nested directories which contained description and Map-URL at the designated location.6. Get data of US States and Counties(Including Boroughs and Parishes)By building an automated generated structured data that allows a non – programmer to create a config for each page allowing a bot to scrap and update the data.Our SolutionWe created an automated python scripts for designated phases with respective requirements. Solutions to various type of problems varied such as most of data scrapping automation was done through python developed scripts including the geospatial KML task. In addition to this different ranges of data was scrapped generated directed output for the respective tasks in the form of CSV format. So the user’s main aim requirement was achieved i.e. a non programmer could create a con-fig and initiate a bot to scrap the required data.Solution ArchitectureThe majority task of project consisted of web data scraping automation so a high- level overview, and specific implementation details of project shall will be as follows:Web Scraping Framework: Python as a coding language was used in almost all of the tasks and the framework used for data scraping included Beautiful-Soup, Selenium and Web drivers. These libraries provide tools and functionalities to navigate web pages, extract data, and handle various HTML elements.
Data Extraction and Parsing: Use the selected web scraping library to extract the desired data from the web pages provided either in the data sheet or within the websites of URLs given in the sheet. This involves locating HTML elements, applying filters or selectors, and parsing the extracted data.
Data Processing:  Followed by data extraction it was cleansed, transformed and aggregated to a structured form such as pandas’ Data Frame followed by a CSV file. In the case of geospatial task it resulted to generation of nested folders in a kml file.
Data Storage: The how and where to store the scrapped data was determined which is local file system in the form of CSV (Comma Separated Values). As it was the appropriate data storage solution according to need of the project. In addition to this the geospatial task had the output in the form of kml file as polygons inside directories of nested folders. DeliverablesTasksOutputs (CSV/KML/XLSX)Python Scripts Canada EOs mydata.csvScript1.pyScript2.pyGeospatial TaskElectoral Districts.kml–Facebook Scrapping of EOs EO_OUTPUT_O.csvfinal_eo_scrapping.pyFacebook Scrapping of 429 CitiesOutput_DRAFT_429_CITIES.csvFacebook_image_scrapping.pyUSA States Website URLsScreenScrapingt.csvfinal_50_states_scrapping.pyUSA Counties Website URLsUS Website_final_write.xlsxcounty_scrapping.pyTools usedPython (Programming Language) 
Beautiful Soup 
Selenium 
Pandas 
Numpy
Simplekml 
re (regular expressions)Language/techniques usedPython (Programming Language) – It is an interpreted language, which allows quick prototyping and interactive coding. Its versatility can be is one of the reasons for its major applications. Different libraries and tools were used in this project for various data solutions.
Beautiful Soup –  A python library used for web scraping and parsing HTML and XML documents. It provides a convenient way to extract from the said files. It eases out the work flow from parsing to data extraction and encoding handling as well.
Selenium – A python library used for web browser automation like Chrome, Firefox, Safari and others. It interacts with elements such as clicking buttons, filling out forms and selecting drop down options. In this project we used it in Chrome. Selenium web driver was used for web automation. It acted as bridge between Python code and the Web browser.
Pandas – It is Python’s versatile library that provides high performance data structure tools and it is built on top of Numpy. Data Frame is one of its key feature due to which this library was used. This key feature allows efficient manipulation, slicing, and filtering of structured data
Numpy – It is also a python library aka Numerical Python as it is a fundamental library for scientific computing in Python. 
Simplekml – It is a python package which enables you to generate KML with as little effort as possible.
 re (Regular Expressions): It is a powerful tool in python sued for pattern matching and manipulations of strings.Skills usedPython Programming
Web Fundamentals
Web Scrapping using libraries such as BS, Selenium.
Data Cleaning and Processing
Problem- Solving and debugging.
KML structure and handling using Python’s programming.Databases usedNone. All the structured data was in the form of either python Data Frames, CSV or Excel Sheets.What are the technical Challenges Faced during Project ExecutionFirstly some of the web URLs were not accessible because they were restricted to particular range of IPs of that region
Couldn’t fetch whole data through Beautiful Soup as it couldn’t parse whole tags.
List of US Counties wasn’t provided in the given resource linksHow the Technical Challenges were SolvedUsed VPN for accessing Official sites which were not generally accessible.
Used Selenium Web driver to automate the direction at URLs which fetched complete html tags of the desired webpages.
Performed a search and created structure data of list of counties of each state which was used as input to gain web URLs of counties of US.Business ImpactEnhanced Analysis: Web scraping allows businesses to gather valuable data from various websites. This information can provide insights to desired aim and objectives enabling businesses to make informed.
Real-Time Monitoring and Upgradation: Web scraping can enable business to monitor changes or updates on website in real-time. This can be useful for tracking regulatory changes. It keeps the business and it’s data updated.
Increased Efficiency: Automation eliminated the need for manual data collection, saving time and resources. With automated web scraping, business can extract large amount data quickly, accurately, improving overall operational efficiency.Project SnapshotsChrome driver initiatedChrome driver visiting the directed links and accessing the image URLsDirected to next linkKML taskFacebook Data extractionData of State Governments of USAccessing links through wiki directing to countiesNesting within the list of counties of a particular stateFinding and Extracting link of the website of CountyProject website urlThe GitHub repository link:- https://github.com/AjayBidyarthy/Paul-Andr-Savoie/tree/mainProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241063,https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/,"Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Ads, Marketing, Campaign, ConsultingOrganization Size: 200+The ProblemThe client has a Google LSA Ads Manager Account with about 100+ accounts and wishes to collect data available through the Google LSA API daily. The client wishes to set up a private Databases that is automatically created for newly added accounts and stores all of the collected data (Lead and Phone Call data). Finally, all collected data must be presented through the Google Looker Studio Dashboards, with the design layouts as suggested by the client.Our SolutionThe solution involves a number of Python-based ETL tools that are responsible for fetching the data from Google’s LSA API daily and updating the same in the Google BigQuery Databases.Two different tools run are:MCC Data Fetching tool.
Lead Record data fetching tool. The fetched data is stored in BigQuery Databases on the client-provided (Google)manager account.Carefully curated Google Looker Studio dashboards implemented with client-suggested theme layout which are updated upon client request, represent a number of KPIs and graphs indicating major data trends.The designed dashboards have a number of data-controlling filters that filter the data account-wise and date-wise.Solution ArchitectureDeliverablesHeroku deployed Python tools
Google Looker Studio Dashboards
BigQuery Database
Maintenance serviceTools usedPython
Google BigQuery
Heroku
Google Looker Studio
Git
Heroku CLILanguage/techniques usedPython
GoogleSQL (BigQuery supported SQL)
Looker Modeling Language (Looker ML)
Git CommandsSkills usedData Engineering skill to fetch data as per client needs.
Data Processing to make it suitable for dashboards, databases
Dashboard designing and data presentation skills
Tool Deployment
Database manipulation
Data pipliningDatabases usedGoogle BigQueryWeb Cloud Servers usedHeroku: Cloud Application PlatformWhat are the technical Challenges Faced during Project ExecutionGoogle LSA API is slow, high data fetching timelines.
BigQuery jobs fail, causing inconsistencies.How the Technical Challenges were SolvedEntire data fetching operation requires 1-2 hrs daily, 2 separate tools run in asynchronously and populate two different databases, the data is grouped in the dashboards
Regular weekly and monthly data refreshes update any inconsistent data.Business ImpactBusiness clients are able to access important KPI’s without the need to understand the complexities involved behind the scenes.
Allows clients to track their performances, responsiveness.Project SnapshotsContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241064,https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data-studio/,"Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)$Client BackgroundClient: A leading financial firm in DubaiIndustry Type:  Financial ServicesServices: Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTechOrganization Size: 200+The ProblemBuild dashboards unifying all the platforms in use: Google Ads, FB ads, Appsflyer, Mixpanel, etc,in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel Our SolutionTrack the app data analytics using various platforms
Prepare the data sources – find and build data connectors for Google Data Studio.
Developed 14 pages of Dashboard reports- creating templates to importing data sources and perform various visualisations.
Maintained and tracked dashboard reports and helped the client with intelligence from these reports.Deliverables1Updating the iOS datasheet2Fixing the incoming data for androids3Correcting a calculation error4Finding an alternative to provide automated data update directly to google data studio for iOS.5Updates done to all the dashboards6Created new dashboards7Created a consolidated dashboard8Added required visualizations and conected to data sources9Created new data sources10Managing the consolidated dashboard with daily data monitoring11Funnel Report for consolidated dahboard12Google analytics installed on website through tag manager13Resolving errors 14work on automation for ad acccounts15Developed a new dashboard 16Ad accounts data Automated 17work towards android data automation18altering of blended data joins as per gds updates19Personalisation of dashboards20Current dashboard updated with google events and widget changes21Added Apple search ads dashboard22Firebase funnel report dashboard developed23Card topups Funnel report dashboard developed 24Porter metrics custom dashboards for trial 25Registration firebase funnel and percentage added 26Updates for all the dashboards running until now and addition of kpi to the new firebase dashboards27User info for firebase dashboard and retention report28Registration Funnel, Cardtopups, KYC funnel Dashboard 29Fixing and Updating user info firebase dashboard and began working on the tiktok dashboard30Tiktok Dashboard Developed and populated with data from porter metricsTools usedGoogle Data Studio
Google Analytics- GA4 and universal analytics
Google Tag Manager
Big Query
Firebase
Appsflyer 
Mixpanel
Google spreadsheetsLanguage/techniques usedGoogle Standard SQL dialect- bigquery
Apps scriptSkills usedAnalytical aptitude
Problem-solving
Communication
Knowledge about SQL
Knowledge in digital marketing and strategies
Google cloud services
Creating data pipelines.Databases usedBigquery
Google spreadsheets 
FirebaseWeb Cloud Servers usedGoogle Cloud PlatformWhat are the technical Challenges Faced during Project ExecutionCommunity/in-built Connectors for Appstore connect didn’t exist
Connector for apple search ads couldn’t be found 
Data tracking from google play console, due to the timezone lag in data updation.
Facebook connector issues How the Technical Challenges were SolvedWorked towards building the custom connector by using apple api for Appstore connect and search ads
Utilised big query to call and store 100% accurate data from google play console and be used as a connector in GDS
Made use of inhouse built facebook connector and google sheet add-on to track and keep connector inaccuracy check.Business ImpactHelped the client to view a consolidated report of all their ad campaigns 
Calculated and executed analytics metrics which helped to track various app events and helped the business to take decisions on UX
Consulted the client and collaborated with them in marketing and ad campaign strategies- helped them cut their marketing expenses over less efficient marketing platforms
Created funnel reports and suggested insights on app traffic to take decisions on important landing pages.Project SnapshotsProject website URLhttps://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9fContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241065,https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/,"Analytical solution for a tech firm$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  ITServices: ConsultingOrganization Size: 100+The ProblemThe client’s organization had a project that matches URLs up using TF-IDF algorithm. The script threw some errors and resolving these errors was the immediate ask. The client also required us to adjust the script for better accuracy and faster computation.Our SolutionR&D on the code developed
Find & List bugs
Solve the Bugs
Find and get the best matching algorithm implemented. 
Check and compare the existing matching algorithm implemented for accuracy. 
if not check of other solution – ngrams or fuzzy logic
Meet the expected outputDeliverablesFully functional code
Solution & Documentation
SupportTools usedGoogle spreadsheets
Microsoft Excel
Google ColaboratoryLanguage/techniquesPython Models usedTF-IDF
BERT
Ngrams
Flair Embeddings
Rapid FuzzSkills usedProblem-solving
Communication
Data Modelling
Data Pipelining
Python CodingDatabases usedGoogle spreadsheetsWhat are the technical Challenges Faced during Project ExecutionBugs on the model used by the client was fairly competent using pretrained libraries
The accuracy for the bug free code on the models used by the client was shaen once the model ran on a different set of data inputHow the Technical Challenges were SolvedA vanilla code to execute the same logic while fine tuning the matching algorithm was written in order to over come the shortcomings of the pretrained model bugs
The data pre-processing was done manually in order to transform every instance of an input into better readable format to be able to go into the model and get best matching accuracy possible in the given timeframe of execution of the code Business ImpactHelped the client to perform the matching process with maximum accuracy and lowest cost on code, by implementing manually written vanilla code from scratch to utilise the matching algorithm.Project Snapshots Project website urlhttps://colab.research.google.com/github/AjayBidyarthy/Daniel-Emery/blob/main/vanilla.ipynb#scrollTo=vPp14xj020RLContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241066,https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/,"AI solution for a Technology, Information and Internet firm$Client BackgroundClient: A leading Technology, Information and Internet firm in IndiaIndustry Type:  ITServices: Emerging Technologies, 2030, and 2050Organization Size: 10+The ProblemThe objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services.Our SolutionAnalyze each use case
Analyze product design
Analyze user type, controls per use cases
For each use case and available product design, provide solution or scope of the data science capabilities 
List attributes needed in each of the product design screensList use cases are driven by the data
For each data-driven use cases      a. Research and design the data science solution       b. List needed data       c. List process                     d. List models                     e. List solutionHelp product design team with data science use cases
Help product design team with data science solutions for each use caseDeliverablesStatement of Work (SoW) with a solution documentationData science use cases document
Data science solution for each use cases document
Data Science methodology, algorithms needed, models, recommended and more in a good documentationTools usedGoogle docs
Microsoft word
Draw.io
Excel
Google DrawLanguage/techniques Python- Flask Models usedK-Nearest Neighbours
K-Means Clustering
NLTK
DeepAvlov
Spacy
Texttiling
Eclat
LSTMSkills usedAptitude for functionalities
Problem-solving
Communication
Data Modelling
Data Pipelining
MLOps
NLP
Recommender systemsDatabases usedAmazon S3Web Cloud Servers usedAWS EC2Business ImpactCollaboration with the client to identify the scope and use cases for the platform
Cost Effective approach taken to document solutions 
Regressive R&D to find and document third-party solutions for certain use cases- saving cost and time.Project Snapshots Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241067,https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-capital-and-private-equity-principals/,"AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals$Client BackgroundClient: A leading Venture Capital and Private Equity Principals in the GlobeIndustry Type:  Venture Capital and Private Equity PrincipalsServices: Private Equity, Venture Capital, Data Analysis, Fund Performance, Alternative Assets, Competitive Intelligence, Limited Partners, Customized Benchmarks, Service Providers, Fund of Funds, M&A, and Financial ServicesOrganization Size: 100+The ProblemExtract funding-related data from news articles (from 1000+ websites) such as company name, funded amount, participated investors, and other details. 
create a web app to manage the extraction of funding  dataOur SolutionThere were 1000+ websites from funding-related articles so we couldn’t make a crawler for each website. So we used an inbuilt web crawler provided by elasticsearch. When we have extracted articles then we need to extract funding related information company name, fund amount and investors participated etc. Then we decided to use NLP’s question-answering method in which we need to train transformers to extract funding-related information. First we have created some keywords based approaches to create labels for each field we need to extract to train models. After that we have trained distil bert model on labelled data on AWS EC2’s GPU server. We applied this approach for all the fields we need to extract. We got 90%+ accuracy for the company name field and for other fields we got 80%+ accuracy.
To manage and view all the fields of extracted funding data we created a web app using python flask. In this we created several pages to show extracted raw data by crawler,  cleaned data after applying some cleaning functions and final output which have all the fields.  We also created admin dashboard pages to show daily crawling status, how many articles processed in one day, total final output etc.Solution ArchitectureDeliverablesFlask Web app
Elasticsearch crawlerTools usedFlask, Spacy, NLTK, pandas, numpy, transformers, elasticsearch etc. Language/techniques usedQuestion answering in NLP, web scraping, web application Flask, PythonModels usedDistil-bert model, en-core-web-sm (pre trained model of spacy)Skills usedNLP, Data Analysis, Flask web app, Pandas, Numpy, transformers, fastapi, elasticsearch etc.Databases usedElasticsearch databaseWeb Cloud Servers usedAWSWhat are the technical Challenges Faced during Project ExecutionThe client wanted to extract data from 1000+ different websites and if we make any crawler it only works for one website so it was not possible to create a 1000+ web crawler.
How to extract funding information from an article. It is very difficult to extract that type of information from normal python code by defining keywords because every website has different types of articles.How the Technical Challenges were SolvedTo solve web crawler-related issues we used elasticsearch web crawler which is very fast and can extract multiple websites at a time. In this we need to create an engine and add websites that we want to scrape. After that we added some keywords to extract only funding-related articles. We set up this crawler to run every hour so we can get new articles every hour.
To extract funding-related information we collected articles from different websites and created labels for each field we wanted to extract. After that we have fine-tuned the transformer’s Distil-bert model on our labeled data.  We used these models to extract funding-related information. We also created an automated python script that uses these model on every extracted article and extracts funding-related information.Business ImpactThis funding-related data would be used in two ways. From this project, companies can find suitable investors for their startups. Companies can search for investors based on industry, verticals, etc., and find investors to help their startups.Investors can use it to find a startup in which they want to invest based on their preferences like industry, verticals, etc.Project Snapshots (Minimum 10 Pictures)Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241068,https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/,"An ETL solution for an Internet Publishing firm$Client BackgroundClient: A leading internet publishing firm in Singapore and AustraliaIndustry Type:  Internet PublishingServices: peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great valueOrganization Size: 100+Project ObjectiveFetch all call logs using zendesk api from drivelah server
Analyse call logs and  number of calls made by a particular phone number to company and fetch recent call timing  Project DescriptionWe need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk api.Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server. Our SolutionTo fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details.After that we need to identify the number of calls made by the user to the company in the last month.  We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user’s call records. For recent dates we used python’s datetime module which can easily identify recent date time.Project Deliverables2 python scriptsfor fetching call details and converting into table format
for identifying number of calls made and recent call timing Tools usedVS Code, Google Drive, and MS Excel.Language/techniques usedPython programming language, Data Analytics with numpy and pandas, python datetime.Skills usedData Analytics,, Python, MathematicsDatabases usedlocal data from MS Excel SheetWhat are the technical Challenges Faced during Project ExecutionFirst one was the api data in json format with other unwanted data so it was a little difficult for us to identify the number of calls and other information from direct json data.
The date format in the api data is not appropriate for us  to handle. Because the date is  stored in string format, it was difficult to compare dates with one another and identify recent ones.How the Technical Challenges were SolvedFor the first technical challenge we first took only useful details from api’s json format and converted these details in tabular format. In python we can easily handle tables with pandas dataframe and can apply whatever operation we want to collect details.
For the second one we know that it would be difficult to handle dates in string format. So we first converted dates to a proper datetime format using python’s datetime module. It has a lot of built in functionalities which can easily compare dates with one another.  So from comparison we have identified recent dates of calls.Project SnapshotsContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241069,https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/,"AI-Based Algorithmic Trading Bot for Forex$Client BackgroundClient: A leading trading firm in the USAIndustry Type:  FinanceServices: Trading, Banking, InvestmentOrganization Size: 100+The ProblemBuild ML/AI Model to predict next 15 min EMA cross on historical and live data by using indicators such as EMA, MACD, RSI etc.
Create a web app to show predicted EMA cross and other indicators movementOur SolutionIn stock market indicators such as EMA, MACD, RSI etc helps us to find cross by using historical price data. If we accurately predict cross earlier then it will help us in investment. So we have used 12data api to collect historical and live EUR/USD price data. We calculated EMA(12), EMA(26), MACD and RSI indicators based on price data.  After that we created labels of ema cross in historical data. When we have training data we used different classifier models for training. We predicted accuracy  with different models and the Logistic regression model gave 91% accuracy. This logistic regression is predicting the cross only for the next step. It means we will know only 15 minutes before that the cross will happen in the next 15 min but we need to know more earlier. For that we predicted the next 45 minutes price values using the LSTM model from historical price data. Based on these price values we have calculated EMA, MACD and RSI and  after that cross using logistic regression. So now we can predict the cross 1 hour earlier based on these 2 models.
To show cross and other  indicators movement we created a python flask web app and hosted it on AWS EC2 server. The process runs every 15 minutes  and checks the cross. If there is any cross in 1 hour it sends a telegram notification.DeliverablesFlask web app
All the python code and machine learning modelsTools usedPandas, numpy, scikit-learn, tensorflow, flask etc.Language/techniques usedData Analysis, Data Visualization, Machine learning, Deep learning, flask web app etc.Models usedLogistic Regression, LSTM modelSkills usedData Analysis, Data Visualization, Machine learning, Deep learning, flask, python etc.Databases usedMongoDBWeb Cloud Servers usedAWS Ec2What are the technical Challenges Faced during Project ExecutionMain challenge in this project is to find the best model. Because we have time series data so we cannot change the orders to get better accuracy.
One machine learning model is only predicting the next 15 min cross but we need the ema cross 1 hour before.How the Technical Challenges were SolvedWe were using time series data so we cannot change the order to find better accuracy in every model.  So we have tried different models with the same order and evaluated the model. Only the logistic regression model worked best for the data it gave 91% accuracy on test data.
To get the next 1 hour prediction we first tried the same logistic regression to predict the next 3 steps but we failed because of poor accuracy. So we trained the LSTM model on price data and predicted the next 3 steps using the LSTM model.  After that we used logistic regression to predict ema cross.Business ImpactIt will help traders to predict the stock market earlier and get better returns from this project.  Project SnapshotsContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241070,https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sector/,"Equity Waterfalls Model-Based SaaS Application for Real Estate Sector$Client BackgroundClient: A leading real estate firm in the USAIndustry Type:  real estateServices: Property business, investment, real estate Organization Size: 100+Project ObjectiveThe objective is to create software that will calculate the equity waterfalls for different cases. And there should be 3 users admin, sponsor and investor. We need to create the equity waterfall calculation according to the csv file that is shared by the client. All users have their own UI portal. Project DescriptionThe project is created using python language, working on django rest framework and for frontend we use reactjs and the code deployed on google cloud app engine service. We need to create a software that will calculate the equity waterfalls. And there should be 3 users admin, sponsor and investor. We need to create the calculation according to the csv file that is shared by the client.All users should have their own UI portal. Sponsors can create deals and send deal invitations to all investors or specific investors.Investors can see all the deals that are offered by the sponsor’s. After that Investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not.Our SolutionWe have created api’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers.Project DeliverablesDjango rest framework api’s with frontend.
Github source code.
Working UI.Tools usedViews.
Routers.
Serializers.
Serializer relations.
Settings.Language/techniques usedPython
Django rest framework
ReactJS
JWT
SMTPSkills usedSMTP
JWTDatabases usedSqlite3 DatabaseWeb Cloud Servers usedGoogle cloud platformWhat are the technical Challenges Faced during Project ExecutionThe technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. And also invite the sponsors by admin or sponsors invite their investors.How the Technical Challenges were SolvedWe have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly.Added the functionality in which admin can invite the sponsors to the website and sponsors can invite their investor through sending the invitation link to their email.Project SnapshotsProject website urlhttps://stackshares.io/dashboard/add-new-dealProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241071,https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/,"AI Solutions for Foreign Exchange – An Automated Algo Trading Tool$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  Financial ServicesServices: Trading, consulting, financial serivicesOrganization Size: 100+The ProblemOur main objective in this project was to help with setting up with given Broker API using MT4 and extracting historical data from it, and solving different tasks which are related to extracting important values from the data. And tasks assigned by the client were related to working around the data, i.e. formatting, connecting with the IG trade broker, automating the Python script and scheduling the script accordingly.Our SolutionDuring the initial phase, we were assigned to set up an MT4 with given Broker API access to extract historical prices, which was delivered to the client. In the second phase, the client requested to implement Profit/Loss, Spread Direction and Time in Trade. There were minute tasks related to the R script, which was duly completed. In the third phase, the client was assigned a task related to distinguishing the tickers according to cluster types which he provided and implemented code to distinguish the sell and buy spread for the given STD. In the fourth phase, I implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code and worked on retrieving Historical prices from another Broker API and retrieving Watchlist given attributes by the client. Automated the Python script to retrieve yesterday’s market price of the given listDeliverablesSuccessfully delivered set-up in MT4 for retrieving historical prices, Created logic for automating the profit and loss, Implemented code to distinguish the tickers according to the cluster type, Implemented code for distinguish the sell and buy spread for the given STD, Implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code. Automated the Python script to retrieve yesterday’s market price.Tools usedMT4, Jupyter Notebook, Excel, IG trade, Remote Desktop setupLanguage/techniques usedMQL, Python, RSkills usedCritical thinking, Logical ThinkingWhat are the technical Challenges Faced during Project Execution?While setting up MT4 platform and its configurations How the Technical Challenges were SolvedThe above-mentioned challenges were resolved after many hours of effort and understanding. Project SnapshotsProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241072,https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/,"AI agent development and Deployment in Jina AI$Client BackgroundClient: A leading tech firm in EuropeIndustry Type:  ITServices: IT and ConsultingOrganization Size: 100+The ProblemThe client’s object was to create AI agents for his website, which the end-users will utilize for many tasks. The client had some recommendations on the models are utilized.Our SolutionCreated a feasible models list that complements the client’s requirement and when ahead and executed the Executor code for every model for compatibility with JinaAI deployment. After implementing Executor codes, I created a Flow to connect every executor and deployed it successfully. DeliverablesSuccessfully delivered executable deployed models in Jina AiTools usedJina AI, VSCode, HuggingFaceLanguage/techniques usedPythonModels usedWhisper, Stable Diffusion, GPT3, Codex, YOLO, CoquiAI, PDF SegmentorSkills usedPython, Model APIs Databases usedJinaAI CloudWhat are the technical Challenges Faced during Project ExecutionThere were minute challenges, such as deployment issues and Execution issuesHow the Technical Challenges were SolvedI resolved the issues effectively after long hours of understanding the concept because JinaAI is a new growing technology that does not have many forums to solve errors and issues. Project SnapshotsProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241073,https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discovery-using-neo4j/,"Golden Record – A knowledge graph database approach to unfold discovery using Neo4j$Client BackgroundClient: A leading retail firm in the USAIndustry Type:  RetailServices: Retail business, consumer servicesOrganization Size: 100+The ProblemTo use data ingested into Neo4j and use the nodes and relationships with its properties to determine which nodes are actually the same person. For eg: we have Person nodes in the data, now people might enter their names in different ways. Our main aim is to identify Person nodes that may have similar data and are actually the same person. This will be represented as a perfect match between the nodes. This single-person view is referred to as the Golden RecordOur SolutionTill date, we have loaded data into Neo4j and created relationships with score property which defines match strength. We have created some criterias by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’.We have considered four properties for our criteria – full name, address, driver’s license, and passport number. We have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation.We have also configured Graphlytics (a viz software) in the virtual machine which connects to the neo4j database and helps vizualize the nodes and relationships. We have also worked on some algorithms using the GDS library in neo4j to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. Other algorithms were tried as well but since all the properties are of String format it did not work on it.We have Resolved issues neo4j is facing when deleting a Large set of data and Provided steps to recover neo4j if it fails by going OutofMemory.We have figured out the issues with the probable and perfect match cypher queries not working as intended and proposed a solution. Solution ArchitectureDeliverablesCreated Perfect match and probable match queries.
Created queries that return the nodes (even if it does not have associated relationship) and it’s associated relationship.
A cypher query that return the result as a json object that can be mapped into a java oject.
A cypher query that will create the relationship if two node’s properties  have same value.
A cypher query that will delete one relationship from bidirectional relationship.
A python code for a sample neo4j query
Adjust the perfect and probable match queries so it would work for  current data. Tools usedNeo4jLanguage/techniques usedCypher Query LanguageModels usedThe common neighbors algorithmSkills usedCQLDatabases usedNeo4jContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241074,https://insights.blackcoffer.com/advanced-ai-for-trading-automation/,"Advanced AI for Trading Automation$Client BackgroundClient: A leading tech firm in EuropeIndustry Type:  Banking & FinanceServices: Trading, and financial servicesOrganization Size: 100+The ProblemCreate an automated trading application with fully automated trading capabilities from selecting pair of assets to buying/selling assets. This application uses AI to decide what action to take while trading.Our SolutionWe have integrated coin_api with the application from which data is extracted. We have created the homepage for this application. We have changed the code structure of the front end to make it more fast and efficient.Solution ArchitectureAn application, where the first automated top asset pair selection happens. If the coins are co-integrated, then only one indicator must be executed else trading starts based on 2 indicators.The AI agent will take specific action to trade based on the algorithm.DeliverablesWe have removed the old API and integrated the new api with the application.We have altered the code structure of the front end to make the code faster and more efficient.Tools usedVisual studio codeLanguage/techniques usedPythonSkills usedDjangoDatabases usedSQliteWeb Cloud Servers usedDigital OceanWhat are the technical Challenges Faced during Project ExecutionWe faced an issue while integrating coin api with the application while retrieving the data. To retrieve the data using the coin api, we need to input a symbol id. This symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. There are N coins that can be retrieved using coin api. There are more than multiple exchanges, multiple symbol types, and multiple quote currencies for ONE SINGLE COIN. This makes there is a huge no. Of combinations for one single coin. This made the execution of the api integration very slow.How the Technical Challenges were SolvedWe created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. The user selects these, and in the backend, a combination is created and is sent as input to the coin api code and the data is retrieved without slowing down the process.Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241075,https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendations-and-a-single-source-of-truth/,"Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  RetailServices: Retail BusinessOrganization Size: 100+The ProblemThe Client was using NoSql Database which was slow and did not provide real-time response for complex queries. The data had many Connections and it was difficult to represent them in NoSQL or Relational Databases.Our SolutionCreate a Knowledge Graph and Provide Real-time Analytics and Recommendations using Machine Learning.Solution ArchitectureNeo4j was Installed on a Cloud VM based on Linodes.DeliverablesKnowledge graphs and Data Pipelines are used to Populate the Graph.API’s to Perform CRUD operations in real-time.Tools usedNeo4j
PostmanLanguage/techniques usedPython
JSONModels usedNode-Relationship modelSkills usedProgramming
Data Engineering
Data AnalyticsDatabases usedNeo4jWeb Cloud Servers usedLinodeWhat are the technical Challenges Faced during Project ExecutionIntegration of Firestore with Neo4j without any native integration method or driver.How the Technical Challenges were SolvedThe challenge was solved by using api to retrieve data from Firestore.Project Snapshots Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241076,https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/,"Advanced AI for Thermal Person Detection$Client BackgroundClient: A leading tech firm in the Middle EastIndustry Type:  SecurityServices: Security servicesOrganization Size: 100+The ProblemDetect a Person from thermal image and videos. Why this model was created was not told to us by the client.Our SolutionUse Deeplearning Computer Vision to train the model on custom dataset and get the results.Solution ArchitectureLinux 22.04
Nvidiva RTX 3080DeliverablesTrained modelTools usedLabelimg
Yolov7
COCO2JSONLanguage/techniques usedPythonModels usedYolov7Skills usedDeeplearning
Computer vision
ProgrammingContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241077,https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/,"Advanced AI for Road Cam Threat Detection$Client BackgroundClient: A leading tech firm in the Middle EastIndustry Type:  SecurityServices: Security servicesOrganization Size: 100+The ProblemDetect the threat level of accidents between a Pedestrian and a Car.Our SolutionUse Deeplearning Computer vision and logic to detect the threat level as defined by the Client.Solution ArchitectureLinux 22.04DeliverablesProgram which detects the threat level.
Pretrained model.Tools usedYolov7
DEEPSORT
OpencvLanguage/techniques usedPythonModels usedYolov7Skills usedProgramming
Computer Vision
Deep learningWhat are the technical Challenges Faced during Project ExecutionIntegration of Object tracking algorithm with Object detection algorithm.
Writing of logic to detect the threat level.How the Technical Challenges were SolvedThe technical challenge was sorted by testing, experimenting and later on finding and modifying an already existing repository to use as a baseline for our code for integration.Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241078,https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/,"Advanced AI for Pedestrian Crossing Safety$Client BackgroundClient: A leading tech firm in the Middle EastIndustry Type:  SecurityServices: Security servicesOrganization Size: 100+The ProblemTraffic Signals are inefficient because even if there are no cars or no pedestrians on the road it still works on a timer and stops the traffic or pedestrian unnecessarily. Our SolutionWe provide a Computer vision-logic to Manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal.Solution ArchitectureYolov7 pose estimation
OpencvDeliverablesThe program Detects Pedestrians and Gives alerts to traffic Signals to turn Red or stay Green.
Yolov7 pose model weightsTools usedYolov7
OpencvLanguage/techniques usedPython
Computer VisionModels usedYolov7 Pose EstimationSkills usedProgramming
Computer Vision
Deep LearningWhat are the technical Challenges Faced during Project ExecutionThere was no existing solution and we had to create the logic from scratch.How the Technical Challenges were SolvedResearching Computer Vision. Learning new Techniques and Experimentation.Project SnapshotsContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241079,https://insights.blackcoffer.com/handgun-detection-using-yolo/,"Advanced AI for Handgun Detection$Client BackgroundClient: A leading tech firm in the Middle EastIndustry Type:  SecurityServices: Security servicesOrganization Size: 100+The ProblemDetecting Handguns in images and videos.Our SolutionWe use Yolov7 instance segmentation model to detect and provide coordinates for handguns.Solution ArchitectureLinux 22.04
YoloDeliverablesTrained model of yolov7 instance segmentationTools usedOpenimages
Roboflow
Yolov7Language/techniques usedPythonModels usedYolov7_maskSkills usedDeeplearning
ProgrammingWhat are the technical Challenges Faced during Project ExecutionRetrieving handgun images in bulk from opensource.How the Technical Challenges were SolvedFound Openimages dataset with good amount of required imagesContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241080,https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/,"Using Graph Technology to Create Single Customer View.$Client BackgroundClient: A leading retail firm in NewzealandIndustry Type:  RetailServices: Retail businessOrganization Size: 100+The ProblemCompanies face issue of having a Single customer under various rows with slightly different information in the same database. This causes unwanted duplication and inaccurate statistics. It also results in inaccurate ad targeting and financial loss.Our SolutionWe leverage graph technology to create a single customer view by using Complex cypher queries  and Graph Algorithms. Solution ArchitectureWe have an Azure VM on which we have installed the Neo4j Database. Deployment architecture is a single Instance because of using the Community version of the software.DeliverablesPopulated Neo4j Database. 
Required Cypher Queries.Tools usedNeo4j
GraphlyticsLanguage/techniques usedJava
Cypher QueryModels usedNode-Relationship modelSkills usedData AnalyticsData EngineeringData ScienceDatabases usedNeo4jWeb Cloud Servers usedAZUREWhat are the technical Challenges Faced during Project ExecutionOnly 1 Difficulty was faced in this Project and that was to migrate data from Elasticsearch to Neo4j.How the Technical Challenges were SolvedResearch and Experimentation.Project SnapshotsContact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241081,https://insights.blackcoffer.com/car-detection-in-satellite-images/,"Car Detection in Satellite Images$Client BackgroundClient: A leading retail firm in the USAIndustry Type:  RetailServices: Retail businessOrganization Size: 100+Project ObjectiveThe objective of this project was to detect cars in satellite images and highlight them using a bounding box.Project DescriptionThe client, Steffen Schneider, approached us with a requirement to develop a Python project that dealt in the field of computer vision. The main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. To achieve this, we decided to use the Darknet model and train it on Yolov4 dataset of cars in satellite images.Our Solution We used Google Colab for coding and training the Darknet model. Kaggle was used to download the Yolov4 dataset of cars in satellite images. We preprocessed the dataset and trained the model on it. Once the model was trained, we tested it on sample satellite images and it worked perfectly fine. Finally, we created a script that detected the cars in an image and highlighted them using a bounding box.Project DeliverablesThe final deliverable was a ipython Notebook presented on Google Colab.Tools usedGoogle Colab, Kaggle, Slack(For Communication)Language/techniques usedPythonModels usedDarknet(CV Model)Skills usedPython programming, AI/ML.What are the technical Challenges Faced during Project ExecutionThe main challenge we faced was related to the pre-processing of the Yolov4 dataset of cars in satellite images. The dataset was large and had to be cleaned and formatted before it could be used for training the model.How the Technical Challenges were SolvedWe used Python programming skills and developed a script that automated the pre-processing of the dataset. This saved us a lot of time and allowed us to focus on training the model.Business ImpactThe project was a success and the client was very happy with the final product. The car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time.Project website urlhttps://colab.research.google.com/drive/1AoeHdZdpi0lWLf3X2G800J0VT_7wJtnEProject Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241082,https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/,"Building a Physics-Informed Neural Network for Circuit Evaluation$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  RetailServices: ConsultingOrganization Size: 100+Project ObjectiveThe objective of this project was to build a Physics Informed Neural Network (PINN) using TensorFlow, which could evaluate circuits based on the parameters provided through a MATLAB simulation.Project DescriptionMohamed provided us with a dataset generated from a MATLAB simulation of a circuit, consisting of various input parameters and the corresponding circuit performance outputs. We were tasked with developing a machine learning model that could accurately predict circuit performance based on the input parameters, while also incorporating the underlying physics principles that govern circuit behavior.Our Solution Our team utilized Jupyter Notebook, Google Colab, Octave, and MATLAB to build the PINN. We used TensorFlow models to build the neural network and Microsoft Excel to clean and preprocess the data. Our team employed Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. We did not use any databases for this project, nor did we use any web/cloud servers.Project DeliverablesThe final deliverable was a functional PINN capable of evaluating circuits based on the provided parameters.Tools usedOur team used Jupyter Notebook, Google Colab, Octave, MATLAB, and Microsoft Excel.Language/techniques usedThe primary languages and techniques we used were Python programming, TensorFlow, and MATLAB.Models usedWe used TensorFlow models to build the neural network for the PINN.Skills usedOur team utilized Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN.Databases usedWe did not use any databases for this project.Web Cloud Servers usedWe did not use any web/cloud servers for this project.What are the technical Challenges Faced during Project ExecutionThe project was very challenging since our team did not have a background in electrical engineering. It was difficult to understand the physics behind the circuit evaluation, and we faced issues when using MATLAB to provide data for the project.How the Technical Challenges were SolvedWe worked with the client to gain a better understanding of the physics behind the circuit evaluation. We also worked with MATLAB experts to help us better understand how to provide data for the project.Business ImpactThe PINN we built for Mohamed Zamil allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process.Project website urlhttps://colab.research.google.com/drive/1HX37MP4Jcb39SWJgkE_5z5n1gQwqWmV9Project Video

Contact DetailsHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthyFor project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you."
Netclan20241083,https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automation/,"Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation$Client BackgroundClient: A leading tech firm in NewzealandIndustry Type:  RetailServices: Retail businessOrganization Size: 100+Project ObjectiveBrodie Johnco had a MongoDB Database that he wanted to connect to a Power BI Dashboard. However, ODBC connectors were not working for his level of subscription, so he needed a cheaper workaround.Project DescriptionBrodie Johnco had a MongoDB Database containing a large amount of data that he wanted to visualize in a Power BI Dashboard. He initially tried to use ODBC connectors to connect his database to Power BI, but ran into issues due to his level of subscription. We were brought in to help find a cheaper workaround.Our solution involved using Python to extract the relevant data from Brodie’s MongoDB Database. We used the Pandas library to create Dataframes, which we then uploaded to Azure Blob Storage as tables. We set up an Azure pipeline that ran a Python script every 30 minutes to update the tables with new data from the database.Our Solution We used Brodie’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added them as tables to Azure Blob Storage and set up a Python script to an Azure pipeline that refreshed every 30 minutes. This allowed us to keep the data in sync and provide Brodie with up-to-date information for his Power BI Dashboard.Project DeliverablesThe final deliverable was a readable CSV file that contained the converted data from the original JSON format.Tools usedJupyter Notebook, Google Colab, Power BI, MongoDB Compass, Microsoft Excel, Azure Blob StorageLanguage/techniques usedPython, Pandas, Azure Cloud StorageSkills usedPython programming, Azure Cloud Storage, data extraction and manipulationDatabases usedMongoDB DatabaseWeb Cloud Servers usedAzure Blob StorageWhat are the technical Challenges Faced during Project ExecutionThe main challenge we faced was finding a way to connect Brodie’s MongoDB Database to his Power BI Dashboard without using ODBC connectors. We overcame this challenge by using Python and Azure Blob Storage to extract and store the relevant data.How the Technical Challenges were SolvedWe solved the issue by using the client’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added these dataframes as tables to Azure Blob Storage and set the Python script to an Azure pipeline that refreshed every 30 minutes. This allowed the client to access the data in Power BI without the need for ODBC connectors.Business ImpactOur solution allowed Brodie to visualize his data in a Power BI Dashboard without having to pay for expensive ODBC connectors. The Azure Blob Storage solution we implemented was much more cost-effective and provided him with up-to-date information every 30 minutes.Project website urlhttps://github.com/AjayBidyarthy/Brodie-Johnco"
Netclan20241084,https://insights.blackcoffer.com/data-transformation/,"Data Transformation$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  RetailServices: Retail businessOrganization Size: 100+Project ObjectiveThe objective of this project was to convert dirty JSON data present in a CSV file to a readable CSV file. The CSV file contained data in JSON format, which was split into columns in an Excel file, making it hard to read. The client wanted the data to be extracted and converted into a readable format to perform further analysis on it.Project DescriptionOur client had provided us with a CSV file that contained data in JSON format, which was split into columns in an Excel file. The data was hard to read and understand, making it difficult to perform any analysis on it. Our objective was to extract the data, convert it to a readable format, and validate the JSON file to ensure that it was in a correct format. Finally, we had to convert the JSON data into a CSV file that could be easily read and analyzed.Our Solution To extract the data, we used Python programming language and Pandas library. We extracted every piece of text present in the Excel sheet using Pandas and converted it into a readable text format. We then validated the JSON file with a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas again to convert the JSON data into a CSV file that could be easily read and analyzed.To perform the conversion, we used Jupyter Notebook, Json Validator, and Microsoft Excel.Project DeliverablesThe final deliverable was a readable CSV file that contained the converted data from the original JSON format.Tools usedJupyter Notebook, Json Validator, and Microsoft Excel.Language/techniques usedPython programming language and Pandas library.Skills usedPython programming and Pandas data manipulation.What are the technical Challenges Faced during Project ExecutionThe main technical challenge we faced during the project was dealing with dirty JSON data present in a CSV file that was split into columns in an Excel file. This made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format.How the Technical Challenges were SolvedWe solved the technical challenges by using Python programming language and Pandas library to extract and manipulate the data. We validated the JSON data using a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas to convert the JSON data into a readable CSV file that could be easily analyzed.Business ImpactThe business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand.Project website urlhttps://colab.research.google.com/drive/1yWDj8_HXu6hOYatrzWQ3ezqBxsUON3JYHere are my contact details:Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthyFor project discussions and daily updates, would you like to use Slack, or Skype or Whatsapp? Please recommend, what would work best for you."
Netclan20241085,https://insights.blackcoffer.com/e-commerce-store-analysis-purchase-behavior-ad-spend-conversion-traffic-etc/,"E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…$Client BackgroundClient: A leading retail firm in the USAIndustry Type:  RetailServices: Retail businessOrganization Size: 100+Project ObjectiveTo create a well-designed and informative dashboard for Symbiome e-commerce website using data sourced from Bigquery Database, Google Ads, Google Analytics, and Facebook Ads.Project DescriptionOur client, Arik Oganesian, approached us with a requirement to create a dashboard for his friend’s e-commerce website, Symbiome. The dashboard needed to be visually appealing and provide comprehensive insights into the website’s performance. We sourced data from various sources such as Bigquery Database, Google Ads, Google Analytics, and Facebook Ads. To create the dashboard, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. The client specifically asked for cohort retention and cohort revenue charts to be included in the dashboard. With our expertise in data analytics, we were able to fulfill the client’s requirements and provide a dashboard that helped the client make data-driven decisions.Our SolutionWe used Google Data Studio to create the dashboard and Google Sheets to link the data sources. To extract data from Bigquery Database, we used SQL language. We created a set of charts including cohort retention and cohort revenue charts to fulfill the client’s requirements.Project DeliverablesSymbiome E-commerce DashboardTools usedGoogle Data Studio and Google SheetsLanguage/techniques usedSQL for BigquerySkills usedData analyticsDatabases usedBigquery DatabaseWhat are the technical Challenges Faced during Project ExecutionOne of the major challenges we faced was extracting data from Bigquery Database using SQL language. However, we were able to overcome this challenge by using our expertise in data analytics.How the Technical Challenges were SolvedTo solve this issue, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. By using these tools, we were able to integrate the data from different sources and create a single comprehensive dashboard that met the client’s requirements.Business ImpactThe dashboard we created provided a clear view of the website’s performance and helped the client to make data-driven decisions. This resulted in an increase in website traffic and revenue.Project Snapshots Project website urlhttps://lookerstudio.google.com/u/1/reporting/c25c55ae-8052-4166-b363-347a2f8059da/page/SI6uCProject Video"
Netclan20241086,https://insights.blackcoffer.com/kpi-dashboard-for-accountants/,"KPI Dashboard for Accountants$Client BackgroundClient: A leading accounting firm in the USAIndustry Type:  Finance and AccoutingServices: Accounting and financial servicesOrganization Size: 100+Project ObjectiveThe objective of the project was to create a simple and easy-to-use dashboard for the accounting firm Tech 4 Accountants to track their highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.Project DescriptionOur client, Andrew Lassise, wanted a KPI dashboard for Tech 4 Accountants that would help them track their business performance easily. The dashboard needed to have various charts and tables that would display important KPIs in a visually appealing manner.Our SolutionTo achieve our client’s objectives, we used Google Data Studio and Google Sheets to create a visually appealing and easy-to-use KPI dashboard. We created various charts and tables that displayed the KPIs that our client wanted to track. We used Google Sheets to store the data and created visualizations using Data Studio.Project DeliverablesWe delivered a KPI dashboard for Tech 4 Accountants that included charts and tables for tracking the highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.Tools usedGoogle Data Studio and Google SheetsSkills usedData AnalyticsWhat are the technical Challenges Faced during Project ExecutionThere were no major technical challenges faced during the project execution as the data was stored in Google Sheets, and Data Studio allowed us to easily create visualizations using the data.How the Technical Challenges were SolvedNo major technical challenges were encountered, and the project was completed smoothly.Business ImpactThe KPI dashboard that we created for Tech 4 Accountants allowed them to track their business performance easily and make informed decisions. The dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly.Project SnapshotsProject website url
https://lookerstudio.google.com/u/1/reporting/fbf7879a-be79-4cb9-b7d4-783bf7447902/page/Hmg2C
Project Video"
Netclan20241087,https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-analytics-using-etl-and-dashboard/,"Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard$Client BackgroundClient: A leading ad firm in IndiaIndustry Type:  AdsServices: Ads, Marketing, and PromotionsOrganization Size: 100+The ProblemThe main problem that was addressed in this project was the manual calculation of Return on Advertising Spend (ROAS) due to the lack of a centralized platform for running ads. The client’s ads were spread across multiple revenue generating platforms, including Google Adsense, Adx, and Ezoic, while the spending was managed through the Google Ads Platform. At that time, the client lacked a centralized dashboard or website that could effectively calculate ROAS by integrating revenue and cost streams. This fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. Therefore, a comprehensive solution was developed and implemented, providing a centralized platform for calculating ROAS, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments. Our SolutionWe developed a comprehensive solution to address the challenges faced by the client in calculating Return on Advertising Spend (ROAS) and centralizing their advertising data. The solution involved collecting data from four different APIs: Google Ads API for spending data, Google Adsense API, Ad Manager API, and Ezoic data for revenue data. To ensure compatibility, we utilized an Extract, Transform, Load (ETL) tool to convert the data received from each API, which was in different formats, into a standardized format storing them Pandas Dataframe for both revenue and spending data.The transformed data was then stored in a Postgres database for easy access and management. To automate the data extraction process, we implemented an ETL script that runs twice daily via cronjob on a Digital Ocean VM, ensuring the latest data is always available.Moreover, we designed a backend API using the Flask framework. This API fetched the required data from the Postgres DB, allowing users to retrieve relevant information efficiently.Finally, we implemented a ROAS Dashboard frontend to display the calculated ROAS using the fetched values. The dashboard provided a visually appealing and intuitive interface for users to track and monitor their advertising performance. With our solution in place, the client could now easily monitor ROAS over time, access consolidated data, and make informed decisions regarding their advertising investments.Solution ArchitectureThe solution architecture involved a multi-step process to address the challenges faced by the client in calculating ROAS and centralizing their advertising data. Data was collected from various APIs, including Google Ads API, Google Adsense API, Ad Manager API, and Ezoic data, and transformed into a standardized format using an ETL tool. The transformed data was stored in a Postgres database, and a backend API was developed using the Flask framework to fetch the required data. The calculated ROAS was then displayed on a Next Js Dashboard, providing users with an intuitive interface to track and analyze their advertising performance.DeliverablesETL Tool
Deployment on Digital Ocean
Backend API
Next js backend/ frontend
ROAS DashboardTools usedGoogle Ads API
Google AdSense API
Adx API
Ezoic API
Python 3.9
Jupyter Notebook
Flask
Digital Ocean Droplet
Next Js frontend/backend Stack
Vuexy Template for ROAS DashboardLanguage/techniques usedPython 3.9Flask APIDigitalOcean DropletFunctional Programming in PythonETL ToolSkills usedPythonGitDeploymentData EngineeringWeb Development using Next jsDatabases usedWe used PostgreSQL database for the project.Web Cloud Servers usedDigital Ocean DropletWhat are the technical Challenges Faced during Project ExecutionSome of the technical challenges encountered were:Ensuring data integrity during the transformation process.
Deployment of Docker image on VM
Setting up an automated ETL pipeline.
Adding SSL certificate to backend API.How the Technical Challenges were Solved1. Ensuring data integrity: Implemented checks, cleansing, and validation to maintain the accuracy and reliability of the data.2. Docker image deployment on VM: Configured VM to support Docker Image for ETL and deployed the image for seamless execution.3. Setting up automated ETL pipeline: Automated data extraction, transformation, and loading processes for efficient data management via cronjob.4. Adding SSL certificate to backend API: Secured backend API with SSL certificate, enabling encrypted communication for enhanced data protection.Business ImpactThe implemented solution had a significant positive impact on the client’s business. By providing a centralized platform for calculating ROAS and integrating data from multiple revenue-generating platforms, the client gained valuable insights into the effectiveness of their advertising campaigns. The availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. The user-friendly interface of the RAOS Dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. Overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcomes.Project Snapshots Here are the project snapshots:Login ScreenLanding page with first selected campaign in the list:Using Date PickerSearch FunctionalityRevenue Breakdown by PlatformShow/Hide Left SidebarSwitching Site’s theme to Light ModeSettings/Log Out MenuChange Email/PasswordProject Website URL:https://roasing.com/Project Video"
Netclan20241088,https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/,"Ranking customer behaviours for business strategy$Client BackgroundClient: A Leading Retail Firm in the USAIndustry Type: RetailServices: Retail BusinessOrganization Size: 100+The ProblemCreate an API service that will parse text, include comments, analyse the remarks, assign a score based on sentiment or other criteria, etc. Feed it comments, and it should analyse the syntax and sentiment of the comments as well as extract key terms to add to the extended meta data of that model. In order for us to know a user’s behaviour, personal information, and more meta data about their interestsOur SolutionCreated a flask API, that will take comments as input and will textual analysis as follows:Spell and Grammar Check: We have used language tool python for this , LanguageTool is an open-source grammar tool, also known as the spellchecker for OpenOffice. This library allows you to detect grammar errors and spelling mistakes through a Python script or through a command-line interface. Sentimental Analysis: For Sentimental Analysis we used FLAIR, Flair is a pre-trained embedding-based model. This means that each word is represented inside a vector space. Words with vector representations most similar to another word are often used in the same context. This allows us, to, therefore, determine the sentiment of any given vector, and therefore, any given sentence. Keywords Extraction: For keywords extraction we used SPACY which is newer than NLTK or Scikit-Learn, is aimed at making deep learning for text data analysis as simple as possible. The following are the procedures involved in extracting keywords from a text using spacy.Split the input text content by tokensExtract the hot words from the token list.Set the hot words as the words with pos tag “PROPN“, “ADJ“, or “NOUN“. (POS tag list is customizable)Find the most common T number of hot words from the listSolution ArchitectureDeliverablesCommentScoringAPI that will take comments/reviews as input, and do the textual analysis on the given comment and will return the Comment Score based on counts of spell and grammar errors, sentiments, hot keywords.Tools usedNumpy, pandas, flask, NLTK, Spacy (Keyword Extraction), language tool python (spell and grammar check), flair (Sentimental Analysis)Language/techniques usedPython Business ImpactClient have a user schema that contain all the information of users that have visited there platform, and he/she want to build a Script that will take all the reviews of a certain User as input and than will do textual analysis on all the comments of the user , by textual analysis we mean Spell and Grammar Check, Sentimental Analysis, and Keywords extraction. Based on these factors our Script scored each user and helped Client to understand his/her users well."
Netclan20241089,https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/,"Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.$Client BackgroundClient: A Leading Trading Firm in the USAIndustry Type: FinanceServices: Trading, Consulting, SoftwareOrganization Size: 100+The ProblemA Trading site will have all the required features, allowing users to trade in multiple commodities markets, like Forex, Agriculture, Metals, Energy etc. Our SolutionDesigned the website with technical indicators, and the ability to trade in live market, plus allows the user to create his/her own strategy to backtest. Functionalities like all types of technical indicators:Trend followingmean reversionrelative strengthvolumemomentum.Strategies are specific scripts, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. Backtesting is the process of recreating the work of your strategies on historical data, essentially all of your past strategic work. Forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data.Solution ArchitectureDeliverablesA Fully functional trading platform that lets you customize technical indicators, create charts, and analyse financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators:Trend followingmean reversionrelative strengthvolumemomentum.Tools usedNumpypandasLanguage/techniques usedPython Business ImpactClients want a social media network, analysis platform, and mobile app for traders and investors. So we designed a website with all the client’s requirements, where traders, investors, educators, and market enthusiasts can connect to share ideas and talk about the market. By actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to backtest. A Fully functional trading platform that lets you customize technical indicators, create charts and analyze financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicatorsProject Snapshots"
Netclan20241090,https://insights.blackcoffer.com/trading-bot-for-forex/,"Trading Bot for FOREX$Client BackgroundClient: A Leading Trading Firm in the USAIndustry Type: FinanceServices: Trading, ConsultingOrganization Size: 100+The ProblemAutomate trading on the MT4 terminal for forex when certain conditions are met, and end trade at the best exit point.Save mt4 forex data for a instrument live for every tick.Our SolutionUse PyTrader to log into trading system (mt4) for 2 brokers.Use live prices to identify when prices diverge.Buy one currency on broker 1, sell currency on broker 2.Hold until prices come back together.Coded a MQL4 script that will save tick data (bid, ask, open, high, low, close) for any instrument when activeSolution ArchitectureDeliverablesPython Script to Automate the two Meta Trader 4 terminals, and trade when some conditions are true and break the trade at a exit point.A MQL4 Sript that will Save the Live tick data (Bid, Ask, Spread, Open, High, Low, Close) in a CSV file. Tools usedPyTradernumpypandasLanguage/techniques usedPython (Automation)Mql4 (To save tick data)Business ImpactClient requirements were  to automate his forex trading strategy  on Meta Trader4 terminal, so that he doesn’t have to bother trading anymore, the Python script we designed to not only do it, plus it offers a safe exit point for Ongoing Trades, that saved the client’s money and time."
Netclan20241091,https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-investment-purposes%ef%bf%bc/,"Python model for the analysis of sector-specific stock ETFs for investment purposes$Client BackgroundClient: A Leading Investment Firm in the USAIndustry Type: FinanceServices: Investment, ConsultingOrganization Size: 100+The ProblemHave an existing Python model that has been built for the analysis of sector-specific stock ETFs for investment purposes. Need to update the existing selection criteria to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameters of the selection criteria to test different variables.Our SolutionThe 2 in 4 Fundamental model screens a fundamental ranking of stock market sectors, picks the top ranked holding and continues to hold that sector as long as it remains in the top four rankings.  The model holds two positions at a time.  The sector ranking data is in the wcm5.xlxs file.  We input data from the PRICES.CSV file to pull up monthly returns.  When I go to run the program, I use the 2_in_4_New.py and that give me the current rankings for both the fundamental and technical rankings.Sometimes a sector is ranked as being fundamentally attractive because it has become cheaper because of problems going on within an industry.  What I would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period.  Here is what the new model would do.  Screen for a the specific number of sectors, probably between three and five, based upon the fundamental ranking over an average time period (currently 3 weeks)Choose either three, four, or five holdingsExclude the holding that has the weakest performance over a specify lookback period, let’s start with 52 weeks, but I would like to be able to adjust this variablecompare the performance of various combinations, seeing the return on an annual basis if possible, as well as showing the maximum drawdownSolution ArchitectureDeliverablesAn Updated, Optimised Python script that will filter and return Technical and Financial holdings, with a Price filter that will do price analysis on a certain lookback period.Tools usedNumpypandasitertools, combinations permutationsLanguage/techniques usedPython Business ImpactThe client now can get more than 2 Financial and technical holdings , up to maximum 5 holdings for both Technical and Financial, plus the holdings were more accurate because of the new added Price Filter that will Exclude the holding that has the weakest performance over a specify lookback period, default 52 weeks. It boosted the Client’s profit because of the more accurate and optimised functional filters.Project Snapshots"
Netclan20241092,https://insights.blackcoffer.com/medical-classification/,"Medical Classification$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT ConsultingServices: Software, ConsultingOrganization Size: 100+Project ObjectiveClassify the medical research paper into 0 if the medical research paper cannot be used in future medical research and 1 if the medical research paper can be used in research based on some research-related phrases.Train an ML/DL model on classified data.Project Description We have given an excel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. If the phrase is not present in a research paper then it will not be used for research. After annotation, we need to find the best ML/DL model to train research data and evaluate the model on test data.Our SolutionWe have created a python script that can compare all medical research paper text to research phrases and annot 0 if research phrases are not present in a medical research paper and 1 if research phrases present in medical research paper. After annotation we have trained different machine learning and deep learning models like Bert base uncased using Tensorflow, bert large, XGBoost Classifier, Random Forest Classifier and Logistic Regression. Among these models we have chosen the best accuracy  parameters model. In our case the bert-base model performed good and gave 95% test accuracy. Project DeliverablesML/DL model which is trained on medical research classification data to classify other medical research papers.Tools usedGoogle Colab notebooks, Tensorflow, PyTorch, Transformers, MS ExcelLanguage/techniques usedPython, Machine learning, Deep learning, Data Science, Natural Language Processing (NLP).Models usedTensorflow-Bert model, PyTorch LSTM model, Random Forest Classifier, XGBoost Classifier, Logistic Regression.  Skills usedMachine Learning, Deep learning, NLP, Python programming. Databases usedused ms excel dataWhat are the technical Challenges Faced during Project ExecutionThere are various technical challenges faced during project execution:The research paper has a huge amount of text data so the model was giving space errors in colab notebooks.Find the best threshold value which gives best test accuracy. How the Technical Challenges were SolvedTo solve space error we have trained the model with lower batch size so this solved the error.To find the best threshold value we created the ROC AUC curve and Precision  Recall curve and checked best points where accuracy will be higher."
Netclan20241093,https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visualization/,"Design & Develop BERT Question Answering model explanations with visualization$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT ConsultingServices: Software, ConsultingOrganization Size: 100+Project DescriptionWe need to use a pre-trained bert question answering model and create a notebook that has explanations of model’s working with some visuals of bertviz, allennlp and gradient values.Our SolutionWe created a notebook first and explained the model with model view and head view visuals of bertviz library. It gives similarity between words so we can easily find related words.We used the allennlp library and created bar charts and heatmaps to show higher and lower attention words. It means when it finds question related words in the context it gives higher value to those words and if words are not related it gives lower values. We used a gradient based method to show higher and lower gradient values word according to question text and created bar charts and text color charts to show higher gradient values.Project DeliverablesA notebook which has an explanation of the bert question answering model using some visualization.Tools usedGoogle colab notebooks, Tensorflow, Bertviz, Allennlp, TransformersLanguage/techniques usedPython programming language, Deep learning, NLP, Data VisualizationModels usedPretrained bert-base-uncased model and distilbert model (both trained on squad2 dataset) Skills usedData visualization, Deep learning, NLP, python What are the technical Challenges Faced during Project ExecutionWe need to use the best pre-trained model which can give good results on different questions and answers.We were working on text data so we need to use charts which can clearly show differences between higher attention and lower attention value words.  How the Technical Challenges were SolvedFor best pretrained we tried different Bert’s pretrained models like distilbert(trained on squad dataset), distilbert(trained on squad2), bert base uncased, bert large and roberta base.Among these models we kept the best one. For solving charts related issues we used heatmap chart, bar chart with dark and light colors and text coloring method.Project Snapshots"
Netclan20241094,https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-problems/,"Design and develop solution to anomaly detection classification problems$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT ConsultingServices: Software, ConsultingOrganization Size: 100+Project DescriptionWe need to create a notebook with solutions to binary classification-related anomaly detection problems. We need to use machine learning and deep learning models which have greater than 90% accuracy. Our SolutionWe created a notebook for anomaly detection. We used 10 to 15 machine learning and deep learning models but only  3 different types of auto encoder models that were giving greater than 90% accuracy. We trained all 3 models on one classification data which have anomalies and evaluated trained models on test data.Project DeliverablesA notebook that has solutions for anomaly detection related classification problems and accuracy should be above 90%.Tools usedGoogle colab notebooks, Tensorflow, Google driveLanguage/techniques usedPython programming language, Machine learning, Deep learning, Data analysis and Data visualization.Models usedAuto Encoder and Variational Auto EncoderSkills usedPython, Data Analysis, Data visualization, Machine learning, Deep learning.Databases usedMS ExcelWhat are the technical Challenges Faced during Project ExecutionMost of the anomaly detection models work with regression type data and this problem was classification problem so we need to deal with classification data.Getting high accuracy is also a tough challenge for us because there are only a few models which work well on anomaly detection related classification problems.How the Technical Challenges were SolvedSo we have limited models for this problem so we used only classification models like Autoencoders, Isolation forest and one class svm. Only Autoencoder was giving high accuracy so we worked with different types of autoencoders like variational autoencoder and normal autoencoder.Project Snapshots"
Netclan20241095,https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/,"An ETL Solution for Currency Data to Google Big Query$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT ConsultingServices: Software, ConsultingOrganization Size: 100+Project ObjectiveFetch currency data from Pure-clear API and store it to Google cloud BigQuery.Create a Google cloud function to automate the above process.Project Description We have given a pure-clear API and a google cloud account. We need to fetch currency data from that pure-clear API using python and need to store fetched data in Google Cloud Bigquery.We also need to automate the above process like the process runs on a daily basis and update the currency data on Bigquery.Our SolutionWe have created a python program that can fetch pure-clear API data. The API data was in JSON format but we needed table format so we used python package pandas. We converted json data to tabular format using pandas. After that, we connected python code to google cloud using google’s authentication module and then stored data frame (table) directly to BigQuery using the “.to_gbq” method. We also need to run the above process daily to update new data in BigQuery. For this Google cloud provides a “Cloud function” tool. In this, we can create a function and set up their running process. So we created a function and attached the above code to that function and set up a cloud function to run daily. Project DeliverablesA Google cloud function that runs daily and updates data on Google BigQueryTools usedCloud function, BigQuery of Google Cloud, Google Colab notebook, Python programming, PandasLanguage/techniques usedPython language and pandas moduleSkills usedPython programming, Data handling, Google CloudDatabases usedGoogle Cloud BigQueryWeb Cloud Servers usedGoogle Cloud ServerWhat are the technical Challenges Faced during Project ExecutionConnecting google cloud to python code is challenging because Its credentials should be in a specified format otherwise it shows an authentication error.How the Technical Challenges were SolvedTo tackle this challenge we created a dictionary format (key-value pair) and stored all the authentication variables in the dictionary as a key value pair. Then we used google’s authentication library “google.auth” and passed a dictionary to the service_account method and stored it in different variables so we can store data from pandas dataframe to Google BigQuery.Project Snapshots"
Netclan20241096,https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/,"ETL and MLOps Infrastructure for Blockchain Analytics$Client BackgroundClient: A Leading Blockchain Tech Firm in the USAIndustry Type: AR/VRServices: Metaverse, NFT, Digital CurrencyOrganization Size: 100+Project ObjectiveCode for extraction of the price of cryptocurrency Required real-time data of cryptocurrency and this is extracted from the cryptocurrency URL Forecast code for prediction of the priceBuilt FastApi to reduce interaction complexity for the user Project DescriptionETL and MLOps Infrastructure for Blockchain Analytics this entire project completes in 4 outlines and stages. In the first segment data scraping for the price of the cryptocurrency. The second stage is, Loading the data into the Microsoft MYSQL server and Transforming data into the required shape for the automated process data Load into the Amazon RDS tool management service which knows as the Amazon relational database service, and creating DB instances (DB instance class – db.t3.small).In the fourth stage, built the FastAPI for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding.Our Solution           This Project Module develops according to the Client’s Requirements which involves Data extraction of Cryptocurrency data from a given URL by the Client, it also changes the data format, and attributes nomenclature according to the requirements. After extracting the data its loads into Microsoft MYSQL Server for the transformation of data and for full automation process, used Amazon RDS and built the FastAPI.Project Deliverables–  Data Scraping code using Python –  ETL code for extracting, Transform and Loading into Microsoft MYSQL server–  AWS RDS (db.t3.samll) instances for storing data and for deployment –  Built FastAPI for getting the price of cryptocurrency  Tools used       – VC code and Google Collab    – Microsoft MYSQL server– AWS RDS services Language/techniques usedData Scraping using Python ETL process to extract, load, and transform the dataFastAPI using PythonAmazon Cloud services Skills used – Data scraping using python– ETL setup– Aws web services– FastAPI using PythonDatabases used– Microsoft MYSQL server– Aws RDS (Amazon Relational Database services)Web Cloud Servers used -AWS RDS servicesWhat are the technical Challenges Faced during Project ExecutionData scraping speed does not meet the expected speed (events/sec)API calls have their own limitation in requesting calls per secStoring the huge amount of dataHow the Technical Challenges were SolvedGet the Premium service of API calls (20 calls/sec)Used the AWS RDS for storing the data and for faster execution Business ImpactThis Project impact is directly responsible to the investors of the cryptocurrency.To get the prices of cryptocurrency on fingers tips and use it for buying and investing money in the right corner of the cap market of finance.It clearly impacts financially to the investors and helps them for investing purposes.The scope impact of product service is worldwide for purchasing any cryptocurrency in the world.To provide these impactful services, there is a tech team of Blackcoffer behind it. Project Snapshots Project website URL 127.0.0.1:62190Project Videohttps://www.youtube.com/watch?v=xDeL5YggxDw&ab_channel=Blackcoffer"
Netclan20241097,https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/,"An agent-based model of a Virtual Power Plant (VPP)$Client BackgroundClient: A Leading Energy Firm in the USAIndustry Type: EnergyServices: Power, Energy, DistributionOrganization Size: 5000+Project ObjectiveTo create an agent based model of a virtual power plant in Netlogo. To see the function of multiple such power plants that worked simultaneously. These power plants created and supplied energy based on a demand parameter that can be controlled by the observerProject DescriptionThe client defined specific requirements as to how he wanted the model to be. The requirements were divided into 4 parts. Each successive part increased in complexity and required the model to be adjusted or configured to fit that part into itThe entire model when completed contained all the four parts defined by the client in the Statement of work. Our SolutionCreated the model according to requirements. The clustering of multiple agents and their position is decided mathematically based on the total number of agents and the sum of their energies. The agents form a cluster based on the condition that the sum of their power is a figure that is above a certain threshold amount, the threshold amount is also decided by the observer.Project Deliverableshttps://github.com/AjayBidyarthy/Shingi-Samudzi-Build-Netlogo-ABM-for-simulating-Virtual-Power-Grid-economicsAbove is the github link to every state of the model that was delivered to the client. The uploads start from a basic model with only clustering of the agentsThe final upload is a model that contains the full representation of a VPP for simulation. Tools used-Netlogo– python Language/techniques usedNetlogo uses a specific language that resembles the logo language but has it’s unique syntax and variations in the way variables are stored and how a list is parsed Models usedClusteringSkills usedNetlogo programming What are the technical Challenges Faced during Project ExecutionThe major challenge was controlling the behavior of each agent in the model. The lack of understanding of the language and the available resources about it made it challenging to figure out the actual behavior of the agents and the overall model. The decision to decide where exactly each agent will cluster on the grid was difficult primarily because each agent spawned on a random patch of the screen. This meant that each agent would have to be given a spot to land on and form a cluster with other agents. The next challenge was deciding the condition on which the agents will cluster as their relative distance to each other couldn’t be used as a parameter as it wasn’t relevant to the model’s purpose. How the Technical Challenges were SolvedThe technical challenges were solved by extensive research and referring to several forums over the span of 2 months. Project SnapshotsProject Videohttps://www.youtube.com/watch?v=1fzCUzZ0q0Q&ab_channel=Blackcoffer"
Netclan20241098,https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/,"Transform API into SDK library and widget$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: ITServices: Consulting, Marketing, HealthtechOrganization Size: 500+Project ObjectiveConvert API documentation into SDK library and widget. Expected deliverables are SDK library and widgets forWeb appsiOS appsAndroid AppsProject DescriptionAPI documentation is available for a tool that allows customers to type in their medication and find the cheapest price near them. For partners who want to have it on their own site, currently using the API documentation but would like to ultimately be able to send them an embeddable widget that incorporates the tool on their siteOur SolutionWe created a flutter widget that uses  SDK libraries that allows the customer to type their medication and find the cheapest price near them.This widget can be embedded in their web, android and IOS applicationsProject Deliverables1)SDK Library/Widget2)Sample flutter applicationTools usedFlutterLanguage/techniques usedDartSkills used1)Knowledge of dart language2)flutter app developingWhat are the technical Challenges Faced during Project Execution1 )Problems while fetching details of drugs and pharmacies2) Showing details of drugs and pharmacies in the widgetHow the Technical Challenges were SolvedAll technical challenges are solved by proper communication with the client and by logical analyzing of dataProject SnapshotsProject Videohttps://www.youtube.com/watch?v=MyNK_DPtsKA&ab_channel=Blackcoffer"
Netclan20241099,https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/,"Integration of a product to a cloud-based CRM platform$Client BackgroundClient: A Leading Logistics Firm WorldwideIndustry Type: LogisticsServices: Import, Export, Supply Chain, Logistics, TradesOrganization Size: 500+Project DescriptionThe main challenge faced by the team was the integration of the two systems themselves.Since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automation using APIs.Our SolutionThe challenge was divided into two milestones and sub-tasks for each.1. First was the ingestion of existing data into the cloud-based CRM platform.2. Second was the question of automating the process of adding newer records to the cloud platform. Project DeliverablesThe client has been provided with python scripting handling bulk data ingestion to CRM and also the script to handle daily synchronization of data.Tools used– Python– MySQL Database– Postman– TeamViewerLanguage/techniques used– Automation– 3rd party APIs– Authentication methods– Multi-Threading of function calls– bat Scripts for easier running of scripts for the clientModels usedPython Frameworks like requests to build own custom client for consumption of APIs.Skills usedPython Programming, Mult-threading, APIs Databases usedThe client provided a MySQL instance.Web Cloud Servers usedZoho What are the technical Challenges Faced during Project Execution?– Writing own client-side API-consumption code handling API calls from Authentication and Other Operations as per task requirements.– Debugging of API responses was messy.How the Technical Challenges were Solved– Multiple alternatives were discussed and implemented in python like conditional refreshing of API tokens.– Automation of daily synchronization handled by use of time deltas.– Logging of all operations to efficiently handle errors in the future.Business Impact– Automated workflow of the client– No need for dull tasks like data entry to CRM modules everything is taken care of using logic.URLhttps://www.exportgenius.in/"
Netclan20241100,https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-records/,"A web-based dashboard for the filtered data retrieval of land records$Client BackgroundClient: A Leading Real Estate Firm in the USAIndustry Type: Real EstateServices: Land, Infrastructure, Real Estate, InvestmentOrganization Size: 100+Project DescriptionThe client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish.Our SolutionCreated the web application as per client needs.Added user functionality to handle signup/logins and added authorization middlewares to protect routes from unwanted access.Transformed raw data into a meaningful NoSQL-based database with a proper schema being served as an instance on a cloud service named ‘ MongoDB Atlas ‘.Project DeliverablesPushed code to the required GitHub repository.Tools used– Vanilla javascript– Javascript Frameworks ( Nodejs, express , cors )– PostmanLanguage/techniques used– JavsScript– Backend Service setup ( express, cors , js )– Fronted logic setup ( HTML , CSS , JavaScript , Jquery )Models usedBackend: An API service created to handle land records database and queries made by users.Frontend: A frontend client is available as a web application where users can signup and access land records. Skills usedJavaScript Programming, APIs, JavaScript Frameworks ( NodeJS, Express  , cors ) , Web Design, NoSQL querying in MongoDB.Databases usedMongoDB (NoSQL)Web Cloud Servers usedMongoDB AtlasWhat are the technical Challenges Faced during Project Execution– UI component creation– User authorization middleware creation– Querying data in NoSQLHow the Technical Challenges were Solved– Created and extended UI components to handle filters like owners, date fields, and area ranges on land records.– API and Frontend are separately built for easier team management of tasks.– Using a cloud-based MongoDB instance provided support for teams to work without any problems with accessibility.Business Impact– Created a platform for clients’ business.– Transformed his raw data into meaningful business applications."
Netclan20241101,https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/,"Integration of video-conferencing data to the existing web app$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project DescriptionIntegration of 3rd party APIs to client’s platform.Client required meeting/conference data from sites like gotomeeting/zoom.Our SolutionUsing APIs fetched data from different platform and rendered data into client’s application.Modifed web application with a UI to handle form data accepting dates as a timeframe – which then makes a request to the API being handled at server end and returns the meeting data from the required source.Project DeliverablesPushed code to client’s github repository.Tools used– Python– PostmanLanguage/techniques used– Automation– 3rd party APIs– Authenication methods– Multi-Threading of function calls ( authentication of api client )– UI component design to get dates from user-endModels usedPython Framework- Django , requestsSkills usedPython Programming, APIs , Multi-threading , Web DevelopementDatabases usedDefault project postgreSQLWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project Execution– UI creation for handling form data– Managing and Validating form data to process request at server endHow the Technical Challenges were Solved– Created autmated functions as views in django to handle requests made to video-conferencing platform.– Which then returns meeting data as per user’s wish.Business Impact– Instead of extracting meeting data and adding it to all usersany authorized user can get meeting data as his wish.Project website urlhttps://www.codanalytics.net/"
Netclan20241102,https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-added-video/,"Design & develop an app in retool which shows the progress of the added video$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project DescriptionThe objective was to develop a progress bar that can help costumes to estimate the analytics of the video. Our SolutionThe client wanted a progress bar with the following filters:Date filter: – Update the progress bar and count of the videos according to the date selectedCategory filter: – Update the progress bar and the count of the videos according to the selected categoryWe have created a SQL query for getting a count of the videos from the full video table according to the filter selected in the appIn added video table some columns were missing to solve this we created a SQL query for joining the added video table to the other tables and return the count of the video according to the filter selectedProject DeliverablesApp in retoolTools usedRetoolLanguage/techniques usedSQL Skills usedSQLDatabases usedSQL DatabaseWhat are the technical Challenges Faced during Project ExecutionClient wanted date filter and a video category filter but this data was not there in added video tableHow the Technical Challenges were SolvedWe had to join multiple data so that we can get category column and date column for applying filter Project SnapshotsProject Video"
Netclan20241103,https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/,"Auvik, Connectwise integration in Grafana$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveGet statistics such as uptime,  availability, cpu throughput etc. from Auvik and Connectwise and make a dashboard from it in Grafana.Project DescriptionUnlike many technologies for which plugins are readily available in Grafana, there are none for auvik and Connectwise. So our task was to device a solution through which all the data from Auvik and Connectwise can be fed to Grafana. This data then would be used to plot graphs in Grafana.Our Solution
Setup Postgres on linux
Create appropriate databases, tables and users in it.
Use python to get data from Auvik and Connectwise and perform necessary preprocesing.
In the same python file, Connect to our postgres database.
Ingest this data into postgres database.
Setup Grafana.
Connect Grafana to postgres using the postgres plugin.
Query our postgres database in Grafana to get desired results.
Plot multiple graphs according to client’s requirement and make a dashboard from it
Project Deliverables
Setup Postgres
Setup Postrges in Grafana
Write Python code to get data from Auvik and Connectwise into Postrges
Plot graphs into Grafana according to client’s requirement
Make dashboards for all the graphs
Tools usedGrafanaPostgresVs CodeAWSPostmanLanguage/techniques usedPythonbashSkills usedPythonnetworkingData visualisationDatabases usedPostgresWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSince, the data received from Auvik was in Json fromat, our first approach was to use Grafana’s built-in Json plugin. But this wasn’t working since, the data received from Auvik was multi-dimensional when the Json plugin required One dimensional data. How the Technical Challenges were SolvedThe above challenge was addressed by transforming the multi- dimensional data into one dimensional when it was store in a python variable. This transformed data was then inserted into Postgres.Project SnapshotsProject website urlhttps://github.com/AjayBidyarthy/Henry-PardoProject Video
https://www.youtube.com/watch?v=7CcbdfjkBzc&ab_channel=Blackcoffer"
Netclan20241104,https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/,"Data integration and big data performance using Elasticsearch$Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots"
Netclan20241105,https://insights.blackcoffer.com/web-data-connector/,"Web Data Connector$Client BackgroundClient: A Leading Marketing Tech Firm in AustraliaIndustry Type: MarketingServices: Marketing SolutionsOrganization Size: 50+Project ObjectiveTo make a software code that takes data from a source and ingests it into a database present on a server. The scripts should automatically execute after regular intervals of time. Project DescriptionThe client had several data sources that were updated with new data regularly. The client wanted software that triggers itself automatically and takes data from those data sources and ingests it into a database that is hosted on a Linode server. Also, the date parameters in the query should be changed dynamically using the current date. Further, we had to assist in setting up the Tableau BI tool on the client’s PC and connect the Postgres database to the tableau. Our SolutionWe setup a linux server on linode.Install Postgres on this linux server.Create a database and create a new user. Grant this new user all privileges on the database.Create a table within the database. This table has columns with datatypes as specified by the client.Write a python script that makes GET request to the client specified data source and store the response in json format.Inside the python script itself, establish  a connection to our postgres database using the pscopg2 module and user credentials.Ingest the data into postgres using INSERT query in python script.Write code to get the today’s date using the datetime module. Using this, calculate yesterday’s date. Now we can use these as parameters inside our query to the data source.Move these python files to our server.Install and setup Cron on our server. Add the task to run specified python files at regular intervals to Cron.Repeat steps 4 to 11 for every new data source.Project DeliverablesPython ScriptWorking linode server with cron installedTableau installation and connection to postgresProject DocumentationTools usedLinode serverVS Code Language/techniques usedPython BashPSQL.Skills usedPython programmingPostgres SQL Linux scriptingDatabases usedPostgresWeb Cloud Servers usedLinodeWhat are the technical Challenges Faced during Project ExecutionAvoiding duplicates was a challenge. Since Client was living in Australia all the timezone (on server and in code) were changed to AEDT. How the Technical Challenges were SolvedUsed uniqueid Column to check for duplicates. Used pytz module to change timezones.Business ImpactThis solution helps in maintaining a copy of all data sources inside our Postgres database. Also, the data is 24/7 available. Since data inside the Postgres is updated regularly, graphs in the tableau are also up to date.Project SnapshotsProject website urlhttps://github.com/X360pro/Web-connector-for-tableu"
Netclan20241106,https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool-using-retool/,"An app for updating the email id of the user and stripe refund tool using retool$Client BackgroundClient: A Leading Healthcare Tech Firm in the USAIndustry Type: HealthcareServices: Healthcare SolutionsOrganization Size: 200+Project DescriptionThe client needed two apps in retoolUpdate the email id of the customerStripe refund app with two options full payment and partial paymentOur SolutionWe create the following two apps in retoolTakes the old email id of the user and new email id of the user when the update email id is clicked then the old email id is updated with the new email id. For updating email id we have used stripe APIThe user has to select the email id of the user and payment id of the user from the table the user get two options for a refundFull payment: – This option refunds the whole amount to the customerPartial payment: – This option refunds the partial amount entered by the userProject DeliverablesApps in retoolTools usedRetoolStripeLanguage/techniques usedJavaScriptModels usedWe have not used any modelsSkills usedAPI Databases usedStripe databaseWhat are the technical Challenges Faced during Project ExecutionThe main challenge was creating a full payment option using stripe API. If the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amountHow the Technical Challenges were SolvedTo solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in retoolBusiness ImpactUsing this apps it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment Project SnapshotsProject website url"
Netclan20241107,https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text-in-a-given-video/,"An AI ML-based web application that detects the correctness of text in a given video$Client BackgroundClient: A Design & Media firm in the USAIndustry Type: MarketingServices: Consulting, Software, Marketing SolutionsOrganization Size: 100+Project ObjectiveCreate a python web application that detects the text and checks the spelling of written text in the videos and prints the count of wrong spelling in the endProject DescriptionDeveloping a dockerized Django web application for detecting the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on google cloudOur SolutionWe have created a python web application with Django framework when user uploads the video the application run keras-ocr model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. For correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words.Project DeliverablesDeployed dockerized web application on google cloud which generate video with bounding box around textsTools usedDockerRedis ServerDjango Celery NginxOpencv NLTK MoviepyLanguage/techniques usedPythonHtmlCSSJavaScriptModels usedWe have used keras-ocr model for detecting the text form the video and creating the bounding box around the wordsSkills usedNatural language processing,Machine learning,Image processing,Web development,Python programmingDatabases usedDjango Sqlite3, Redis ServerWeb Cloud Servers usedGoogle cloudWhat are the technical Challenges Faced during Project ExecutionRunning model on each frame of the video Show progress bar for the progress of the workHow the Technical Challenges were SolvedFor running the model on each frame of the video we have used celery it runs the model in the backend of the applicationWe have used celery backend progressrecorder and updated it every time when model has detected the text from the frame of the videoProject SnapshotsProject website urlhttp://34.68.134.64/"
Netclan20241108,https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-manager/,"Website Tracking and Insights using Google Analytics, & Google Tag Manager$Client BackgroundClient: A leading marketing firm in the USAIndustry Type: MarketingServices: Consulting, Software, Marketing SolutionsOrganization Size: 400+Project ObjectiveThe project objectives are as follows: Assisting the businesses with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.Setup pixels of Social Media platforms like LinkedIn and Facebook which assists users in tracking conversions.Providing monthly insights on their website performance to analyse the businesses’ strengths and opportunities for growth.Project DescriptionThis project includes assisting businesses with digital analysis for their marketing.Digital analytics allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. The importance of digital analytics is that it allows for a data-driven approach to marketing, and as such it can produce better results.The primary objective of the project is to help the businesses in knowing their target audience, understanding the trends in digital marketing, and providing insights on the analytics part of their website performance. Use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. As the picture of the customer’s needs unfolds, adjust the objectives accordingly.  Our SolutionThe main aim of this project is to assist the businesses to improve their website performance with the use of technologies like Google Analytics, Google Tag Manager and dashboards built on Whatagraph. Google Analytics: Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.Google Tag Manager:Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. Whatagraph Dashboards: The whatgraph dashboards previews the important metrics related to the website including conversions, events, number of users and performance about ads and campaigns by the website. This dashboard helps in drawing some of the useful insights for the website notifying the strengths,gains and areas of improvement. Project DeliverablesMain deliverables for the project are: Setup the Google Analytics and Google Tag Manager for the website. Tracking events on Google Analytics using Tags created in Google Tag Manager.Monthly Reporting of Analytics for businesses on Whatagraph dashboards or via presentations. LinkedIn and Facebook Pixel setup and validation for the website. Setup Goal Conversions for the website to track the important and valuable metrics from the website. Tools usedGoogle Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Whatagraph: To visually represent important metrics like impressions, clicks, goal completions and many more related to Ads management and Google Analytics. Clickup: This tool is used to manage tasks given. Skills usedDigital AnalysisData AnalysisDigital MarketingGoogle AnalyticsWhat are the technical Challenges Faced during Project ExecutionThe main technical challenge faced was that any changes in Google Analytics are operational after 24 hrs. Thus, we can’t judge if the setup works as per required. How the Technical Challenges were SolvedWe had to wait for 24 hours to check the setup. We could use real-time report as well to check the setup on-the spot. Business ImpactThis analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.Some of the common metrics that are important in digital analytics include:Dashboard metrics:Some examples are pages per visit, bounce rate, and average duration of each visit.Most exited pages:Pages with an exit rate of 75–100% show that you need to examine the problem with the content and improve upon it.Most visited pages:These pages will make the customers either exit or explore the website further.Referring websites:These are other websites that link to your website.Conversion rate:This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.Frequency of visitors:This tells you about the loyalty of the customers.Days to the last transaction:This refers to the time lapse between the first visit and the sale. The shorter the time taken, the better it is for your business.Project SnapshotsFigure 1: Google Tag Manager DomainsFigure 2: Google Tags Figure  3: Google Analytics Figure 4: Google AnalyticsFigure 5: Tracking Facebook Pixels for a websiteFigure 6: Whatagraph dashboardFigure 7: Whatagraph Dashboard(Conversions) Project website urlhttps://unite.ca/https://livelike.com/ http://essencelle.ca/ https://www.decorium.com/ https://www.everafterfest.com/2022-tickets/https://winagetaway.com/"
Netclan20241109,https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analytics-and-google-tag-manager/,"Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager$Client BackgroundClient: A Automobile firm in IndiaIndustry Type: AutomobileServices: Retail, AutomobileOrganization Size: 1000+Project ObjectiveThe project objectives are as follows: Assisting the client with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website.Dashboards on website analysis presenting the important metrics and analysis related to websites.Project DescriptionThis project includes assisting the client to study the user flow and behaviour flow of the users on the websites. It had one main website and three other sub websites to analyse the button clicks, impressions and understanding the user’s behaviour on the website. Many events were to be tracked and converted to a dashboard in Google Data Studio to make it simpler to understand. This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.Our SolutionThe main aim of this project is to display the event flow, user flow and behaviour flow through dashboards and analyse them to work on the areas of improvements. Google Analytics: Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.Google Tag Manager:Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. Google Data Studio Dashboards: The dashboards preview the important metrics related to the websites using graphs, tables to understand the trends, patterns in the users. The following steps were carried out for the project:  Get the important metrics for website performance like the number of users visiting the websites, the average session duration, graphs related to the user acquisition like number of new users vs the returning users. This is related to the main website.For the sub websites, track the number of users clicking on specific buttons. Through this I understand the user flow. Compare between the number of users entering the website and those clicking on buttons. Track the metrics related to goal conversion like goal completions, goal conversion rate, goal completion rate and different goals and present it using visualisations.Provide data insights in the end providing scope of improvements and recommendations.Project DeliverablesThe main deliverable for this project were dashboards on Google Data Studio depicting important metrics related to website performance. There were three sub websites for which there were two types of views each. Each of the views had several buttons related to the product. The project was about finding the user flow and event flow on the views.Tools usedGoogle Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Google Data Studio: To visually represent important metrics like impressions, clicks, goal completions using Google Analytics. Skills usedDigital AnalysisData AnalysisData VisualisationsGoogle AnalyticsWhat are the technical Challenges Faced during Project ExecutionThe main technical challenge faced was that there were multiple events setup in Google Analytics for one event and thus identifying a particular one was difficult. How the Technical Challenges were SolvedWe had to communicate with the client to clarify about the event names. Although this took some time but it was necessary since accurateness of data is very essential for the project.Business ImpactThis analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.Some of the common metrics that are important in digital analytics include:Dashboard metrics:Some examples are pages per visit, bounce rate, and average duration of each visit.Conversion rate:This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.Source/Medium Analysis: This analysis helps in understanding the traffic sources and medium on the website. This helps the businesses to work on strengthening the traffic sources to get better reach to the target audience.Traffic Analysis: The overall traffic analysis for the website provides information regarding the important metrics like users,avg. session duration and goal completions according to different source/medium. This will help the  business to analyse different traffic channels performances.Project SnapshotsFigure 1: Tracking of Buttons for Triber Virtual StudioFigure 2: Triber Goal ConversionsFigure 3: Kiger 360 Experience Website Tracking    Figure 4: Traffic Medium AnalysisFigure 5: Overview of Dashboard MetricsFigure 6: Kiger Studio Experience WebsiteProject website urlWebsite URL: https://www.renault.co.in/Dashboard URL:"
Netclan20241110,https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embedding-the-dashboard-to-web-app/,"Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App$Client BackgroundClient: A leading tech firm in the USAIndustry Type: IT ServicesServices: Consulting, Software, Marketing SolutionsOrganization Size: 100+Project ObjectiveCreate a dashboard with Assets Performance With react App. So users can evaluate with Key metrics from data analytics and forecasting.Project DescriptionThe client requires two pages:Screening Asset Performance Portfolio Investing according to criteria and sector-based.Our SolutionBy using Power BI We can achieve this requirement without any additional stack. It requires a subscription to enhance the report.Using Page Navigation and bookmarks to create reports like Web Application with React App.Project DeliverablesAsset Report PageInvestor PageTools usedPower BIAzure AADMongo DB BI ConnectorODBC ConnectorDAX StudioLanguage/techniques usedSTAR SCHEMASkills usedDATA MODELLING.Performance Analyser.Vertipaq Analyser.Databases usedMongo DBWeb Cloud Servers usedAZURE What are the technical Challenges Faced during Project ExecutionTime for loading pages is increased due to raw data.Cold start of Report taking more time than usualHow the Technical Challenges were SolvedFrom Snowflake to Star Schema  achieved performance of Report By using Performance Analyser debugging resolved many glitches and where it is happening.Extraction, Transformation makes data less complex and removing unwanted data from a website perspective makes data shrink and achieved 75% of Data Reduction.Business ImpactLess coding with Power BI speeds the development process and achieves Best UX with less time.Project SnapshotsProject website urlhttps://digital.bctriangle.comProject Video"
Netclan20241111,https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/,"NFT Data Automation (looksrare), and ETL tool$Client BackgroundClient: A leading tech firm in the USAIndustry Type: IT ServicesServices: Blockchain, NFTOrganization Size: 10+Project ObjectiveTo scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on.Project DescriptionMatthew Brown – extract all events, all time from this https://looksrare.org/explore/activity . We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available.Our SolutionWe provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created.Project DeliverablesWebcrawler and databaseTools usedPython  Selenium  GBQLanguage/techniques usedPython  Selenium web scraper  Pandas Google big query Parallel processing.Databases usedSQL Google BigQueryWeb Cloud Servers usedGoogle BigQueryWhat are the technical Challenges Faced during Project ExecutionThe only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper.How the Technical Challenges were SolvedIdentifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.Business ImpactSupplied upto 50 million rows data regarding NFTs.Provided a python solution with optimal functions and code to be used and automate them to save the data into a database on a daily basis.Caused a huge influx of data which can be used to make many insightful decisions regarding the nft market.Project SnapshotsProject website urlhttps://looksrare.org/explore/activity"
Netclan20241112,https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files-and-solve-oom-errors/,"Optimize the data scraper program to easily accommodate large files and solve OOM errors$Client BackgroundClient: A leading tech firm in IndiaIndustry Type: IT ServicesServices: SAAS services, Marketing services, Business consultantOrganization Size: 100+Project DescriptionBuilding a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government websites, multilateral banks, state and local government agencies, data aggregating websites, etc. Our SolutionWe had tried multiple solutions to prevent the program from running out of memory. We used python pandas techniques to control the use of memory which worked for some files and did not work for others. Provided more solutions using vaex ,dask module and datatables.Project DeliverablesDesired changes to the code and committing them to github.Tools usedVscodePythonGithubSlackLanguage/techniques usedChunking dask Dataframevaex  datatable python.Skills usedCloud PythonTime complexityWhat are the technical Challenges Faced during Project ExecutionSystem specs requirement was the main issue during this project because the RAM available was too less and got used up quickly.How the Technical Challenges were SolvedTeam viewer to use remote desktop which had higher specs would be sufficient enough to solve the problem.Business ImpactProvided various techniques to solve memory issues.Suggested parallel programming to decrease the execution time by 12% making getting the tender data at a much faster rate.Project SnapshotsProject website urlhttps://github.com/Taiyo-ai/opentenders-euhttps://opentender.eu"
Netclan20241113,https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-python-etl-solution/,"Making a robust way to sync data from airtables to mongoDB using python – ETL Solution$Client BackgroundClient: A leading tech firm in the USAIndustry Type: IT ServicesServices: SAAS services, Marketing services, Business consultantOrganization Size: 100+Project DescriptionEquilo is a social impact start-up focused on gender equality and social inclusion. We need to link data in Airtable (1 million+ records spread across 20+ bases) to MongoDB (v3.x.x).Most of the data is backend data for our app, in which case the flow is only AT to MDB.Need to create a code that can calculate a scores by pulling from indicators in many different bases and putting result in new database.Our SolutionUsed Python and MongoDB module along with Airtable API to fetch all the data from airtables and push them to the database. Stayed in touch with the client through slack and asana completing daily tasks and applying a cronjob for the program to run on a scheduled time.Project DeliverablesPython code for sync into their staging server and then to production.Tools usedVScode MongoDBAirtable APISlackAsanaGithubLanguage/techniques usedPython  MongoDbSQLSkills usedData extractionData handlingData storageComputational data queriesDatabases usedAirtablesMongoDBWeb Cloud Servers usedAirtableWhat are the technical Challenges Faced during Project ExecutionMain challenge faced was regarding the new concept of Airtables and syncing up the data into mongodb in a very complex schema as proposed by the client. Dissimilar columns in mongoDB and Airtables for 100s of tables took lot of time.Also insufficient information provided by client while coding and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem.Not proper code management which could help next coders like me to complete the remaining stuff quickly.How the Technical Challenges were SolvedThese issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. For eg: whereabouts of the previous codes and people who run that code.Business ImpactHelped them immensely making their backend to frontend integration seamless.Sped up their product development by 20% to calculate various different scores and visualize them on the frontend.Project SnapshotsProject website urlhttps://www.equilo.io/"
Netclan20241114,https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/,"Incident Duration Prediction – Infrastructure and Real Estate$Client BackgroundClient: A leading research institution in the middle eastIndustry Type:  ResearchServices: R&DOrganization Size: 1000+Project ObjectiveTo complete a Research Paper draft by training various Machine Learning models which can predict the Incident Duration based on various parameters given in the dataset and summarising the results.Project DescriptionGiven a set of researches, need to analyse and compare various machine learning and deep learning models to predict the Incident Duration for the given dataset. The dataset contained Short durations as well as Long durations. Build models for each set of durations, compare and get the best out of all.Our SolutionHere, we had to predict the traffic incident duration with some machine learning tools and techniques i.e. XGBoost, SVR and Deep Learning algorithm using tensor flow. First two models were run on Python Interpreter whereas Deep learning model was run on R studio, all the three with the same dataset and then we had compared these models based on their MAE (mean absolute error). Initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variables used in our research.Project DeliverablesPython Script for each model.Documentation for Research Work.Tools usedPython InterpreterLanguage/techniques usedLanguage Used: PythonLibraries Used: pandas, sklearn, numpy, keras, pickleModels usedXGBRegressorSVRSGDRegressorSequentialDecisionTreeRegressorSkills usedProgramming, Statistical AnalysisProject Snapshots"
Netclan20241115,https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/,"Statistical Data Analysis of Reinforced Concrete$Client BackgroundClient: A leading research institution in the middle eastIndustry Type:  ResearchServices: R&DOrganization Size: 1000+Project ObjectiveConducting statistical data analysis on the data provided for different types of reinforced concrete (using 3 different fibers – Steel, Date Palm and Polypropylene fibers) and also helping in preparing good research paper based on laboratory data.Project DescriptionThe project had two phase:Phase 1:In this phase, we had to do a comprehensive analysis on the data given and finally build statistical models for the variables present. The main motive was to understand the behaviour of concrete based on various parameters – Compressive strength, Flexural strength, water absorption capabilities of the concrete and many more. The analysis should include, but was not limited to:Comparison of Mo (control mix) with all mixes at 28 days for each parameter testComparing all parameters for all specimens (all concrete mixes) with 28 days and also 6 months heat-cool and wet-dry all other expected analysis we could see you and doPhase 2:In this phase, we had to develop a structure for the research paper based on the results and analysis. The paper included sections – Abstract, Introduction ( literature, background and objective), Experimental program ( materials and methods), Results and discussion ( analysis and interpretation) and Conclusion ( summary, insights and remarks).Our SolutionProviding a Comprehensive analysis for the concrete data – showcasing the key insights from it based on the parameters (compressive strength, etc). On the basis of results from the analysis, research paper was drafted which included all the deliverable.Project DeliverableA manuscript (drafted article) with the following:AbstractIntroduction ( literature, background and objective)Experimental program ( materials and methods) Results and discussion ( analysis and interpretation)Conclusion ( summary, insights and remarks)ReferencesTools usedTools used:Jupyter – Notbebook (Python)NumpyPandasSklearnMatplotlibSeabornMS ExcelGoogle spreadsheetsLanguage/techniques usedPythonStatistical ModellingStatistical InferenceModels usedStatistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete mixes due to mixing of different fiber content and its effect on different parameters specified above.Skills usedCoding – PythonPerforming statistical analysis – extracting inferencesBuilding statistical models – through python or through Excel and its counterparts.Databases usedNo database was used.Web Cloud Servers usedNo Cloud server was used.What are the technical Challenges Faced during Project ExecutionThe Challenges faced during project execution are:Getting statistical models from seaborn libraries, there is no direct way to get the models from the graphs created from data.Building models in excel and validating it (didn’t know how, had to learn it before applying it).How the Technical Challenges were SolvedI had to use different libraries for building the models, later on turned to MS excel and spreadsheet because they were building models and were also able to showcase it on the data itself. For this, I learned how to build models on the aforementioned software through YouTube and blogs.Project Snapshots Project Video"
Netclan20241116,https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboard-insights/,"Database Normalization & Segmentation with Google Data Studio Dashboard Insights$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  Market ResearchServices: Marketing, ConsultancyOrganization Size: 60+Project ObjectiveTo combine the different datasets.To make dashboards for each and every dataset individually. Project DescriptionPhase – 1: In this project first of all we have to combine different datasets individually to make single file for each source.Phase – 2: Make Good looking reports for each file individually.Our SolutionWe used pandas dataframe to combine different files to make single file for each source. We used Google Data Studio to make good looking and better reports with good UI.Project DeliverablesWe have provided a Google Data Studio report file as deliverable for the project.Tools usedPython, Google Data Studio, Google ChromeLanguage/techniques usedPython Programming and SQL queries editor.Models usedSDLC model used in this project. We have used the SDLC model as analysis, design, implementation, testing and maintenance.Skills usedData cleaning, Data Pre-processing, Data Visualisation are used in this project.Databases usedWe have used the traditional file systems as database storage.What are the technical Challenges Faced during Project ExecutionCombining Data sets into single file.Making good looking UI dashboards.How the Technical Challenges were SolvedI used pandas dataframe to combine different datasets and made a single file of every individual source. I used Google Data Studio to make dashboard for the project.Project Snapshots Project Video"
Netclan20241117,https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-business-insights/,"Power BI dashboard to drive insights from complex data to generate business insights$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  Market ResearchServices: Marketing, ConsultancyOrganization Size: 100+Project DescriptionPhase – 1: In this project first of all we have made heatmap between two columns named Author and Data Source. Then after two combining two tables named NY_data and nodeid_views made the report of all of the data.Phase – 2: Success of story was given by if pageviews is more than 35000, if pageviews lies between 3500-35000 the story was labelled as needs improvement and if it was below 3500 the story was labelled as failure. Phase – 3: The powerbi report was made to find different insights in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. The data is updated every week and the report is generated automatically.Our SolutionWe provided them Phase 1 in the powerbi sql editor by combining two tables using sql queries. For phase 2 we just used the power bi program tool and written a script in Python to calculate the success of story. For Phase 3 we used the internal features of Power BI to find insights of the data.Project DeliverablesWe have provided a PowerBI report file as deliverable for the project.Tools usedPython, PowerBI, Google ChromeLanguage/techniques usedPython Programming and SQL queries editor.Models usedWaterfall model used in this project.Skills usedData cleaning, Data Pre-processing, Data Visualisation are used in this project.Databases usedWe have used the traditional file systems as database storage.What are the technical Challenges Faced during Project ExecutionDrawing heatmap in the PowerBI.Combining two tables on the basis of the pageviews.Converting the time series to data to 5 minute format.How the Technical Challenges were SolvedWe installed a new add on in the PowerBI to draw heatmap for the project and used the SQL editor to combine the tables on the basis of page views. We used python programming to convert the time series data to 5 minute time gap format.Project Snapshots Project Video"
Netclan20241118,https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines/,"Real-time dashboard to monitor infrastructure activity and Machines$Client BackgroundClient: A leading tech firm in EuropeIndustry Type:  ITServices: Software ServicesOrganization Size: 30+Project ObjectiveFor the current project, we hope to develop a real-time dashboard (* it updates every several minutes). Currently, we have multiple Ubuntu machines that are sending messages every minute to Apache Pulsar.Project DescriptionDeveloping a realtime updating dashboard to display the metadata of various machines on a server from pandio queue.The dahboard must display the count of “inactive” , “active” and “down” servers with a table displaying the details of all the machines in different color scheme for each type of server/machine.Our SolutionWe used Django framework to develop the dashboard as it didn’t require the ec2 instance to be active on machine which was the problem with using streamlit.For communication between webpage and fetched data we used django channel .We used django background task module to make the fetching run forever in background.Project DeliverablesReal time updating Dashboard with separate color scheme for different types of machines.Storing the historical data in sqlite3 db.Tools usedDjangoWeb ChannelsD3 jsReddis serverSkills usedPythonDjango FrameworkDjango web channelsHTML/CSS + JSDatabases usedDjango sqlite3 database.Web Cloud Servers usedAWSWhat are the technical Challenges Faced during Project ExecutionMaking the dashboard run forever using streamlitData updation in realtime when using django channelsHow the Technical Challenges were SolvedSwitched the entire dashboard to django frameworkWe redirected data to channels on local reddis server.Project SnapshotsProject website urlDevelopment hosted URL"
Netclan20241119,https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-demand/,"Electric Vehicles (EV) Load Management System to Forecast Energy Demand$Client BackgroundClient: A leading energy consulting firm in the USAIndustry Type:  EnergyServices: Energy solutions, ConsultancyOrganization Size: 100+Project ObjectiveCreate a Machine learning solution to manage electricity for electric vehicles.Main Tasks:Percentage probability of  user plugin his vehicle today by user’s plugin date historyReduce the probability of plugin time according to user’s plugin time historyProject DescriptionWe need to calculate the date and time probability that the user will plugin his vehicle today based on his plugin date and plugin time history. We also need to decrease time probability based on the user’s past time range.Our SolutionWe converted the user’s plugin data into binary values like 0 if the user hasn’t plugged-in his vehicle on that day and 1 if he plugged-in. We identified the driven distance based on the amount of charge used between two plug-in times. Then we trained the Ridge Regression ML model for identifying each day driven kilometer. From these kilometres we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle.For time probability we have used Probability Distribution Function (PDF) and Cumulative Distribution Function  (CDF). These functions will decrease probability according to the user’s time range.Project Deliverables2 python scripts to:Train regression model every day.Use model weights to generate probability values.Tools usedGoogle Colab, VS Code, Google Drive, and MS Excel.Language/techniques usedPython programming language, Data Analytics with numpy and pandas, Data Visualization with matplotlib, Statistics and Mathematics, Machine learning with SKlearn.Models usedRidge Regression ModelSkills usedData Analytics, Data Visualization, Machine learning, Python, StatisticsDatabases usedlocal data from MS Excel SheetWhat are the technical Challenges Faced during Project ExecutionThere are a lot of challenges faced during project executionAt the start, we have only imaginary data so need to convert in a good format to apply machine learning models.Find the best machine learning model for the data.Decrease the time probability according to user’s time range How the Technical Challenges were SolvedWe have converted the data into weekday’s binary values like marked 0 if not plugged-in vehicle on that day and 1 if plugged and calculated driven distance by amount of charge used between two plugin dates.Tried different regression based machine learning models like Random Forest Regressor, XGBoost Regressor, Ridge Regression and checked accuracies of all models and choosed best one.For decreasing time probability we used Probability Distribution Function (PDF) and Cumulative Distribution Function (CDF). These functions decrease probability according to the user’s time range.Project Snapshots"
Netclan20241120,https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/,"Power BI Data-Driven Map Dashboard$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  Market ResearchServices: Marketing, ConsultancyOrganization Size: 60+Project ObjectiveChange bubble colors dynamically.Make table and charts linked. If a user clicks on tables values, then the bubble chart on the map should be highlighted that relates to the table. Project Description“I have a map visual. I would like to dynamically change the colours of some of the bubbles.”The report page has several filters and KPI Dashboard, whose metrics change dynamically when the user clicks a certain element. Similarly the map should also change dynamically relative to the filter.Our SolutionAdded the website data from Details table to the map visualization, it makes the bubbles get coloured dynamically according to the requirement for websites data.Project DeliverablesThe Power BI ( .pbix ) file updated with solutionTools usedPower BISkills usedPower BIData VisualizationData AnalysisDatabases usedThe database that came in with the Power BI file received from clientWhat are the technical Challenges Faced during Project ExecutionThe map was not linkedMap Bubbles were not dynamicHow the Technical Challenges were SolvedRefactoring the data model and using appropriate keys to link the data togetherThat made Map to change according to Slicers/FiltersTo Change the colour, Bookmark buttons were used in the dashboard to bring up the dynamic colour changing with slicing (works after being published)Project SnapshotsProject Video"
Netclan20241121,https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/,"Google Local Service Ads (LSA) Leads Dashboard$Client BackgroundClient: A leading law firm in USAIndustry Type:  LawServices: Law practiceOrganization Size: 40+Project Objective:For a better understanding, provide visualisations of the data on the LSA Dashboard.Learn how to enhance Rank and push the Ad to potential consumers by gaining data insights.Project DescriptionLocal Service Ads is a newer program by Google that allows advertisers to achieve a “Google Guaranteed” status in search engines when a visitor makes a search. Advertisers who participate in Google Local Service Ads will receive a larger ad space with their competitor’s local services ads and they will be able to feature their local businesses throughout organic search queries. There are various aspects that firms must concentrate on in order to win the Google services ad and so raise their ranking. These enhancements may be implemented if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future.This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.Our SolutionThe solution for the project includes data insights through visualisations which will help businesses to better analyse the available data. This solution will help the businesses in improvising the factors to increase their potential customers and raise their respective ranks. It is divided into two parts: databases and data dashboard. The databases will store the important data retrieved from the LSA dashboard and use them to calculate some important metrics. The data dashboard will represent those metrics in form of graphs and data in form of tables. Project DeliverablesThe project deliverables can be divided into two parts: Data in databases: The data is divided into three parts: Historical Account Data, Historical Phone Lead and Historical Message Lead. Using these three data, we calculate and store other important metrics like Cost per Acquisition, Conversion Rate, number of booked leads, number disputed leads, pending leads and approved leads. Google data studio dashboard: The dashboard will show the count of important metrics like total number of records, total interactions and different types of leads. It will represent different types of graphs portraying different kinds of information and tables containing major data like Lead data combined and Net monthly spent on Ads. Tools usedFor extracting the data from the LSA Dashboard, we have made our own tool by python scripts. The automation tool will store data in the excel sheets and google bigquery for respective businesses on a day to day basis. PyCharm for compiling and running the code. JsonViewer for processing Language/techniques usedWe have used the LSA API to extract data from the LSA Dashboard. Google Sheets API to store data in excel sheets. Bigquery API for storing data in google bigquery. The scripts for the automation tool were written in the Python programming language. Models usedSoftware Model: RAD(Rapid Application Development model) ModelIn the RAD paradigm, less emphasis is placed on planning and more emphasis is placed on development activities. It aims to create software in a short period of time.Advantages of RAD Model: Changing needs can be addressed.Progress may be quantified.Increases component reusability.Encourages responses from consumers.Integration from the start solves a lot of integration concerns.Skills usedAPI Data AbstractionData VisualisationAutomation of toolsException Handling from PythonData PreprocessingData WranglingDatabases usedTwo types of databases: Google excel sheets and google bigquery. Web Cloud Servers usedGoogle BigQuery Cloud Database with up to 1 TB of free storage is being used.What are the technical Challenges Faced during Project ExecutionSome minor technical challenges were faced for clients with minimum data. For those, plotting graphs became difficult. How the Technical Challenges were SolvedWe tried to process the data, remove the blank data spaces and plotted the graph with available data. Business ImpactIt’s undeniable that Google’s Local Services ads (LSA) have changed the way home service businesses advertise online.The pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized businesses a better shot at competing with national brands and massive budget operations.To win with the Local Services the businesses need to take care of some factors where data comes to help. Dialling in your service area, Profile and Budget: The data from the message and phone leads help to know whether they are potential customers. If they are potential customers, their location and profile can help you in charging them or not charging the leads. Mark your JOBS as Booked: The dashboard will display the number of archived leads and booked leads. This count can help you analyse your performance and how you can work to increase your potential customers. Deal with disputes: The dashboard will also represent the disputed disputes and approved disputes which will help you to deal with the disputes. Net Monthly Ad Spend: This is an important metric which helps the firms to make better decisions for their expenditure. They can have an efficient control over their expenditure once they have proper data available. Other metrics related to finances include Cost per lead, Cost per Acquisition and Conversion rate. Project Snapshots Fig.1: Data Dashboard for individual businesses-1Fig.2: Data Dashboard for individual businesses-2Fig.3: Consolidated DashboardFig.4: Historical Account DataFig.5: CPA and CPL datasheetFig.6: Lead Dispute Status"
Netclan20241122,https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/,"AWS Lex Voice and Chatbot$Client BackgroundClient: A leading tech firm in USAIndustry Type:  ITServices: eCommerceOrganization Size: 40+Project ObjectiveCreate a Voice and chatbot using AWS lex which can book flights, hotels, cars and book some fun activities in a city.Project DescriptionWe need to create a voice and chatbot using AWS lex and lambda function. The bot should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. We also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities. Our SolutionWe have created aws lex intents and lambda functions for all bookings. Intents manage front ends like utterances (user can ask to the bot) and slots (bot replies with relevant questions). Lambda functions manage backend parts like which intent should be triggered if the user says “ book a flight” or “book a hotel” or “book a car”. For search results we have used some external APIs like Amadeus for flight, sabre for hotels and blablacar for car booking.  We have modified search results by using Data Analytics (for getting the cheapest and good star flight and hotel), Machine learning (for getting user’s preferences by analyzing user’s history) and NLP (Differentiate search results by text analysis) techniques so users can get the best search results. Project DeliverablesAn aws lex voice and chatbot which can book flight, hotel, car and fun activities. This can be integrated with IOS applications. Tools usedAWS Lex, AWS Lambda, AWS Cognito, AWS EC2, Google colab, VS code, FAST API, Uvicorn.Language/techniques usedpython, machine learning, data analytics, NLP.Models usedTfIdf-Vectorizer and cosine similaritySkills usedData Analytics, Machine learning, NLP, Python, AWS, REST APIs.Databases usedMySQLWeb Cloud Servers usedAWSWhat are the technical Challenges Faced during Project ExecutionThe first challenge we have faced is the integration of AWS lex and lambda functions. Amadeus and Sabre APIs data was not in a good format so we have to clean some data and organize it in a usable format.We need to make some APIs so we can pass flight or hotel parameters and the APIs will give flight or hotel related data. Create a book button in the bot for booking flights, hotel,s and car.How the Technical Challenges were SolvedSo the integration of AWS lex and lambda function was very tough for us. Because lex uses some intentes to show responses from the lambda function. So we have created different lex intents to pass messages to lex bot from lambda function. And put some good coding to the lambda function so different messages can be handled by different intents.For flight, hotel and car search results we were using some external apis like amadeus, sabre and blablacars apis. These APIs have a lot of data and are not in a format we need.  So first we cleaned data and then sorted data according to cheaper and best ratings results. We have used the best two results among all the results. We cannot use all the machine learning and data analytics part in aws lambda function. So what we did was we created some REST APIs which can handle all the data analytics and machine learning part and we hosted these APIs on AWS EC2 instance. We used these APIs in our lambda functions.So Creating a button in a chat bot or voice bot is always so different from providing text messages. For creating a button we used a response card structure in lambda function which can handle button and button related responses.Project SnapshotsProject Video"
Netclan20241123,https://insights.blackcoffer.com/metabridges-api-decentraland-integration/,"MetaBridges API Decentraland Integration – AR, VR$Client BackgroundClient: A leading tech firm in the USAIndustry Type:  ITServices: Consulting, Software, Blockchain, MetaverseOrganization Size: 20+Project ObjectiveTo integrate with Metaverse environments with the help of EC2, S3 bucket and the Decentraland SDK.Project DescriptionMove 3D model files from EC2 instance to S3 bucked using aws-sdk.Our SolutionConfigure  s3 bucket in aws account, create an user for s3 bucket api keys, and api secret. Put the api key, aapi secret, bucket name and bucket region in environment variable to use them in app. Install aws-sdk to implement s3 bucket.Create a function to send file from nodejs server to s3 bucket.Project DeliverablesAws ec2 instance credentials, s3 bucket credentials. Code used in the projectTools usedvs code editor, git bash terminal, google chrome web browser.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality Language/techniques usedJavascript language is used.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality  Models useddcl SDK (Decentraland sdk for nodejs), aws-sdk, awscli.Skills usedNode js project setup, Dcl sdk setup, Aws ec2 instance setup with aws cli,S3 bucket connection with aws-sdk. cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented RealityDatabases usedNo database is usedWeb Cloud Servers usedAWS cloud server is usedWhat are the technical Challenges Faced during Project ExecutionMaking the application port in ec2 instance available globaly.How the Technical Challenges were SolvedSearch few blogs and videos for the solution. And make it done by doing some change in Security group in ec2 instance.Business Impact As Decentraland is a platform based of NFT so main part of business is related to NFT and cryptocurrency.  Project SnapshotsProject Video"
Netclan20241124,https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/,"Microsoft Azure chatbot with LUIS (Language Understanding)$Client BackgroundClient: A leading retail firm in the USAIndustry Type:  RetailServices: e-commerce, retail businessOrganization Size: 100+Project ObjectiveTo create an advanced chatbot using Microsoft Azure cognitive service to take orders from customer on behalf of a pizza restaurant and give order summary as end result to the user. Project DescriptionThe project uses MS Azure LUIS service for language understanding to receive order details from a customer and provide an order summary. Also display various menu options to the customer in a dynamic method.Our SolutionOur solution is to create a chatbot on MS Azure platform using their LUIS service in bot-framework composer environment. Use dynamic hero cards to display menu so that user can get a better experience.Project DeliverablesChatbotTools usedBot Framework composerBot emulatorMS Azure LUIS servicesLanguage/techniques usedBot framework composer Natural language processingModels usedMS Azure LUISMS Azure QnAMS Azure speed SDKSkills usedDeep learningWeb developmentCloud techWeb Cloud Servers usedMicrosoft Azure web platformWhat are the technical Challenges Faced during Project ExecutionMonthly quota for LUIS authoring service was reachedTracking multiple items ordered by userAccessing relevant images for each menu itemHow the Technical Challenges were SolvedSwitching to a more suitable pricing tier which would have to eventually switch to when move onto production phaseCreating custom functions and intents for different trackersUsing open license images from internetProject Snapshots Project website url Demo"
Netclan20241125,https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investments/,"Impact of news, media, and press on innovation, startups, and investments$Client BackgroundClient: A leading research institution in the wordIndustry Type:  Research, R&DServices: R&DOrganization Size: 1000+Project ObjectiveMake data ready for predictive modelling. Making Google Data Studio dashboard.Project DescriptionPhase – 1: In this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data.Phase – 2: Finding co-relation between the pitchbook data and the other output files.Phase – 3: Making dashboard in Google Data Studio for the project.Our SolutionWe used pandas and numpy to clean the data and make useful for it to be used in predictive modelling. We have found the co-relation between the tempa msa pitchbook data and the output files like textual file, ai_ml_tm file etc. We have made the dashboard using the Google Data Studio.Project DeliverablesWe have provided a excel file consisting of clean data and the Google Data Studio report.Tools usedPython, Google Data Studio, Google ChromeLanguage/techniques usedPython Programming Models usedWaterfall model used in this project.Skills usedData cleaning, Data Pre-processing, Data Visualisation are used in this project.Databases usedWe have used the traditional file systems as database storage.What are the technical Challenges Faced during Project ExecutionCleaning the data was the major challenge faced while executing the project. The data has a lot of noise. It was difficult to find which data was useful and which data is not useful in this project. Secondly the co relation between the output files and pitchbook data. There was nothing common between both the datasets. So was difficult to find co-relation between them. How the Technical Challenges were SolvedWe used pandas dataframe to clean the data and make it ready for predictive modelling and used the Google Data studio to find insights between the different datasets.Project Snapshots Project Video"
Netclan20241126,https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/,"AWS QuickSight Reporting Dashboard$Client BackgroundOverviewAs a Singapore and Australia based startup, Drive lah (known as Drive mate in Australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. All trips on Drive lah are comprehensively insured through our insurance partners so car owners don’t have to worry about their insurance. The idea is simple: car ownership is expensive in Singapore (per month yet only use the car 5% of the time – cars are mostly parked. With Drive lah you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. Renters can rent those cars when they are not used by their owners at good value.In a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, Drive lah is envisioning to take the lead in distance travel and simplifying car accessWebsitehttp://www.drivelah.sgCompany size11-50 employeesFounded2019Project ObjectiveAutomating the process to get updated Metrics every week.Evaluate the following Performance Metrics which will be used on AWS Quick Sight for Performance Evaluations:Total CancellationsCancellations by HostWeekly Guest Success Rate.Monthly Active User’s {MAUs}Monthly Active Listings {MALs}Total Approved & Live ListingsApproved & Live InstantBookingsApproved & Live Dl GoDelivery Booking ListingsWeekly Active Listings {WALs}Successful HDMUnsuccessful HDMBooking Acceptance RateTotal Requested TripsNew Listings Made LivePercentage of Live Listings Made ActiveMap Location Metrics Table with Postal Districts.DL Live Cars & DL L3M Active Cars Host Experience Team Weekly DashboardNew Weekly Listings DashboardTwo Transaction MetricsBuild Code for extracting Daily Agent Activity Report on Daily Basis.Our SolutionFor Performance Metrics, we suggested that we will Code for each Metric & will store them in a Table on AWS RDS which will be directly synced to the AWS Quick Sight for Performance Evaluations.For Automating the process to get updated Tables of Metrics every week, we suggested to use a Virtual Machine on which we can upload all code files & can run a Cron Job for each file to automatically get updated on specified time every week.Tools usedJupyter NotebookPyCharmMySQL WorkbenchAWS QuicksightLanguage usedPythonDatabase UsedAmazon Relational Database Service (RDS)What are the technical Challenges Faced during Project Execution?Tried with AWS Lambda Function to update tables on AWS RDS but Lambda Function was unable to run complete code.How the Technical Challenges Were Solved?Suggested to use a Virtual Machine on which we can upload our Code Files & can run Cron Job for automatically updating tables on regularly basis.Project SnapshotsMetrics from Listings Table:Host Experience Metric:New Live Listings of Last 7 Days:Line Chart of Total Cancellations & Cancellations by Host:Line Chart of Monthly Active Users (MAU’s):Area Chart of Percentage of Live Listings Made Active:Line Chart of Number of DL GO Listings & Number of Instant Booking Listings:Line Chart of Monthly Active Listings (MAL’s):Line Chart of New Listings Made Live:Vertical Bar Chart of Total Approved & Live Listings:Project Video Link"
Netclan20241127,https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/,"Google Data Studio Dashboard for Marketing, ads and Traction data$Client BackgroundOverviewBankiom – the super banking app for MENA on a mission to make managing your finances easier.☞ Open an account on your phone and get a virtual card in 3 minutes or less☞ Manage all your bank accounts from one app and one control panel☞ Save money and grow your wealthWebsitehttp://www.bankiom.comCompany size2-10 employeesFounded2019SpecialtiesBanking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTechProject ObjectiveBuild a dashboard unifying all the platforms that we use: Google Ads, FB ads, Appsflyer, MixpanelProject DescriptionWe want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel):– App settings in Appsflyer– SDK Installation, test it (+ instruction for devs)– Ad sources setup in ad accounts (Facebook, Google Ads, etc)– Ad sources setup in Appsflyer– In-app conversions mapping– Conversion set up in ads sources– One link, smart script, and deep link setup– SKAD Network for IOS appOur SolutionBuilt dashboard for each data source like Google Ads, Facebook Ads for tracking installs, channel spend, cost per install for both Android and IOS.Then, we made a dashboard for tracking the retention rates of customers and other events that they execute on the app like transfer money, user registration, connect banks. The data for these events was fetched from MixPanel.These dashboards were made using Google Data Studio.Project DeliverablesWe need to deliver dashboards for tracking the ads data from Google and Facebook and to track the events which the users perform on their app and for this data was collected from MixPanel.Tools usedFollowing Tools were used for successful execution of the projectGoogle Data StudioAdveronix Mixpanel ApiBigQueryGCPLanguage/techniques usedCode was written to create the pipeline to fetch MixPanel data through mixpanel Api and store it in bigquery. So, the code was written in Python.Skills usedFollowing Skills were used to complete the projectData PreparationData VisualizationPythonAPIBigQueryGoogle Cloud PlatformDatabases usedFor storing the data of the project Google Sheets and Google BigQuery were used.Web Cloud Servers usedWeb Cloud server used in this project was Google Cloud Platform.What are the technical Challenges Faced during Project Execution?Technical Challenges faced during the execution of the project was to understand how the api of the mixpanel works and how to connect it to Google BiqQuery. Another technical challenge that we faced was to find a free resource to connect the facebook ads data to data studio.How the Technical Challenges were SolvedTo solve the technical challenges we went through the documentation of the mixpanel api to get a understanding of how the things work. Based on that we built the pipeline to connect the mixpanel data to big query. The other technical challenge of finding a free resource to connect the facebook ads to datastudio for free was solved by researching for the various connectors available and we found an add on named ‘Adveronix’ which could connect the facebook ads data to google sheets which can eaily be connected to data studio.Project SnapshotsProject website urlhttps://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9fProject Video"
Netclan20241128,https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/,"Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse$Client BackgroundClient: A leading eCommerce firm in the USA, Columbia, India, and Latin AmericaGangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …Industry Type:  eCommerceServices: e-commerce, retail businessOrganization Size: 100+Project TitleGangala.in: E-commerce site gathering data of different products from various sources and providing it on a single platformProject ObjectiveProvide up-to-date data of any given product on the website along with 3-5 prices of that product from different sites for the customer to compare and buy. Project DescriptionA platform in which users can get price data of any product from multiple sites. The client provided us with raw data. We were tasked with building a pipeline for the data, build API’s to get product data such as price and update them and make sure that all the data is available for the front end team to access.  Our SolutionWe built them a pipeline to process and clean the raw data provided. We built API’s to fetch the updated data of the products. Neo4j was used as the intermediary data and mongoDB was used as our primary database. We also process the images of each product and remove any unwanted texts from it and add the client’s watermark. Project DeliverablesA fully-updated database with up to date data on all the products and each product having atleast 3-5 prices from different sites. Tools usedNumpy package Json package csv package concurrent futures package (for multithreading)Py2neo package (to connect to neo4j using python)Language/techniques usedPython Cypher Query Language (CQL)APOC QueriesDatabases usedNeo4jMongoDBDataiku OdooDSSWeb Cloud Servers usedLinode cloud servers What are the technical Challenges Faced during Project ExecutionWe were asked to process 3million products per day and this was a challenge as the VM’s we used were not able to handle the load. How the Technical Challenges were SolvedWe were able to overcome the challenge by using Asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as wellProject website urlhttps://gangala.in/"
Netclan20241129,https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-business/,"Big Data solution to an online multivendor marketplace eCommerce business$Client BackgroundClient: A leading eCommerce firm in the USA, Columbia, India, and Latin AmericaGangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …Industry Type:  eCommerceServices: e-commerce, retail businessOrganization Size: 100+Project ObjectiveTo give User experience of easy and convenient Shopping by searching all the products like any medicines , Clothes , Gadgets etc in a single Website without going through all the E-Commerce Sites and make shopping easy and get the most affordable and best product.Project DescriptionIt’s an E-Commerce Sites that’s helps customer to compare different  products that were available on different E-Commerce Sites like Flipkart , Amazon , Netmeds etc.It’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.The gives the user a great and friendly Experience in Buying any Products.It’s Also have some Unique Similar Products Recommendation Based on user search and also have a ChatBot That’s solves User Query .It’s uses Big data and Rest API that’s help the projects for regular updates and regular fetching of the new products.Our SolutionIn BlackCoffer We create the flow of the Big Data and all Backend Solution That is requires for this futuristic E-Commerce Sites.We Create Pipelines for the data of all the products and their price and url fetch from different E-Commerce Sites using Custom made APIs And perform many data cleaning, data transformation and data validation techniques to make sure the standard of data to be used by Our Sites .We also get Additional Feature from the scraped data By using Different APIs . We also create automation and custom python scripts that helps us to achieve some outstanding data related tasks.Project DeliverablesPython script for performing ETL and Cypher Query for big data Handling.Tools usedJupyter NotebookDSSVS CodeLanguage/techniques usedPythonNo SQlCypherETLModels usedSimilar Price APIWhatsapp Chat APISimilarity Server to get similar productsSkills usedData EngineeringData AnalysisPython ProgrammingRest APIsDatabases usedDSSNEO4JMongoDBWeb Cloud Servers usedLinodeAWS What are the technical Challenges Faced during Project ExecutionData Cleaning : -The Scraped that will be used by our sites is coming from different sources and also it’s not’s that clean to be used by sites .This is the very first problem every data scientist faced during the whole process.Data Merging :-  The data is scraped from around 140 sources that’s why it’s very difficult to maintain the attributes that should be used by all the sources and we can get a clean and sufficient amount of data to process.Data Validation :-  There are many records that have null values and missing values that disturb the users experience a lot .That should be handle with very care.How the Technical Challenges were SolvedData Cleaning : – For Data cleaning we used Python Data Frame and Pandas and data structure and handles the data cleaning and optimize our data for get correct data format and useful data.Data Merging : –  For data Merging and data transformation we used pandas that help to get the appropriate data that can used further And also make Python pipelines for future updation.Data Validation :- For that data validation we use some fundamental property and feature selection that’s help us to make the appropriate data format and records to be used In our sites.Project SnapshotsProject website urlhttps://gangala.in/"
Netclan20241130,https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-atera-api/,"Creating a custom report and dashboard using the data got from Atera API$Client BackgroundClient: A leading Marketing firm in USAIndustry Type:  MarketingServices: Marketing, consulting, ads, business solutionsOrganization Size: 20+Project DescriptionAtera.com is used as our RMM, we have an agent on every machine. Which tracks the if a machine goes down, initial response time etc.., The website doesn’t provide any standard reports, So we needed to create a custom report.Our SolutionImporting the data from Atera API into JupyterUsing Web Scraping download the JSON dataConvert the JSON data to Data Frame and download it into PC.Clean the data with only required columnsUpload the data into google sheets.Connect google sheets and google data studioCreate the dashboard with the dataTools usedPython (Pandas, requests)Google SheetsGoogle Data StudioSkills usedAnalyticsProgramming LanguageDatabases usedContacts.csvCustomers.csvTickets.csvAlerts.csvWhat are the technical Challenges Faced during Project Execution?I found it difficult on downloading the data.How the Technical Challenges were SolvedOnce I figured I have been using the wrong Authorization key to login I was able to solve the issue, and convert the curl command into pythonProject SnapshotsProject website urlhttps://datastudio.google.com/reporting/5e61aecb-a420-41cc-afba-d0ca37f69132Project Video"
Netclan20241131,https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/,"Azure Data Lake and Power BI Dashboard$Client BackgroundOverviewStone is a video bibliographic tool for journalists and other researchers.It allows users to capture, annotate and share their journeys through digital and physical space, producing verifiable logs and generating monetizeable video highlight reels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story.Our mission is to address distrust and disinformation with transparency and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work.Research is valuable. Make it Visible.Write In Stone.Websitehttp://www.writeinstone.comCompany size2-10 employeesHeadquartersBlackheath, New South WalesFounded2017SpecialtiesResearch Transparency, Trust, Video Content, Journalism, Proof Of Work, and Bibliographic StandardsProject ObjectiveWorking on Microsoft Azure Analytics ServicesVerifying that indicators are being gathered in an intended manner, in line with GDPR provisionsBuilding and analyzing dashboards and, specifically, conversion funnelsProject DescriptionTo determine whether the already implemented indicators in are in intended fashion (separated by where these indicators are placed in the currently constituted funnel)Implement New IndicatorsResearch LoggedAverage Number of Highlights per ProjectTotal Hours of Content ProducedTotal Hours of Content WatchedDaily unique visitors engaging with Stone, including the landing page, public research page(s), and the research portalAssess the dashboard set up in Azure, refine the existing dashboard, and determine whether an alternative is preferable. Review, refine, and optimize the WIS conversion funnel(s)Our SolutionBuilt a Power BI dashboard as per the requirement. Also built a separate dashboard for the metric data from Azure.Project DeliverablesPower BI dashboard which contains indicators funnels, new indicators(Research logged, Average number of Highlights per projects, Total hours of content watched etc), visualizations  extracted from metric data.Tools usedPower BIAzureLanguage/techniques usedPower BIDAXKusto QueryAzureSkills usedData collectionData AnalysisData cleaningFeature engineeringQueryingVisualizationDatabases usedAzure databaseWeb Cloud Servers usedAzureWhat are the technical Challenges Faced during Project ExecutionDifficulty in data collection."
Netclan20241132,https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/,"Google Data Studio Pipeline with GCP/MySQL$Client BackgroundClient: A leading IT firm in EuropeIndustry Type:  ITServices: e-commerce, retail business, marketing, ConsultingOrganization Size: 100+Project ObjectiveCreating a Data Pipeline to sync live data from FieldPulse to Google Data Studio using GCP/MySQL.Project DescriptionThere is a Virtual Machine up and running and MySQL in Google Cloud(GCP). Get the following live data from FieldPulse to Google Data Studio(GDS) for making Business Dashboard in GDS –Job DataTag DataTeam Member DataTeam DataSuch that if data changes in FieldPulse , GDS Dashboard should update automatically.Our SolutionFor fetching data from FieldPulse –Data Pipeline (FieldPulse to GCP MySQL) :  We have created a Data Pipeline that uses web scraping to fetch data from FieldPulse. It makes a GET request to the FieldPulse API , and the API returns raw data. Convert this into json format then in Dataframe. Now , create new tables in GCP MySQL and insert/update the data accordingly.Insertion & Updation of Data : Insertion : If any data fetched from Fieldpulse is not present in their respective database table , then  insert that data in the table.Updation : If any data fetched from Fieldpulse is present in their respective database table , then update that data in the table.Deploy the above Data Pipeline in GCP VM instance :  Deploy the above data pipeline in GCP VM so that data gets updated every hour from FieldPulse to MySQL.For getting data from GCP MySQL to Google Data Studio(GDS) :Connecting GCP MySQL to Google Data Studio :  Connect GCP MySQL to GDS as follows –Open a new reportClick on add dataChoose MySQL connectorEnter following fields :Host Name or IP  :  xxx.xxx.xxx.xxxDatabase             :   xyzUsername            :   xyzPassword             :   ********** Enable SSLUpload server-ca.pem certificateUpload client-cert.pem certificateUpload client-key.pem certificateClick AuthenticateAdd whatever table you wantBuild VisualizationProject DeliverablesBelow are the services that we provided to client after completion of this project –Deployed Data Pipeline in GCP :  A Data Pipeline connecting FieldPulse(https://webapp.fieldpulse.com/) to GCP MySQL that is deployed on a client’s GCP VM instance. It updates the data in MySQL every hour. It extracts the following data tables from FieldPulse –Job DataTag DataTeam Member DataTeam DataMaintaining a log file in Google Cloud :  There is a log file in cloud to resolve unexpected error quickly if any , log file stores following details –last pipeline synced timeError type if anyError location if anyWork Order Data :Job idWork order no.Tags titlesStart_timeJob_typeCreated ByStatusInvoice_statusAssigned teams nameProject_idAssignment_countAssignable_typeNotesCustomer_notesCustomer_first_nameCustomer_last_nameLocationAssigned_team_members nameEnd_timecreated_atJob Tag Data :Tag idsCompany_idMongo_idTitle (Tag name)TypeColorCreated_atUpdated_atdeleted_atSetup to Connect GCP MySQL to Google Data Studio(GDS) :  Provided a setup to connect GCP MySQL to GDS easily. Client can access his live data from MySQL to GDS and make visualizations out of it. Tools usedGoogle ColabLanguage/techniques usedPythonWeb ScrapingMySQLSkills usedProgramming in Python  Data Structure & Algorithm Web ScrapingFile HandlingGoogle CloudGoogle Data StudioDatabases usedMySQLWeb Cloud Servers usedGoogle Cloud Platform (GCP)What are the technical Challenges Faced during Project ExecutionGetting Data from FieldPulse : As there is no open source package/library in Python for accessing Fieldpulse API , we struggled a lot to get the desired data from FieldPulse.Setting Up Connection from GCP MySQL to GDS :  Due to firewall and VPN , connection was not set up as IP address changes while using VPN. It was showing an error every time someone tries to connect to MySQL from their Google Studio account.How the Technical Challenges were SolvedGetting Data from FieldPulse : We did use web scraping for this. We explored all the API addresses. We connected to each possible address and got the data then explored the data. Made a list of addresses which contains data of our interest. Also data is stored in a scattered and cascaded manner in FieldPulse with ids. So , we had to fetch a lot of extra tables and then join multiple tables to get a desired data table.Setting Up Connection from GCP MySQL to GDS :  To resolve this issue , we did as below –set up the IP address in GCP MySQL security to 0.0.0.0 , so that any system can access it. (VPN issue resolved)Enabled the SSL in GCP MySQL. (to prevent it from unauthorized access)Project Video"
Netclan20241133,https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecasts/,"QuickBooks dashboard to find patterns in finance, sales, and forecasts$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: e-commerce, retail business, marketingOrganization Size: 100+Project ObjectiveBuild a fully Integrated BI Platform in PowerBI using native connectors and APIs(QuickBooks and Airtable) to pull real time data from many sources.Project DescriptionFor building a fully integrated BI Platform , the data has to come from the following sources to feed it to PowerBI –·         QuickBooks :  An accounting software that accepts real-time business payments ,  manage and pay bills, manage organisation’s deposits/expenses , customers ,and payroll functions. The following data/tables has to be fetched from Quickbooks –o   Customero   Invoiceso   Product & Serviceso   Paymentso   Expenseso   Depositso   Accountso   Vendorso   Departmentso   Classes·         Airtable : An online database hybrid platform for creating and sharing relational databases with friendly user interfaces. The following databases with multiple data table has to be fetched from Airtable –o   Marketing Data Analytics Base (Google Ads , Facebook Ads)o   Payroll Tracking (Payroll , Hours Log) This Quickbook and Airtable real time data has to go to the powerBI service (https://app.powerbi.com). Then create useful visualisation and dashboards based on plan and feedback from the executive team. All visuals in dashboards should automatically update without any intervention to make it fully integrated.Our SolutionCollecting data tables from data sources :Data Pipeline(QuickBooks to Airtable) – We have built a Data Pipeline in Python that uses quickbooks API(https://pypi.org/project/python-quickbooks/) to get raw data tables from QuickBooks and uses Airtable API (https://api.airtable.com/v0/base_key/Table_name?api_key=YOUR_API_KEY) to write/update data in Airtable. It fetches all the below raw tables after making requests to QuickBooks API –Customers , Invoices , Expenses , DepositsAccounts , Departments , Vendors etc.After getting these raw data tables , pipeline converts it into DataFrame , then writes/updates it into Airtable. The Pipeline is deployed in a server that runs every night , it fetches the data from QuickBooks API and writes/updates to Airtable.Airtable to PowerBI – As there is no connector available to sync data from Airtable to PowerBI. We have used pagination using DAX queries to get data from Web Sources i.e. Airtable API. Pagination fetches the data page by page  using a source and offset technique set by the Airtable API developers. It successfully fetches all the below bases from Airtable API –Marketing Data Analytics Data (Google Ads , Facebook Ads)Payroll Data (Payroll , Hours Log)Scheduled Refresh :  To refresh visualization/dashboard (If incoming data from Airtable API has updated) , set refresh time in powerBI service.Preprocessing of  Data – We have used DAX queries to prepare and process the raw data coming from Airtable like –Split data , typecast dataFilter data (fill missing values , delete irrelevant rows etc.)Create visualizations/Dashboards – We have used following techniques to create visualizations –Used M code queries to extract useful/desired dataUsed measure to perform calculations on dataUse a calculated table to create a relationship between two tables.Used data joining (Union , Intersection) to get desired dataProject DeliverablesBelow are the services that we provided to client after completion of this project –Deployed Data Pipeline :  A Data Pipeline connecting QuickBooks to Airtable to sync in the following data tables –CustomersInvoicesProduct & ServicesExpenseDepositsPaymentsAccountsVendorsDepartmentsClassesQuickBooks Data Dashboard : It contains following visualizations –KPIs –Total RevenueTotal SpendTotal ProfitProfit MarginNo. of CustomerLine Charts –Revenue/Expense over daysBar Charts –Revenue & Expenses by BusinessesProfit/loss by BusinessesRevenue & Expense by ClassProfit/loss by ClassPie ChartExpenses by CategoryPaid/Unpaid InvoicesTables –[Class , Business , Revenue , Spend , Profit , Profit Margin)[Customer , Balance , Due(in days)][Customer , Balance , OverDue][Account , QuickBooks Balance]Filters/Slicer –Transaction DateBusinessClassMarketing Analytics (Facebook Ads) Dashboard –KPIs –All ImpressionsTotal ReachTotal Link ClicksAverage CPMAmount Spent on AdsTotal BudgetBudget LeftLine Charts –Avg. Frequency Over DaysAvg. CPC over daysImpressions , Reach and Page Engagement over daysLink Clicks by day and Account NameResults , Cost per Results over daysAd set Budget and Amount Spent Over daysBar Charts –Ad set Budget and Amount Spent by Account NameGauge –Daily Avg. LinksCount of Ongoing CampaignsTables –Top Compeigns [Account name , Compeign name , Link Clicks , Impressions , Reach , Avg. Frequency , Social Impressions]Filters/Slicer –Account nameDate RangeMarketing Analytics (Google Ads) Dashboard –KPIs –Total ImpressionsTotal ClicksTotal ConversionsTotal CostDaily Avg. CostDaily Avg. CTR Daily Avg. Conversion RateDaily Avg. Cost per ConversionLine Charts –Clicks and Conversions over daysAvg. CPC over days by Day and Google Ad AccountClicks per Impressions by Day and Google Ad AccountImpressions by Day and Google Ad AccountCost by Day and Google Ad AccountClicks by Day and Google Ad AccountGauge –Avg. Daily new ConversionsPie Chart –Count of Google Ad AccountsTables –Top Ads [Ad name , Ad Group , Conversions][Google Ad Account , Impressions , Clicks , Conversions]Filters/Slicer –Date RangeGoogle Ad Account NamePayroll Dashboard –KPIs –$ Total Payroll$ Avg. RateCount of InvoiceTotal Payroll Time (in hrs.)Avg. TurnArroundTime (in Days)Total HoursLine Charts –Avg. Rate over DaysAvg. daily Pay AmountBar Chart –Payroll time by Employee$ Payroll by EmployeeHours by EntityTotal hours by EmployeePie Chart –Paid/Unpaid InvoicesTables –Payroll [Employee , Count of Invoice , Total Due , Paid Before/After Due Date]Filters/Slicer –Date RangeEmployee nameEntity nameTools usedPowerBILanguage/techniques usedPythonPaginationSkills usedProgramming in Python  Data Structure & Algorithm API Integration (QuickBooks , Airtable) File Handling PowerBI(with DAX , M code queries) Data AnalyticsWhat are the technical Challenges Faced during Project Execution?QuickBooks Refresh Token Expired Issue : As stated in QuickBooks Developer Guide , we need refresh token to access QuickBooks API and it expires after 101 days. But that is not the case , it usually expires within 2 to 4 days depending on how frequently we access the API. In that case our deployed Pipeline does not work if the token expired.Getting data from Airtable to PowerBI : As PowerBI has no Airtable data source connector to fetch data from Airtable , we did use Web Source connector using Airtable data web links. It only fetches the 1st page that is 100 rows from Airtable base because Airtable API gives only 100 rows/request.Dynamic Data Source Refresh Issue :  As the URL of Airtable bases data changes based on the size of data. PowerBI recognizes it as Dynamic Data Source , hence it gives the error “Dynamic Data Source Refresh Error” in PowerBI Service.How the Technical Challenges were SolvedQuickBooks Refresh Token Expired Issue :  As the token may expire anytime after 2 days , so to resolve this we have added a gui element in Pipeline so that if token expires a pop up will appear asking for a new refresh token , until the consumer enters a valid new token from their QuickBook developer account , a pop up will keep coming and pipeline will be paused. Once the user enters a new token , the pipeline will continue working.Getting data from Airtable to PowerBI : To resolve this issue , we have used Pagination technique as below –First request Airtable API with proper URL , api_key and blank_offset (data_url?API_KEY=api_key?OFFSET=blank_offset)This request returns first 100 rows of data and a new offset valueNow replace the previous offset value with a new offset value in the URL , and again make an API request.This request will return the next 100 rows of data and a new offset.Do this until you get a null offset (null offset means , all data has been fetched)This is how we get all the data of any size from Airtable bases.Dynamic Data Source Refresh Issue : The above mentioned Pagination technique converts dynamic URLs of Airtable bases data into static URLs. So PowerBI gives no error as it has been converted to a static data source. Now the client can refresh the dashboard manually by clicking the refresh button or can set automatic refresh daily at some given time. Project SnapshotsProject Videohttps://www.youtube.com/watch?v=iemcyRtWPNU&ab_channel=Blackcoffer"
Netclan20241134,https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/,"Marketing, sales, and financial data business dashboard (Wink Report)$Client BackgroundClient: A leading retail firm in AustraliaIndustry Type:  RetailServices: e-commerce, retail business, marketingOrganization Size: 100+Project ObjectiveBringing in data from many sources(Google Analytics , ServiceM8 and Xero etc.) and making Business Dashboard KPIs in Wink Report.Project DescriptionFor building Business Dashboards in Wink Report , collect data from the following sources – ServiceM8XeroFacebookGoogle AdsCommuniqaExplore/analyze the underlying data tables from each Data Source. Make useful reports using different tables from different data sources based on client’s requirement. Set up formulas in each report to calculate desired fields. Add a custom visualization to each report for making dashlets. Add dashlets to newly created dashboards. Our SolutionFor collecting the data from the sources (ServiceM8 , Xero , Facebook , Google Ad) native connectors have been used , available in the Wink Report. It fetches the following data/tables from around the given data sources –ServiceM8 Connector –AssetsClientInvoicesJob AllocationsJobsMaterialsPaymentsXero Connector –Bank Transaction ItemsBudget Vs ActualEmployeesPaymentsPayslipProductsPurchase OrdersPurchase InvoicesSales InvoicesTransactionFacebook Connector –Facebook Ad InsightsGoogle Ads Connector –Ad InsightsGoogle Analytics Connector –eCommerce CampaignTotalsData Pipeline : For collecting data from Communiqa website (https://www.communiqa.com.au/) , web scraping has been used as there is no connector available for Communiqa to Wink Report. By scraping Communiqa , we get the following data –Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc.Then , we have merged different tables from different sources to get desired reports. Store all reports belonging to the same dashboard in a separate folder. Do this for all the dashboard , then setup formula for calculating desired fields. Add appropriate visualization to each report for each folder. Then , finally add all dashlets belonging to the same folder to a newly created dashboard.Project DeliverablesBelow are the services that we provided to client after completion of this project –Data Pipeline(Communiqa to Wink Report) :  A Data Pipeline connecting Communiqa to Wink Report to sync in the following data tables –CSR calls [Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc]Company Performance Dashboard : It contains following visualizations –KPIs –Sales This MonthLeads Booked TodaySales TodayRevenue This MonthCash Payment This MonthConversion RateOpen Warranty JobsBar Charts –Scheduled Jobs by CategorySales by MonthRevenue by MonthTables –Open Jobs from Last month[Job Id , Opened Date , Status, Invoice Amount, Amount Paid]Filters/Slicer –Date RangeJob StatusDate Grouping(Daily/Monthly/Yearly)Lead Generation Dashboard –KPIs –Total Website Traffic this monthAverage Daily Website Traffic this monthNo. of Conversion this monthTotal Marketing Investment this monthMarketing Budget TrackingCost per AcquisitionLine Charts –Link Clicks and conversion by monthTotal marketing spend by monthBar Chart –Lead Generation Count by SourcePie Chart –Lead Generation Source by Invoice AmountFilters/Slicer –Date RangeJob StatusLead Conversion Dashboard –KPIs –All Employees monthly Sales TargetAll Employees monthly Conversion RateFilters/Slicer –Date RangeJob StatusCompany Leads/Target Dashboard –KPIs –Total Hi-pages Lead todayTotal Hi-pages Lead this monthTotal OneFlare Lead todayTotal OneFlare Lead this monthTotal Google Ads Lead todayTotal Google Ads Lead this monthTotal Facebook Ads Lead todayTotal Facebook Ads Lead this monthCompany Daily Sales TargetCompany Monthly Sales TargetFilters/Slicer –Date RangeJob StatusTools usedWink ReportLanguage/techniques usedPythonWeb ScrapingSkills usedData AnalyticsData VisualizationProgramming in Python  Data Structure & Algorithm Web ScrapingFile Handling What are the technical Challenges Faced during Project ExecutionMerging reports from different data sources : Faced the issue of making the cross report from different data sources.Take live parameter input daily in Dashboards from User : Taking live user parameter input daily to feed in Wink report Dashboard. So that dashboard KPIs can change accordingly.How the Technical Challenges were SolvedMerging reports from different data sources : Resolved this issue by using merge report configuration. Using this we were able to join tables from different data sources like – Left join , Right join , Union etc.Take live parameter input daily in Dashboards from User : To resolve this issue , we added a custom field in reports with input tag. Users can enter their parameter in this custom field and all dashlets in the dashboard would update automatically.Project Snapshots Project Video"
Netclan20241135,https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/,React Native Apps in the Development Portfolio$Here are the list of react native apps developed by the team and the resources:https://itunes.apple.com/us/app/truckmap-truck-gps-routes/id1198422047?mt=8https://play.google.com/store/apps/details?id=com.truckmap.truckmaphttps://play.google.com/store/apps/details?id=com.verifai.standalonehttps://apps.apple.com/nl/app/verifai/id1504214033https://apps.apple.com/de/app/meetlist-lokale-aktivit%C3%A4ten/id1439183715https://play.google.com/store/apps/details?id=de.mlug.meetlisthttps://play.google.com/store/apps/details?id=com.payroo.employeehttps://play.google.com/store/apps/details?id=com.vahcarehttps://play.google.com/store/apps/details?id=com.candorivf
Netclan20241136,https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/,"A Leading Law Firm in the USA, Website SEO & Optimization$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveConnect website to Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.Fix SEO of the website.Project Description Connecting website to Google Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.Fixing SEO of the website.Our SolutionWebsite connected to Google Search Console, Google Analytics and Facebook Pixel successfully.Fixed the meta description errorbroken link error404 error, etc.Tools usedSquarespaceGoogle Tag ManagerGoogle AnalyticsGoogle Search ConsoleLanguage/techniques usedJavaScriptSkills usedSquarespaceGoogle Tag ManagerGoogle AnalyticsGoogle Search ConsoleJavaScriptProject SnapshotsProject website URLhttps://www.keepingorlandomoving.com/"
Netclan20241137,https://insights.blackcoffer.com/a-leading-hospitality-firm-in-the-usa-website-seo-optimization/,"A Leading Hospitality Firm in the USA, Website SEO & Optimization$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveWorking On-page SEO of the pages to make it user-friendly and feasible for crawlers to make the site indexing better.Project DescriptionFirstly, exploring the Liverez as it was a new platform then, performing intermediate SEO like page titles and description, completing word count, alt. text and removing duplicate page title and description.Our SolutionTo increase the organic traffic of the site and improve the insights.Project DeliverablesThere was a bit of improvement in the traffic of the site.Tools usedBrightlocal.com, Yoast SEO, GrammarlyLanguage/techniques usedBasic HTMLSkills usedON-page SEOProject SnapshotsProject website urlhttps://www.missionbeach.com/"
Netclan20241138,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-website-seo-optimization/,"A Leading Firm in the USA, Website SEO & Optimization$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveFixing On-Page SEO of the websiteProject DescriptionFixing On-Page SEO contains things like title, meta description, image-alt text, broken links, 404 error page, multiple h1 tag in one page, duplicate title/description, dynamic URL, sparse content page (word count <500), etc. Our SolutionFixed all the possible solutions that we can do for improving the SEO health score. Fixed, image-alt text error, title, meta description, broken links, dynamic URL, 404 error page, sparse content pages, contact information on all pages, connecting website with Google search console.Tools usedAhrefsWordPressGoogle Search ConsoleLanguage/techniques used HTMLRedirection pluginSkills usedHTMLWordPressGoogle Search ConsoleProject SnapshotsProject website URLURL https://www.jupiteroutdoorcenter.com/
Home"
Netclan20241139,https://insights.blackcoffer.com/a-leading-musical-instrumental-website-seo-optimization/,"A Leading Musical Instrumental, Website SEO & Optimization$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveConnect website to Google Tag Manager.Remove error.Project DescriptionRemove all previously added code and add new code for connecting to Google Tag Manager.Remove 5xx error from the website.Our SolutionWebsite connected to Google Tag Manager successfully.Removed 5xx error.Tools usedGoogle Tag ManagerWordPressLanguage/techniques usedJavaScriptSkills usedWordPressGoogle Tag ManagerProject website URLURL: https://www.hamiltonpianoco.com/"
Netclan20241140,https://insights.blackcoffer.com/a-leading-firm-in-the-usa-seo-and-website-optimization/,"A Leading Firm in the USA, SEO and Website Optimization$Client BackgroundClient: A leading marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveConnect website to Search Console. Add Call Rail CodeProject DescriptionConnecting website to Google Search Console through Google Tag Manager.Connect website with CallRail.Our SolutionWebsite connected to Google Search Console successfully.Added CallRail code to the website.Tools usedkvCoreGoogle Tag ManagerGoogle Search ConsoleCallRailLanguage/techniques usedJavaScriptSkills used: kvCoreGoogle Tag ManagerGoogle Search ConsoleCallRailJavascriptProject SnapshotsProject website URL: https://www.12stonesnwa.com/"
Netclan20241141,https://insights.blackcoffer.com/immigration-datawarehouse-ai-based-recommendations/,"Immigration Datawarehouse & AI-based recommendations$Client BackgroundClient: A leading business school worldwideIndustry Type:  R&DServices: R&D, InnovationOrganization Size: 100+Project ObjectiveObjective of this project is to research and collect news article data sourcing from Canada, based on the keyword. Project DescriptionThere were 3 phases of the project. Phase 1– Data collection and selectionData related to anyone coming to Canada (new comers)Data related to anyone coming to Canada (new comers) Canadian policy to new comersi.e. from any country to CanadaData containing News, press, think tanks, government policy documents, or research institutions releasing the news or press aboutThe news source should be limited to Canada onlyTime span- 2005 to 2021Output- Excel having URLs or the documents along with the source type, keywords, and date on which that article is posted.Phase 2– Documents text data extraction Develop tool to collect and extract data from each URL.Clean and save the texts in the text documentsPhase 3– Textual AnalysisSentiment AnalysisAnalysis of readabilityTopic modelling Our SolutionWe provide them with completed Phase 1 in an excel sheet and ongoing samples for Phase 2. Also work for Phase 3 has been started in between to complete the Project as soon as possible in a best way.Project DeliverablesThere is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from Phase 2.Tools usedPython, PyCharm, Jupyter Notebook, Microsoft Excel, Google Chrome is used to complete different phases of this projectLanguage/techniques usedPython programming language is used to do Web Scraping, Automation, Data Engineering in this project.Models usedSDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.Figure 1 SDLC Iterative Waterfall ModelSkills usedData scraping, cleaning, pre-processing and creating data pipelines are used in this project.Databases usedWe used the traditional way of storing the data i.e file systems. What are the technical Challenges Faced during Project ExecutionThere were a lot of challenges we faced during the project execution. As on the internet, raw data is available to us. So, to search for the important data specifically related to Canada only, with a lot of keywords was a challenging part for us.Then, if we somehow manage to do the task by automating it upto some extent only, we are required to find the dates of the articles, news, think tanks, documents etc, that was also a challenging part.While working on Phase 2, we need to scrape the data from the URLs, so sometimes, the news articles were removed from the website, which we earlier took in our datasets which cause problems in extracting the data.Then cleaning the webpages was also challenge for us, because this project is for research, so data is important to us. So, it was difficult to take only that data from website which we require and are most important.How the Technical Challenges were SolvedBelow are the points used to solve the above technical challenges-We used sitemaps of websites to find different articles that we require according to the keywords, manual research was done to find out which URL will solve the purpose. Manual checking of results of automation tools, that we created, was done.To find the dates of the articles, we wrote multiple regular expressions, that will find the match for the dates that we need, also manual checking was done after that.To scrape removed webpages, we used WayBack machine or google archives, which stores all the deleted webpages.To clean the data, we filtered out various HTML tags, classes, ids by using regex, manual research.Project Snapshots"
Netclan20241142,https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/,Lipsync Automation for Celebrities and Influencers$Client BackgroundClient: A leading tech firm in IndiaIndustry Type:  EntertainmentServices: B2COrganization Size: 100+Project ObjectiveTo change the lipsing of the original video with the new replaced audio.Project DescriptionWe needed to create an output video that will have the new lipsing according to the new replaced audio. Also we will have to change the actual audio with the new audio with automated editing.Our SolutionWe have created two different files which will perform 2 different operations 1st will replace the original audio with new and extract only video from original. 2nd will take the muted video and replaced audio and we will get the output of the new replaced audio lipsync. This is done by pre-defined model Wav2Lip on github.Project Deliverables2  google colab notebooksTools usedgithubGoogle driveLanguage/techniques usedPython 3.6moviepyffmpegModels usedWav2lipSkills usedPython programmingData scienceDatabases usedProvided by the company (Hrithik Roshan video files)Project SnapshotsProject website urlhttps://colab.research.google.com/drive/18mlREpLmV9hj-uDfufkGJ_-m_E37Hct9?usp=sharinghttps://colab.research.google.com/drive/1FZHvcVKyJxOUkUFI2auPt3vTOu4jh09K?usp=sharing
Netclan20241143,https://insights.blackcoffer.com/key-audit-matters-predictive-modeling/,"Key Audit Matters Predictive Modeling$Client BackgroundClient: A leading business school worldwideIndustry Type:  R&DServices: Research & InnovationOrganization Size: 10000+Project ObjectiveDo regression modeling on the data provided, cross-country determinants of Key Audit Matters (KAMs) and its usefulness to Investors and Debt Market ParticipantsProject DescriptionUSEFULNESS TO EQUITY MARKETSExamine whether the number and content of KAMs varies with country-level determinants. Explore whether the usefulness of KAMs to investors varies with country level variables such as type of law, enforcement etc. Examine whether adoption of the expanded auditor’s report associated with change in audit quality? Examine whether the content in the auditor’s report improves audit quality. Does this vary across countries?Is the adoption of the expanded auditor’s report associated with a change in audit fees?Explore whether the content of auditor report moderates the usefulness of KAMs to investors (also check by country-level variables)Can the number and content of KAMs be used to predict restatements (2017 onwards)? In order to do the analysis and hypothesis testing, create a mapping to divide the audits into sub category and category according to the sub category and category provided in the question document. Clean the data before proceeding and calculate variables ABRET, ABVOL, CAR and CAAR according to the description provided.Our SolutionCreated a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.Calculation of variable ABRET and ABVOL is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. Cleaning is done on the data by removing the repetitive entries from the dataset and then selected the data around the date for which the variable is to be calculated. Similarly, calculated ABVOL in which extracted the data around the annual report filing date and mean value for 40 days interval that ends 21 days before earning announcement dates.Couldn’t proceed because dataset provided by the client was incomplete in order to calculate ABRET.Language/techniques usedR language to create mapping for the key audit matters and save data set for question 1.Python pandas library to deal with dates and extract data around annual report filing date.Skills usedData mapping, data cleaning, data manipulation, debuggingDatabases usedKey audit matterGDP rule lawAudit feeTrading dataEarning date Report filing dateWhat are the technical Challenges Faced during Project ExecutionDataset provided by the client was too big and made my system slow when the data is loaded in the environment. Too many datasets and variables made it bit difficult to understand and time taking.How the Technical Challenges were SolvedCalculated the number of unique identifiers in the large dataset and sorted those. Then selected the data for 1 unique identifier and sorted dates for it and append it to the dataframe and saved group of such unique identifiers to reduce the size of the dataset and performed the calculations in loop. To tackle the difficulty of understanding the data I made a document tracking all the columns or variables present in the data."
Netclan20241144,https://insights.blackcoffer.com/splitting-of-songs-into-its-vocals-and-instrumental/,"Splitting of Songs into its Vocals and Instrumental$Client BackgroundClient: A leading Entertainment firm in the USAIndustry Type:  EntertainmentServices: MusicOrganization Size: 100+Project ObjectiveThe objective of this project is to split a song into its vocals and instrumental.Project DescriptionThe project aims at taking a Hindi language song as input and separating the vocals(lyrics) from the instrumental music of the song. Save both the vocals and instrumental files separately as output.Our SolutionI have used Python programming language for this project. The use of a Python library called Spleeter developed by Deezer has been made to achieve our goal.Spleeter is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavor of separation :Vocals (singing voice) / accompaniment separation (2 stems)Vocals / drums / bass / other separation (4 stems)Vocals / drums / bass / piano / other separation (5 stems)2 stems and 4 stems models have high performance on the musdb dataset. Spleeter is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.Project DeliverablesPython tool that takes Hindi song as input and gives two audio files as output: vocals file and instrumental file.Language/techniques usedPythonModels used2 Stems modelSkills usedAdvanced Python programmingProject Snapshots"
Netclan20241145,https://insights.blackcoffer.com/ai-and-ml-technologies-to-evaluate-learning-assessments/,"AI and ML technologies to Evaluate Learning Assessments$Client BackgroundClient: A leading EduTech firm in the USAIndustry Type:  EduTechServices: Educations. TrainingOrganization Size: 1000+Project ObjectiveConfirmation / Identification of data that can be used / obtained without bias.Understanding the Actions that are required to be performed post analytics.Converting the data into metrics using formulae that can be used to conduct the analysis.  Project DescriptionIt is a culture management platform that uses learning as the fundamental mode of communication. The platform requires an Analytics portion that captures a variety of data related to the interaction of the learner with content, assessments, engagements and forums to create personalized learning plans for each user to increase the effectiveness of learning and its retention which together make an impact on the overall productivity of the learner and the organization.Our SolutionWe helped the client in deciding the data required for the analysis process. We came up with the appropriate models for various tasks and interpretations of how the data will be collected and analysed for the initial response, final response, retention, proficiency, and learning intent of the user. We designed the models in such a way that one can perform seamlessly grading for each question type (based on difficulty level) and at a different hierarchical level (sub-section, section, training, and so on). We knew that each user has its unique aptitude level (basic, intermediate, and advanced) and keeping that in my mind, we incorporated those aptitude levels in our analytics too. Moreover, we integrated the grade and time factor into the analysis so that more points are allotted for comparatively tough questions and quick responses, respectively.Project DeliverablesMS Excel sheet, Google spreadsheets with proper tables and visualizations. Tools usedJupyter notebook, MS Excel, Google Spreadsheets. Language/techniques usedPython.Skills usedData science and analytics.Databases usedGenerated our data through data simulation.What are the technical Challenges Faced during Project Execution?Data analytics is all about analysing and finding patterns in the data that already exist or are getting generated in real-time. However, this project is in the budding stage, and we had no data to start our analysis. Moreover, this project is novel, and the dataset that meets our requirements was nearly impossible to find online. How the Technical Challenges were SolvedWe performed data simulation techniques and tried to generate the data as authentic as possible using some libraries in python and random functions in spreadsheets. We also generated the data manually at a small scale, but we made sure that we are including every human factor in it.Project Snapshots (Minimum 10 Pictures)"
Netclan20241146,https://insights.blackcoffer.com/datawarehouse-and-recommendations-engine-for-airbnb/,"Datawarehouse, and Recommendations Engine for AirBNB$Client BackgroundClient: A leading hotels chain in the USAIndustry Type:  Real Estate, HospitalityServices: HostpitalityOrganization Size: 1000+Project ObjectiveTo download the data from the servers using Cyberduck on the daily basis and perform data engineering on it. Project DescriptionFirstly, download the property and forward files from the serverSecondly, From the property master file a new data set was created with the conditions that the Bedrooms from Property file should be 5 or more or Max Guests from Property File should be 16 or more and City from Property File should be Sevierville or Pigeon Forge or Gatlinburg.In the forward file only those with status = R were kept and the other data was removed.Finally, forward file was merged with the new data set on ‘Property ID’ i.e., keeping only those forward data with the common ‘Property ID’ and City, Bedrooms, Max Guests columns from the new dataset was added to the forward file.Our SolutionWe created a Python Script which performs the task and create property and forward master files, which we deliver to client on weekly basis.Project DeliverablesTwo csv files named property master file and forward master file to be delivered weekly after applying various steps.Tools usedPyCharm, PowerBi, Cyberduck, Microsoft Excel.Language/techniques usedPython Programming Language is used to create scripts performing Data Manipulation in different files.Models usedSDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.Figure 1 SDLC Iterative Waterfall ModelSkills usedSkills such as Data Pre-processing, cleaning, and data manipulation are used in this project.Databases usedWe used traditional way of storing the data i.e file systems.Web Cloud Servers usedCyberduck, which is a libre server and cloud storage browser for Mac and Windows with support for FTP, SFTP, WebDAV, Amazon S3 etc, was used in this project with Amazon S3 servers.What are the technical Challenges Faced during Project Execution?Data to be processed was very big in size, so space complexity was a challenge in this projectHow the Technical Challenges were SolvedTo solve the space complexity issues, we tried PowerBi, but now time complexity arises. Then we did processing in chunks, by reducing file sizes to avoid memory errors.Project Snapshots (Minimum 10 Pictures)"
Netclan20241147,https://insights.blackcoffer.com/real-estate-data-warehouse/,"Real Estate Data Warehouse$Client BackgroundClient: A leading Real Estate firm in the EUIndustry Type:  Real EstateServices: Real EstateOrganization Size: 1000+Project ObjectiveThe objective of this project is to build a data warehouse from a website given search and filter criteria.Project DescriptionThe objective of this project is to collect data from a website given search and filter criteria.Data Brief:Crawl all the information for the property adverts once a week and store them in a database. Data language: EnglishFilters:Federal StatesContains a list of the federal states in Germany to Crawl:https://en.wikipedia.org/wiki/States_of_GermanyCategories to CrawlMieten WohnungKaufen WohnungKaufen AnlageobjekteKaufen GrundstückOur SolutionWe have developed a Python tool that crawls and scrapes all the apartment listings for all the states in Germany under each category namely: mieten wohnungen, kaufen wohnungen, kaufen anlageobjekte and kaufen grundstuck. The Scrapy library has been used to crawl and scrape. Beautiful soup could have also been used for the scraping purpose, but for the sake of consistency, Scrapy has been used for both purposes.Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.Four Spiders have been created for each category to be scraped. Every spider crawls all the states in Germany and scrapes all the apartment listings for important data. Every spider creates a separate JSON file to store all its data. This data is then converted to CSV using another python script called “conversion”.The python tool has been completely automated and only needs the “Controller” script to be run. The script also has the capability of running every two weeks automatically. Project DeliverablesFour CSV files (one for each category):Mieten Wohnungen.csvKaufen Wohnungen.csvKaufen Anlageobjekte.csvKaufen Grundstuck.csvLanguage/techniques usedPythonWeb Crawling & ScrapingSkills usedData ScrapingData CrawlingAdvanced Python programmingProject Snapshots"
Netclan20241148,https://insights.blackcoffer.com/traction-dashboards-of-marketing-campaigns-and-posts/,"Traction Dashboards of Marketing Campaigns and Posts$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveFor the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used and also view the data according to Impressions and Likes? Project DescriptionWe are testing AWS Comprehend. I performed a key phrase analysis of our LinkedIn posts. We have an output file. Now we need your help to visualize the data so that we can interpret it.I also have the original export file from LinkedIn. I want to answer this business question:For the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used?I want to match up Engagement Rate with key phrase analysis. The business question is this: For the LinkedIn posts that received the highest engagement, what were the most common keywords, phrases and hashtags used?Beyond matching to Engagement Rate, please check if there is a way to also view the data according to Impressions and Likes.Our SolutionData Driven Dashboards which will give the summary of Most used words, keywords, Phrases and also Analysis of Posts as per their interaction with their audience.Project DeliverablesTwo Dashboard Links in which First dashboard represents Key Phrase analysis of the output by AWS Comprehend.Second Dashboard represents the Linked In data Analysis Tools usedPython, Google Data studioLanguage/techniques usedPython Skills usedPython and Data StudioDatabases usedMongoDBWeb Cloud Servers usedGoogle Data StudioWhat are the technical Challenges Faced during Project ExecutionOne of the major problem was to match the output of AWS Comprehend data with the data of excel sheet to find out which posts received maximum interactions and make a dashboard out of it.How the Technical Challenges were SolvedWorking on the output.json file in code editor and comparing it to the Linked In data sheet to check the accuracy of the output file with each post.Project Snapshots (Minimum 10 Pictures)Project website Url1 Key Phrase Analysis Dashboard https://datastudio.google.com/reporting/efbabbff-55ba-4326-8133-78ae304aeb992 Linked IN Data Analysis Dashboard https://datastudio.google.com/reporting/3525e1c1-6c4f-4613-b260-d6e975fe1652"
Netclan20241149,https://insights.blackcoffer.com/google-local-service-ads-lsa-data-warehouse/,"Google Local Service Ads (LSA) Data Warehouse$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveAutomated tool to extract daily review data from Local Service Ads dashboard for all clients.Project DescriptionExtracts data from a company’s Google LSA page for the last 24 hoursThe data is uploaded to the Bigquery database called “LSA_Review_db”.The script runs once a day and is deployed to Heroku by the name “lsa-daily-reviews”.The script runs for all companies in the Google sheet “LSA Review Automation master file”.The following data is uploaded:DateCompany NameLocationTotal ReviewsVerified ReviewsOverall StarReviewer NameReview DateReviewer StarReviewer CommentOur SolutionGet list of companies to monitor along with their LSA URLUse Selenium automated browsing to open the review page for each company.Web scrape the data from the review pagePrepare reportUpload to databaseProject DeliverablesAn automated tool that runs daily and extracts and uploads review data for all companies.Tools usedSeleniumHerokuSheets APIBigQueryLanguage/techniques usedPythonSkills usedData extraction, cleaning and summarising. Web scraping.Databases usedBigQuery –  LSA_Review_dbWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionUsing Selenium to automate web browsing since it takes a large amount of RAM.How the Technical Challenges were SolvedUsing the proper type of dynos and managing their allotment to lower both costs as well as memory usage."
Netclan20241150,https://insights.blackcoffer.com/google-local-service-ads-missed-calls-and-messages-automation-tool/,"Google Local Service Ads Missed Calls and Messages Automation Tool$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveA real time tool to send a report of missed calls and messages to the client.Project DescriptionExtracts data from CallRail database for the last 5 minutesAll the calls which are marked as “missed” and all messages in the data are sent in the form of a report to the client.The script runs every 5 minutes and is deployed to Heroku by the name “missed-messages”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Company NameDateTimeCustomer NameContact No.Customer LocationCall TypeIn case of messages:Company NameDateTimeCustomer NameContact No.No. of messagesDirection (Inbound/Outbound)ContentOur SolutionTo provide data real time, schedule the tool to check for data every 5 minutes.Extract data from CallRailFilter out all answered callsPrepare reportGet email ids from sheetsSend email through SendGridProject DeliverablesAn automated tool which provides real time updates to the client along with all information about the call.Tools usedHerokuCallRail APISendGridSheets APILanguage/techniques usedPythonSkills usedData extraction, cleaning and summarisingDatabases usedGoogle Big QueryWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionSending correct reports only to the companies which are activeHow the Technical Challenges were SolvedUsing Google Sheet’s cell formatting in Python"
Netclan20241151,https://insights.blackcoffer.com/marketing-ads-leads-call-status-data-tool-to-bigquery/,"Marketing Ads Leads Call Status Data Tool to BigQuery$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectivePrepare a daily report for the companies and upload it to BigQuery database. Data is from callrail and contains all call information about a company.Project DescriptionExtracts data from CallRail database for the last 24 hoursThe data is uploaded to the Bigquery database called “Call_Status_From_CallRail”.The script runs once a day and is deployed to Heroku by the name “lsa-call-status-db”.The script runs for all companies in the CallRail database.The following data is uploaded:Company NameStatusLocationCustomer NameCall DateCall TimeContact NoCall StatusCall LeadOur SolutionUse CallRail API to get data from database.Run script dailyFilter out excess dataPrepare reportUpload to BigQueryProject DeliverablesA working deployed automated tool that runs once a day in the morning hours and uploads the data to BigQuery database. Tool is monitored daily.Tools usedHerokuCallRail APIBigQuerySheets APILanguage/techniques usedPythonSkills usedData extraction, cleaning, and summarisingDatabases usedBigQuery –  Call_Status_From_CallRailWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionEnsuring proper data upload to databaseHow the Technical Challenges were SolvedProper monitoring of tool post-deployment."
Netclan20241152,https://insights.blackcoffer.com/marketing-analytics-to-automate-leads-call-status-and-reporting/,"Marketing Analytics to Automate Leads Call Status and Reporting$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectivePrepare a daily report for the companies and upload it to Google Sheets. Data is from callrail and contains all call information about a company.Project DescriptionExtracts data from CallRail database for the last 24 hoursThe data is uploaded to the Google sheet “Call status record”The script runs once a day and is deployed to Heroku by the name “call-status-to-sheets”.The script runs for all companies in the CallRail database.The following data is uploaded:Company NameStatusLocationCustomer NameCall DateCall TimeContact NoCall StatusCall LeadOur SolutionUse CallRail API to get data from database.Run script dailyFilter out excess dataPrepare reportUpload to Google SheetsProject DeliverablesA working deployed automated tool that runs once a day in the morning hours and uploads the data to Google Sheets. Tool is monitored daily.Tools usedHerokuCallRail APIBigQuerySheets APILanguage/techniques usedPythonSkills usedData extraction, cleaning and summarisingDatabases usedGoogle Sheets –   Call status recordWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionEnsuring proper amendment of data to sheets without overwriteHow the Technical Challenges were SolvedProper monitoring before final deployment"
Netclan20241153,https://insights.blackcoffer.com/callrail-analytics-leads-report-alert/,"CallRail, Analytics & Leads Report Alert$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectivePrepare an annual report for the companies and upload it to database. Data is from callrail and contains call analytics.Project DescriptionExtracts data from CallRail database for the last 1 year.The data is uploaded to BigQuery database “lead_report_alert_callrail”The script runs once a year and is deployed to Heroku by the name “lead-report-alert”.Currently, the script is programmed to run for only 2 companies (on a trial basis) – Capital Law Firm and Wilshire Law Firm.The following data is uploaded:Company NameNo. of calls answeredNo. of calls missedNo. of calls abandonedNo. of calls to voicemailTotal CallsOur SolutionUse CallRail API to get data from database.Set time window to be one year.Filter out excess dataPrepare reportUpload to BigQueryProject DeliverablesA working deployed automated tool that runs once a year in the morning hours and uploads the data to BigQuery. Tool is in prototype phase and hence is operational for 2 companies.Tools usedHerokuCallRail APIBigQueryLanguage/techniques usedPythonSkills usedData extraction, cleaning and summarisingDatabases usedBigQuery –  lead_report_alert_callrailWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionWorking on a large amount of data since a year’s data contains hundred of thousands of recordsHow the Technical Challenges were SolvedOptimized code for faster processing."
Netclan20241154,https://insights.blackcoffer.com/marketing-automation-tool-to-notify-lead-details-to-clients-over-email-and-phone/,"Marketing Tool to Notify Leads to Clients over Email and Phone$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectivePrepare a daily report for data from Local Service Ads dashboard and email to client.Project DescriptionExtracts data from the LSA dashboard for the last 24 hours.The data is sent to the client email in the form of a daily report using SendGrid.The script runs every morning and is deployed to Heroku by the name “lead-details-to-email”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Number of LeadsCost Per LeadLead TypeDispute amount to be approvedDispute amount approvedCost per CallOur SolutionUse LSA API to extract data.Clean the data to make it readable and dispose the data not needed.Get the email id of each company from the given SheetSend an email to the client using SendGridDeploy to HerokuProject DeliverablesA working deployed automated tool that runs everyday in the morning hours and sends a report to the client. Tool is monitored everyday.Tools usedHerokuLSA APISendGridSheets APILanguage/techniques usedPythonSkills usedData extraction, cleaning, and summarisingDatabases usedData is not stored and is sent directly to the clientWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionEnsuring a company’s data does not go to another companyHow the Technical Challenges were SolvedTesting on multiple dummy email ids"
Netclan20241155,https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/,"Data ETL: Local Service Ads Leads to BigQuery$Client BackgroundClient: A leading Marketing firm in the USAIndustry Type:  MarketingServices: Marketing consultingOrganization Size: 100+Project ObjectiveUpload daily data from Google Local Service Ads dashboard to BigQuery database.Project DescriptionExtracts data from LSA dashboard for the last 24 hours.The data is uploaded to BigQuery database “lsa_lead_daily_data”The script runs every morning and is deployed to Heroku by the name “lead-details-to-db”.The data is collected only for the companies that are not marked in red in the “Missed Messages Notification Automation – Master File” sheet.The following data is uploaded:Number of LeadsCost Per LeadLead TypeDispute amount to be approvedDispute amount approvedCost per CallOur SolutionUse LSA API to extract data.Clean the data to make it readable and dispose the data not needed.Upload data to a BigQuery database everyday at a fixed time.Deploy to Heroku to run the script everyday.Project DeliverablesA working deployed automated tool that runs everyday in the morning hours and uploads a report to database. Tool is monitored everyday. Tools usedHerokuLSA APIBigQuery APISheets APILanguage/techniques usedPythonSkills usedData extraction, cleaning and summarisingDatabases usedBigQuery –  lsa_lead_daily_dataWeb Cloud Servers usedHerokuWhat are the technical Challenges Faced during Project ExecutionMaking sure that the data uploaded is for the right company.How the Technical Challenges were SolvedMonitoring daily logs and uploads for some time and making sure data was correct"
Netclan20241156,https://insights.blackcoffer.com/marbles-stimulation-using-python/,"Marbles Stimulation using python$Client BackgroundClient: A leading consulting firm in the USAIndustry Type: IT ConsultingServices: ConsultantingOrganization Size: 100+Project ObjectiveFor all 4 cases, use a random number generator that will give you numbers between 1 & a million [1,000,000].  Whatever generator you use, make sure to adjust the numbers so that they are between 1 & 1,000,000 distributed randomly. For all tasks, we will have 5 colors, for example in Task 1, when the random number selected is between 1 & 5857 choose a bright color that is easily visible [I have called it Br. Clr. 1], for numbers between 5858 & 8678 choose another bright color [Br. Clr. 2], for numbers between 8679 & 11500 choose B (Blue), for numbers between 11501 & 50,000 choose R (Red), and > 50,000 choose G (Green). Simulate these 4 Task scenarios and represent them in a Table (1000 x 32) and collect statistics at the end. Replicate the simulation exercises for each Task with 3 different initial seed numbers. Likewise for 16 other Tasks.Our SolutionTask involves creating 20 excel files running a Python Script in Jupyter Notebook which contains certain integer ranges indicating certain values and some other criteria the Random number range [1 to 1 million].There are 20 Different tasks which have different conditions based on which need to form. Simulate these 20 Tasks and represent them in a Table (1000 x 32) and collect statistics at the end. Replicating the simulation exercises for each Task with 3 different initial seed numbers.Then using the Find and Replace tab of excel to make it in the correct format with proper color. Data Representation in particular format and formatting colors, Text based on condition passed within excel.Project DeliverablesExcel FileTools usedJupyterNBSublime TextMS ExcelLanguage/techniques usedPython Models usedNo Software model is being Used to Solve this ProjectSkills usedPython programmingMS Excel FormattingDatabases usedNo database were used stored complete data in MS ExcelWeb Cloud Servers usedNo cloud servers were used for this projectWhat are the technical Challenges Faced during Project ExecutionFormatting Excel Files How the Technical Challenges were SolvedFormatting Excel Files Discovered a lot of Shortcuts Available within Excel to deal with Data Representation in particular format and learned about formatting colors, Text based on condition passed within excel.Replication and Selecting Rows and Columns with shortcuts and in simplest way possible, transposing selected data and many more.Project Snapshots   Figure 1: Sample Output File for Task 12 stimulation 3In total there were 16 conditional tasks all of them had 3 stimulation which needed to be performed."
Netclan20241157,https://insights.blackcoffer.com/stocktwits-data-structurization/,"Stocktwits Data Structurization$Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing"
Netclan20241158,https://insights.blackcoffer.com/sentimental-analysis-on-shareholder-letter-of-companies/,"Sentimental Analysis on Shareholder Letter of Companies$Client BackgroundClient: A leading financial firm in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project ObjectiveProject “Sentimental Analysis on Shareholder Letter of Companies” objective was to Predict the Sentiments columns Shareholder Letter in terms of Polarity and Subjectivity finally classification of data into positive, negative and neutral tone.Project DescriptionThe project ‘Sentimental Analysis on Shareholder Letter of US Companies’ task involved data cleaning on shareholder letters of different companies which includes lemmatization, lower case conversion, removing special character, \n , \t , punctuations, numbers & single character and tokenization. To generate polarity and subjectivity columns for the letter 1 & letter 2 columns using the Textblob library of NLTK. Based on the polarity categorizing it into positive, neutral  &  negative. Our SolutionLetter Text Length VariationContraction mapping on datasetReplacing missing value with some neutral tone string like None so that cleaning doesn’t generate any issue. Data Cleaning and Preprocessing which involves :             i.  Lemmatisation             ii. lower case conversion             iii.  Removing Special character             iv.  Removing \n , \t etc             v.  remove punctuations, numbers & single character removal            vi.  forming list of letter data using tqdmTokenization and word count.Used Textblob Library which is part of NLTK for Sentiment analysis.Created Polarity and Subjectivity column for the Letter1 & Letter2 columnsBased on the polarity of letter 1 created a letter1_type column with values “positive” , “neutral”  &  “negative” category.Project DeliverablesOutput iPython FilePreprocessed DatasetTools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● Python 3.4Language/techniques usedPythonMachine LearningNLP (Natural Language Processing)Models usedMy project ‘Sentimental Analysis on Shareholder Letter of Companies’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : Waterfall Model● For Project ‘Sentimental Analysis on Shareholder Letter of US Companies’ is a Waterfall Model as our model is not forming the loop from end to the start using Textblob which predicts Sentiments, Polarity and Subjectivity as the output following the Waterfall Model.Skills usedPandas OperationsData Chunking and IntegrationData VisualizationDatabases usedNo Database is used to complete this project.Web Cloud Servers usedNo Web cloud Server was required for this work.What are the technical Challenges Faced during Project ExecutionI have worked before on tasks similar to this so there were no challenges faced but the data cleaning was a bit different and required time to complete.How the Technical Challenges were SolvedAs Discussed no technical Challenges were faced during this project.Project Snapshots Figure 1: Input Data SchemaFigure 2: Output Data SchemaFigure 3: Sample Input Datasetfigure 3 is pandas dataframe which was fetched from google cloud database there were 7 columns and 13290 rows.Figure 4: Sample Output Datasetfigure 4 is output pandas dataframe after data cleaning and modeling of sentiment identification there are 13 columns and 13290 rows.Figure 5: Sentiments assignment based on polarityfigure 5 represents the identification of sentiments and tone based on polarity and subjectivity. polarity>0 then sentiment type is positive,  if the polarity<0 sentiment type is negative and if the polarity=0 sentiment type is neutral.Figure 6:  Histogram Representation of Length of Shareholder Letter 1figure 6 is histogram plot between length of shareholder letter 1 among the final output dataset.Figure 7:  Histogram Representation of Length of Shareholder Letter 2figure 7 is Histogram plot between length of shareholder letter 2 among the final output dataset.Figure 8: Flow Chart"
Netclan20241159,https://insights.blackcoffer.com/population-and-community-survey-of-america/,"Population and Community Survey of America$Client BackgroundClient: A leading marketing firm in the USAIndustry Type: Marketing services & ConsultingServices: Marketing consultantOrganization Size: 100+Project ObjectiveProject ‘Population and Community Survey of America’ objective were to perform Data Abstraction, Data Structurization, Data Preprocessing, Data Cleaning, and Combining Data from all the years listed and finally presenting insights of the data by Exploratory Data Analysis.Project DescriptionFor Project ‘Population and Community Survey of America’ task involved fetching json and unformatted csv data from numerous web links further needed to process data, handling nested JSON, data conversion of JSON data in dataframe, performing certain pandas operation for feature selection and structuring data. Concat all this data into one csv file then handle missing value by mapping with another dataset finally perform certain data visualization and exploratory data analysis.Our SolutionModule 1: Data AbstractionThe process of data abstraction involves collecting data from numerous web links from Year 2005 to 2017 and viewing the data using JSON viewer in tree format.Module 2: Data Chunking and IntegrationWas unable to process data in pandas so performed data chunking with chunksize 10000 rows at a time for year 2005 likewise performed for all other years data till 2017 and finally combined all the dataframes into one containing all data from year 2005 to 2017.Module 3: Handling Complexity of Nested Data & format the Unformatted CSV FilesHandling unformatted CSV in proper comma separated format so that data frame can be formed. Dataframe produced after merging for all the years from 2005 to 2017 contains a lot of nested JSON data among certain attributes so performed normalization of nested Json forming new_columns naming them based on their attributes key.2.2.4 Module 4: Data Cleaning and PreprocessingInvolves handling missing value, contraction mapping with another dataset to fill the missing State_Zip_Code column, handling inf and -inf within the dataset for some attributes and forming a new column population_ratio based on passing formula among other attributes.2.2.5 Module 5: Data AnalysisThis step involves forming a correlation matrix to understand the relation between numeric attributes. performed Exploratory Data Analysis on strong correlated attributes to understand pattern/relation between them. Project DeliverablesAfter completion of Project we provided:Final Preprocessed CSV Files Three iPython files: Preprocessed dataset from year 2010 to 2015Preprocessed dataset from year 2008 to 2017 Data Visualization and EDA.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● Python 3.4● JSON ViewerLanguage/techniques used● Python● ETL Techniques● Advanced Excel Formatting Models usedMy project ‘Population and Community Survey of America’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This Project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills usedPandas OperationsData Chunking and IntegrationData VisualizationExploratory Data AnalysisDatabases usedNo Database is used in this project, only used Google Drive for Storing and Transferring Data.Web Cloud Servers usedNo Web Server is UsedWhat are the technical Challenges Faced during Project ExecutionData Cleaning and Filling out Missing Values by Data mapping with another dataset as the Data was not in proper format in the another dataset.How the Technical Challenges were SolvedData Cleaning was done using a few built in pandas operations to deal with Missing Values, Ordering Data Columns, Data Formatting, Changing of data types and many more. Filling of remaining Missing Data from columns using Outer Join among the datasets and using Map Function of Python.Project Snapshots  Figure 1: Input Data Schema for Year 2008  Figure 2: Output Data Schema from Year 2005 to 2017Figure 3: Dataset for Year 2008figure 3 is pandas dataset of year 2008 which has 169595 rows and 25 columns which was fetched from authenticated survey web portal, data obtained were in JSON format which were converted into pandas dataframe likewise there are dataframes created from year 2005 to 2017.Figure 4:  Output Preprocessed Datasetfigure 4 is an output preprocessed dataset from 2005 to 2017 which has 26,41,363 rows and 25 columns.Figure 5: Describing Numeric Data of Preprocessed DatasetFigure 6: Bar plot of attribute state_namefigure 6 represents the bar plot among the state_name on the final output dataset from year 2005 till 2017.Figure 7: KDE Graph for all numeric population data column of datasetfigure 7 represents the Kernel Density Estimate Plot(KDE) among all Population estimate data columns for the Preprocessed Dataset. KDE plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions. Plotted many more graphs apart this between highly correlated attributes like pair plot, box plot, line plot etc.  Figure 8: Flow Chart"
Netclan20241160,https://insights.blackcoffer.com/google-lsa-api-data-automation-and-dashboarding/,"Google LSA API Data Automation and Dashboarding$Client BackgroundClient: A leading marketing firm in the USAIndustry Type: Marketing services & ConsultingServices: Marketing consultantOrganization Size: 100+Project ObjectiveFor this project objective was to perform API Data Abstraction using Google LSA API in GCP, Automation of data fetching and storing in BigQuery on daily basis, Storing Historical data for all active companies, Fetching Customer Report then storing data on daily basis in BigQuery also storing Historical data for all companies, Perform Linear Regression Modelling on Historical data for all companies and storing the modeling Summary in google sheet in a structured manner, Basecamp Automation with LSA Daily Data, Creating 4 BI Dashboard in Data Studio for Live, Historical, Modelling and Customer Report data for all companies. Project DescriptionFor this project task was to obtain an account report and detailed lead report for a specific dates and customer_id using Google Local Service Ads API Service in Google Cloud Platform. Further need to integrate with Google BigQuery database storing MCC data for all companies on a daily basis then storing Historical data for all active companies. Also notifying clients through email and passing messages containing daily account data in a message format to BaseCamp Message Board and Campfire of respective company projects through its API all with python programming, further deploying the script on Heroku Server for automating all this task. Then Creating BI Dashboard in Data Studio connecting with BigQuery and Creating Live Dashboard, Historical Dashboard for all companies. On historical data for all companies, Linear Regression Modelling needs to perform and to create Modelling Dashboard for all companies in Data Studio. Further needs to do  Exploratory Data Analysis for all companies on Historical Data. To Store Customer Account Report for message lead and phone lead on a daily basis, Script needs to be created and deployed in Heroku and also need to store Historical data for these companies and Finally Create Data Studio Dashboard on it.Creating Sales Representation Dashboard for two Companies which involves multiple Reports and blending of multiple data sources from Big Query.Our Solution>> Module 1: API Data AbstractionWhich first includes generation of the access token and refresh token with the scope of Google AdWord API for the authentication and connecting with Google LSA API. Then fetching daily data in JSON format for particular account name based on customer_id assigned in API URL while fetching data. Likewise generating a script that would Handle data generation for all other active accounts based on their customer id.>> Module 2: Data Imputation and StoringConverting the JSON data to the pandas data frame forming a list of data frame for all the active accounts by looping them then deriving certain more attributes based on their handling the missing and inf values. Finally storing the data in Google Big Query database within the respective table for all accounts using Bigquery API.  >> Module 3: Data Storing in BigQuery and Notification AutomationThe task was to automate notifications sent to email and to Basecamp and the data transferred to the database on a daily basis by deploying the script to Heroku Server setting time parameters based on the New York time zone.>> Module 4: Automation tools created till now:i. LSA_AccountReport_daily_BigQuery tool: For Automation of Account Report for all companies on a daily basis. Scheduling it at 1:00 am in the Los Angeles Timezone.ii. LSA_AccountReport_Historical_API tool:  For Storing Historical Data for companies for the last few Years till the end date which we set.iii. Basecamp_lsa_automation: This is used to pass the lsa data in a message format to Campfire for respective companies groups and store lsa data combined for all companies to Messageboard and Campfire at one Automation Python Group in Basecamp.iv. LSA_DateRange Tool: Used to store missed out data for all the companies for a few sets of days or months as per the need.v. LSA_MainSheet_AutoUpdation tool: For Auto updation of main sheet  ‘LSA Client Lead’  Google Sheet. As Daily Data are fetched on the basis of this list so it is required to auto update this sheet for all the new companies entered would store information of those like company name, account id and database name.vi. LSA_daily_CustomerReport tool: Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ on daily basis.vii. Historical_LSA_CustomerReport tool:  Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ storing historical data for year 2021.>> Module 5: Data Studio BI Dashboards Created:i. Historical Dashboardii. Live Dashboardii. Customer Report Dashboardiii. Modelling Report Dashboardiv. Sales Representation DashboardProject DeliverablesData Studio Dashboard Main SheetAll Codes for the Deployed tools and for Modelling EDA and Test Purpose .Tools used● PyCharm● Jupyter Notebook● Anaconda● Heroku● Notepad++● Google Sheet API● Google LSA API on GCP● Google BigQuery● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python● SQLModels usedMy project ‘Google Adword LSA API Reports automation into Google Big Query database and Basecamp’ developed with a software model which makes the project high quality, reliable and cost-effective.● Software Model: RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with the use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from the very beginning solves a lot of integration issuesSkills used● API Data Abstraction● Data Mining and Statistical Modelling● Data Wrangling● Deployment for Automation● Data Visualization● SQL● Machine Learning● Python Programming including OOPs and Exception HandlingDatabases used● Google Firestore (Just for Testing Purpose)● Google BigQueryWeb Cloud Servers usedGoogle BigQuery Cloud Database with up to 1 TB of free storage is being used. What are the technical Challenges Faced during Project Execution● Scheduling Automation of Python Script.● Data Exceptions and Duplication in BigQuery Tables.● Refresh token Expiration After 7 Days.● Data Exception due to Inactive companies or not Updation of LSA Main sheet. ● Basecamp ProjectId Issue for transferring Data to multiple companies projects.● Data Studio Time Series Plot data mismatch due to multiple account id.How the Technical Challenges were Solved● Scheduling Automation of Python Script.Python Library BlockingScheduler were used and the Timezone variable ‘TZ’ was set to Los Angeles in Heroku● Data Exceptions and Duplication in BigQuery Tables.       Structuring SQL Query to deal with all the database issues which were being used in BigQuery to solve those issues.● Refresh token Expiration after 7 Days.Initially ‘Auth Playground’ was used for generating Refresh token which was getting expired after every 7 Days so to last it longer for more than a year we are now using the refresh token which was generated using Python script where proper token endpoints and many other headers were defined before generating the refresh token.● Data Exception due to Inactive companies or not Updation of LSA Main sheet. Data Exception occurred while API data abstraction for few of the companies which were solved by adding more nested try and except statements after understanding issues also ‘LSA Clients Lead’ main sheet was not being updated by other members due to which we missed out data for few of the companies which were solved by creating script which will automatically update the mainsheet when an error occurred.● Basecamp ProjectId Issue for transferring Data to multiple companies projects.This issue was solved by creating Basecamp Main sheet where data was fetched now by mapping the account id of fetched data using LSA Main sheet and project id of all the basecamp companies.● Data Studio Time Series Plot data mismatch due to multiple account id.Solved by adding many parameters like setting the metrics which will do a summation of all the companies on a particular day for all the account id."
Netclan20241161,https://insights.blackcoffer.com/healthcare-data-analysis/,"Healthcare Data Analysis$Client BackgroundClient: A leading healthcare tech firm in the USAIndustry Type: Healthcare ConsultingServices: Management consultantOrganization Size: 100+Project ObjectiveThe main objective of this project is to find the pattern in the vital signs of patients who were admitted to the hospital in past. And from this pattern, we get some ranges that help us to give early warnings.Project DescriptionWe are more interested in non-survivor patients’ vital signs as compare to survivor patients. we find patterns in vital signs that could better determine that patient died (ex. if Sp02 is below 70, patient in 95% of cases died, if Sp02 is below 50%, the death rate is 99.9%) or we can take correlations which can help us to find better patterns to define death cases.Data The dataset which was used for analysis here is taken from the mimic website. But the dataset is not in the correct format which we want, after some manipulation, we get the data ready for the analysis.Our SolutionApproachTo protect patient confidentiality date and time is shifted to future that’s not the actual time so from shifted time column we create an extra column hour which tells us the time passed in hours since first observation in ICU.After all manipulation our final dataset contain vital signs values for each observation of patients with time in separate column and also the label fo Death (0 or 1) in another column.There are two options to deal with missing valuesDrop all rows which contain null values.2.Fill the missing values by some method using pandas. I can’t go with 1st option because a major part of the data has missing values. so, I decided to go with the second option and fill missing values with the average of upper and lower values. But before that, I filtered the data and take only those patients’ data who died in a hospital or survive.Project DeliverablesAfter performing EDA which also include the removal of some impossible outliers, we come up with a result of Analysis.This result helps to build an early warning system which predict the condition of patients on the basis of their score, calculated on their condition using vital sign values.Tools usedGoogle Colab NotebookLanguage/techniques usedPythonSkills usedData visualizationData analysisPandasNumpySeabornDatabases usedSQLMongoDBWeb Cloud Servers usedGoogle CloudProject SnapshotsProject website urlhttps://colab.research.google.com/drive/1mo7i32BoEVb0Ac6_CWwJd7_HVbliktx0?usp=sharing"
Netclan20241162,https://insights.blackcoffer.com/budget-sales-kpi-dashboard-using-power-bi/,"Budget, Sales KPI Dashboard using Power BI$Project DescriptionWeekly Data – clustered bar chart for weekly Budget & Actual value , weekly Total Budget & Actual value (completed)YTD Data – clustered bar chart for monthly Budget & Actual value , monthly Total Budget & Actual value (completed)Sales History – stacked chart for yearly sales with each month sales , total yearly sale (completed)Dashlet – weekly data – Total weekly Budget , Total weekly Actual , % weekly Budget (completed)Dashlet – YTD data – Total YTD Budget , Total YTD Actual , % YTD Budget (completed)Dashlet – Sales History – Total Sales (completed)Filters – select Area , select City , select Years (completed)Data Visualization DeliverablesPresentationMapDashboardAPI IntegrationData Visualization ToolsKibanaGoogle Data StudioMicrosoft ExcelMicrosoft Power BIData Visualization LanguagesJavaScriptSQLPythonDAXDemo"
Netclan20241163,https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/,"Amazon Buy Bot, an Automation AI tool to Auto-Checkouts$Client BackgroundClient: A leading consulting firm in the USAIndustry Type: ConsultingServices: Management consultantOrganization Size: 100+Project ObjectiveThe main objective of this project is to build the automation tool to buy product on amazon.Project DescriptionThis project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium.Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again.Our SolutionA simple python code which uses selenium web driver to do all work.Project DeliverablesPython CodeTools usedSelenium WebdriverLanguage/techniques usedPythonSkills usedWeb ScrapingSeleniumProject Snapshots"
